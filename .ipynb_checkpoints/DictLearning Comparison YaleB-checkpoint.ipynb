{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8541f4bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import semisupervised_params\n",
    "from supervised_embedding_params import Multilearn_GLRM_GetEmbedding\n",
    "from importlib import reload\n",
    "import Multi_Learn\n",
    "import generate_AXY\n",
    "import scipy.io as sio\n",
    "import pkg_resources\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8b740d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_mat(filename):\n",
    "    return sio.loadmat(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ea8fb2ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_fn = pkg_resources.resource_filename('data/myYaleB' + '.mat')\n",
    "vars_dict = load_mat('./DICTOL_python-master/dictol/data/myYaleB' + '.mat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "03344876",
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_to_range(label):\n",
    "    \"\"\"\n",
    "    Convert label to range\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    label: list of integers\n",
    "        must be in the form of [1, 1, ..., 1, 2, 2, ..., 2, ..., C, C, ..., C]\n",
    "        i.e. nondecreasing numbers starting from 1, each element is greater\n",
    "        than the previous element by at most 1\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    a list of intergers with C + 1 elements, start with 0\n",
    "    the i-th element is number of elements in label that equals to i\n",
    "        \n",
    "    \"\"\"\n",
    "    res = [0]\n",
    "    assert label[0] == 1, 'label must start with 1'\n",
    "    for i in range(1, len(label)):\n",
    "        if label[i] == label[i-1]:\n",
    "            continue\n",
    "        if label[i] == label[i-1] + 1:\n",
    "            res.append(i)\n",
    "        else:\n",
    "            assert False,\\\n",
    "                ('label[{}] and label[{}] must be equal or two consecutive '\n",
    "                 'integers, got {} and {}').format(\n",
    "                     i-1, i, label[i-1], label[i]\n",
    "                 )\n",
    "    res.append(len(label))\n",
    "    return res\n",
    "\n",
    "def get_block_row(matrix, block_indices, row_range):\n",
    "    \"\"\"\n",
    "    Extract a subset of rows from a matrix\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    matrix: 2-d numpy array\n",
    "        block matrix\n",
    "    block_indices: integer of list of integers\n",
    "        indices of extracted blocks, 0-indexed. If indices is a list, return\n",
    "        the concatenation of all blocks\n",
    "    row_range: list of intergers\n",
    "        in the form of [0, c_1, c_1 + c_2, ..., c_1 + c_2 + ... + c_N]\n",
    "        where c_i is the number of rows in the i-th block\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    a 2-d matrix\n",
    "    \"\"\"\n",
    "    assert matrix.ndim == 2, 'Expect to receive 2-d array input, got shape {}'.format(matrix.shape)\n",
    "    if isinstance(block_indices, int):\n",
    "        block_indices = [block_indices]\n",
    "    # if isinstance(block_indices, (list, np.ndarray, np.generic))\n",
    "    ids = []\n",
    "    for i in block_indices:\n",
    "        ids = ids + list(range(row_range[i], row_range[i+1]))\n",
    "    return matrix[ids, :].copy()\n",
    "\n",
    "def get_block_col(matrix, block_indices, col_range):\n",
    "    \"\"\"\n",
    "    Extract a subset of columns from a matrix\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    matrix: 2-d numpy array\n",
    "        block matrix\n",
    "    block_indices: integer of list of integers\n",
    "        indices of extracted blocks, 1-indexed. If indices is a list, return\n",
    "        the concatenation of all blocks\n",
    "    row_range: list of intergers\n",
    "        in the form of [0, c_1, c_1 + c_2, ..., c_1 + c_2 + ... + c_N]\n",
    "        where c_i is the number of columns in the i-th block\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    a 2-d matrix\n",
    "    \"\"\"\n",
    "    assert matrix.ndim == 2, 'Expect to receive 2-d array input, got shape {}'.format(matrix.shape)\n",
    "    assert matrix.shape[1] == col_range[-1]\n",
    "    return get_block_row(matrix.T, block_indices, col_range).T\n",
    "\n",
    "def get_block_col(matrix, block_indices, col_range):\n",
    "    \"\"\"\n",
    "    Extract a subset of columns from a matrix\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    matrix: 2-d numpy array\n",
    "        block matrix\n",
    "    block_indices: integer of list of integers\n",
    "        indices of extracted blocks, 1-indexed. If indices is a list, return\n",
    "        the concatenation of all blocks\n",
    "    row_range: list of intergers\n",
    "        in the form of [0, c_1, c_1 + c_2, ..., c_1 + c_2 + ... + c_N]\n",
    "        where c_i is the number of columns in the i-th block\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    a 2-d matrix\n",
    "    \"\"\"\n",
    "    assert matrix.ndim == 2, 'Expect to receive 2-d array input, got shape {}'.format(matrix.shape)\n",
    "    assert matrix.shape[1] == col_range[-1]\n",
    "    return get_block_row(matrix.T, block_indices, col_range).T\n",
    "\n",
    "def randperm(n):\n",
    "    \"\"\"\n",
    "    get a random permutation of range(n)\n",
    "    \"\"\"\n",
    "    return np.random.permutation(list(range(n)))\n",
    "\n",
    "def normc(A):\n",
    "    \"\"\"\n",
    "    normalize each column of A to have norm2 = 1\n",
    "    \"\"\"\n",
    "    return A / np.tile(np.sqrt(np.sum(A*A, axis=0)), (A.shape[0], 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "443db3e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_train_c = 15\n",
    "\n",
    "Y = vars_dict['Y']\n",
    "d = Y.shape[0]\n",
    "if 'Y_range' not in vars_dict:\n",
    "    Y_range = label_to_range(vars_dict['label'].flatten(1)).astype(int)\n",
    "\n",
    "else:\n",
    "    Y_range = vars_dict['Y_range'].flatten('C').astype(int)\n",
    "\n",
    "C = Y_range.size - 1\n",
    "N_total     = Y_range[-1]\n",
    "N_train     = C*N_train_c\n",
    "N_test      = N_total - N_train\n",
    "\n",
    "Y_train     = np.zeros((d, N_train))\n",
    "Y_test      = np.zeros((d, N_test))\n",
    "label_train = [0]*N_train\n",
    "label_test = [0]*N_test\n",
    "cur_train   = 0\n",
    "cur_test    = 0\n",
    "for c in range(C):\n",
    "    Yc        = get_block_col(Y, c, Y_range)\n",
    "    N_total_c = Yc.shape[1]\n",
    "    N_test_c  = N_total_c - N_train_c\n",
    "    label_train[cur_train: cur_train + N_train_c] = [c+1]*N_train_c\n",
    "    label_test[cur_test:cur_test + N_test_c] = [c+1]*N_test_c\n",
    "\n",
    "    ids = randperm(N_total_c)\n",
    "\n",
    "    Y_train[:, cur_train: cur_train + N_train_c] = \\\n",
    "        Yc[:, np.sort(ids[:N_train_c])]\n",
    "\n",
    "    Y_test[:, cur_test: cur_test + N_test_c] = \\\n",
    "        Yc[:, np.sort(ids[N_train_c:])]\n",
    "\n",
    "    cur_train += N_train_c\n",
    "    cur_test += N_test_c\n",
    "\n",
    "Y_train = normc(Y_train)\n",
    "Y_test  = normc(Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a1c6d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(label_train), len(label_test), Y_train.shape, Y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c52b55e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_classes = max(max(label_train), max(label_test))\n",
    "label_train\n",
    "one_hot_targets = np.eye(nb_classes)[np.array(label_train)-1]\n",
    "TRAIN = np.hstack((Y_train.T, one_hot_targets))\n",
    "np.random.shuffle(TRAIN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "96597f86",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0\n",
      "iter: 0 Total loss: tf.Tensor(536137.6479161897, shape=(), dtype=float64)\n",
      "iter: 100 Total loss: tf.Tensor(454013.39193253673, shape=(), dtype=float64)\n",
      "iter: 200 Total loss: tf.Tensor(376150.39858584135, shape=(), dtype=float64)\n",
      "iter: 300 Total loss: tf.Tensor(304858.4977633714, shape=(), dtype=float64)\n",
      "iter: 400 Total loss: tf.Tensor(239660.01719713365, shape=(), dtype=float64)\n",
      "iter: 500 Total loss: tf.Tensor(180244.26248253655, shape=(), dtype=float64)\n",
      "iter: 600 Total loss: tf.Tensor(126598.82177648568, shape=(), dtype=float64)\n",
      "iter: 700 Total loss: tf.Tensor(78812.97115928272, shape=(), dtype=float64)\n",
      "iter: 800 Total loss: tf.Tensor(37828.99609290162, shape=(), dtype=float64)\n",
      "iter: 900 Total loss: tf.Tensor(9940.008800080088, shape=(), dtype=float64)\n",
      "iter: 972 Total loss: tf.Tensor(6685.636241575325, shape=(), dtype=float64)\n",
      "iter: 0 Total loss: tf.Tensor(6682.9067556126065, shape=(), dtype=float64)\n",
      "iter: 100 Total loss: tf.Tensor(3054.779030568089, shape=(), dtype=float64)\n",
      "iter: 200 Total loss: tf.Tensor(2727.1932783201487, shape=(), dtype=float64)\n",
      "iter: 300 Total loss: tf.Tensor(2538.788838602018, shape=(), dtype=float64)\n",
      "iter: 371 Total loss: tf.Tensor(2441.393271096743, shape=(), dtype=float64)\n",
      "epoch: 1\n",
      "iter: 0 Total loss: tf.Tensor(2440.1655905079415, shape=(), dtype=float64)\n",
      "iter: 51 Total loss: tf.Tensor(2093.1451940475745, shape=(), dtype=float64)\n",
      "iter: 0 Total loss: tf.Tensor(2087.572031170111, shape=(), dtype=float64)\n",
      "iter: 100 Total loss: tf.Tensor(1825.2923294817826, shape=(), dtype=float64)\n",
      "iter: 195 Total loss: tf.Tensor(1722.785746735965, shape=(), dtype=float64)\n",
      "epoch: 2\n",
      "iter: 0 Total loss: tf.Tensor(1721.9215204994434, shape=(), dtype=float64)\n",
      "iter: 15 Total loss: tf.Tensor(1732.945490957385, shape=(), dtype=float64)\n",
      "iter: 0 Total loss: tf.Tensor(1722.6840041013384, shape=(), dtype=float64)\n",
      "iter: 100 Total loss: tf.Tensor(1577.6703517723854, shape=(), dtype=float64)\n",
      "iter: 112 Total loss: tf.Tensor(1567.8304592670622, shape=(), dtype=float64)\n",
      "epoch: 3\n",
      "iter: 0 Total loss: tf.Tensor(1567.0205800984822, shape=(), dtype=float64)\n",
      "iter: 31 Total loss: tf.Tensor(1533.969591855781, shape=(), dtype=float64)\n",
      "iter: 0 Total loss: tf.Tensor(1527.9571613640608, shape=(), dtype=float64)\n",
      "iter: 83 Total loss: tf.Tensor(1428.899352788655, shape=(), dtype=float64)\n",
      "epoch: 4\n",
      "iter: 0 Total loss: tf.Tensor(1428.1769648928737, shape=(), dtype=float64)\n",
      "iter: 43 Total loss: tf.Tensor(1419.2421221731556, shape=(), dtype=float64)\n",
      "iter: 0 Total loss: tf.Tensor(1416.775102470087, shape=(), dtype=float64)\n",
      "iter: 69 Total loss: tf.Tensor(1340.3700530507533, shape=(), dtype=float64)\n"
     ]
    }
   ],
   "source": [
    "reload(semisupervised_params)\n",
    "reload(Multi_Learn)\n",
    "reload(generate_AXY)\n",
    "from semisupervised_params import Multilearn_GLRM_Semisupervised\n",
    "from Multi_Learn import alternating_minimization, predict \n",
    "from generate_AXY import get_semisupervised_glrm_train_form\n",
    "\n",
    "embedding_dim = 100\n",
    "n_class = 38\n",
    "\n",
    "A_prime, X_prime, Y_prime = get_semisupervised_glrm_train_form(TRAIN[:,:-nb_classes], TRAIN[:,-nb_classes:], nb_classes, embedding_dim)\n",
    "[GLRM_loss_list, X_regulariation_list, Y_regulariation_list, X_grad_restrictions, Y_grad_restrictions] = Multilearn_GLRM_Semisupervised(A_prime, X_prime, Y_prime, nb_classes)\n",
    "\n",
    "num_iterations=10000\n",
    "learning_rate=0.001\n",
    "result = alternating_minimization(A_prime, X_prime, Y_prime, GLRM_loss_list, X_regulariation_list, Y_regulariation_list, X_grad_restrictions, Y_grad_restrictions, num_iterations, learning_rate, n_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "6df30f32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9894736842105263"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_prob = tf.nn.softmax(np.matmul(result[0], result[1][:,0:n_class])).numpy()\n",
    "label_train_shuffled =  np.argmax(TRAIN[:,-nb_classes:], axis=1)\n",
    "accuracy_score(label_train_shuffled, np.argmax(train_prob, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "918b960c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter: 0 Total loss: tf.Tensor(2255977.369314763, shape=(), dtype=float64)\n",
      "iter: 100 Total loss: tf.Tensor(1890080.1143864787, shape=(), dtype=float64)\n",
      "iter: 200 Total loss: tf.Tensor(1553099.1565338247, shape=(), dtype=float64)\n",
      "iter: 300 Total loss: tf.Tensor(1250871.9985032575, shape=(), dtype=float64)\n",
      "iter: 400 Total loss: tf.Tensor(979625.9600345112, shape=(), dtype=float64)\n",
      "iter: 500 Total loss: tf.Tensor(737365.454929934, shape=(), dtype=float64)\n",
      "iter: 600 Total loss: tf.Tensor(523012.58594212803, shape=(), dtype=float64)\n",
      "iter: 700 Total loss: tf.Tensor(336644.6458529381, shape=(), dtype=float64)\n",
      "iter: 800 Total loss: tf.Tensor(180423.58435286777, shape=(), dtype=float64)\n",
      "iter: 900 Total loss: tf.Tensor(60433.19692413834, shape=(), dtype=float64)\n",
      "iter: 1000 Total loss: tf.Tensor(8886.37492568298, shape=(), dtype=float64)\n",
      "iter: 1080 Total loss: tf.Tensor(6373.618469712981, shape=(), dtype=float64)\n"
     ]
    }
   ],
   "source": [
    "beta = result[1][1:,0:n_class]\n",
    "Y_final = result[1][1:embedding_dim+1,n_class:]\n",
    "\n",
    "A = Y_test.T\n",
    "A_prime =tf.constant(A)\n",
    "\n",
    "\n",
    "X = np.random.uniform(-1,1, (A.shape[0], embedding_dim))\n",
    "X_prime = X\n",
    "X_prime = tf.Variable(X_prime)\n",
    "\n",
    "Y_prime = Y_final\n",
    "X_regularization_loss_list, Y_regularization_loss_list = [], []\n",
    "\n",
    "GLRM_loss_list = [{\n",
    "    \"A_start_row\": 0,\n",
    "    \"A_end_row\" : A_prime.shape[0],\n",
    "    \"A_start_col\" : 0,\n",
    "    \"A_end_col\" : A_prime.shape[1],\n",
    "\n",
    "    \"X_start_row\": 0,\n",
    "    \"X_end_row\" : X_prime.shape[0],\n",
    "    \"X_start_col\" : 0,\n",
    "    \"X_end_col\" : X_prime.shape[1],\n",
    "\n",
    "    \"Y_start_row\": 0,\n",
    "    \"Y_end_row\" : Y_prime.shape[0],\n",
    "    \"Y_start_col\" : 0,\n",
    "    \"Y_end_col\" : Y_prime.shape[1],\n",
    "    \"weight\": 1,\n",
    "    \"loss\": 'MAE'\n",
    "}]\n",
    "\n",
    "X_regularization_loss_list = [\n",
    "    {\n",
    "        \"X_start_row\": 0,\n",
    "        \"X_end_row\" : X.shape[0],\n",
    "        \"X_start_col\" : 0,\n",
    "        \"X_end_col\" : X.shape[1],\n",
    "        \"penalty_type\" : 'L2',\n",
    "        \"alpha\": 0.001\n",
    "    },\n",
    "    {\n",
    "        \"X_start_row\": 0,\n",
    "        \"X_end_row\" : X.shape[0],\n",
    "        \"X_start_col\" : 0,\n",
    "        \"X_end_col\" : X.shape[1],\n",
    "        \"penalty_type\" : 'L1',\n",
    "        \"alpha\": 0.001\n",
    "    }\n",
    "]\n",
    "\n",
    "num_iterations=50000\n",
    "learning_rate=0.001\n",
    "result_val = predict(A_prime, X_prime, Y_prime, GLRM_loss_list, X_regularization_loss_list, Y_regularization_loss_list, num_iterations, learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "9b5f6089",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7483731019522777"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_pred = np.hstack((result_val[0], A_prime))\n",
    "val_pred = np.hstack((val_pred, np.ones(shape=(val_pred.shape[0],1))))\n",
    "val_pred = np.matmul(val_pred, beta)\n",
    "val_pred = tf.nn.softmax(val_pred).numpy()\n",
    "accuracy_score(np.array(label_test)-1, np.argmax(val_pred, axis=1))\n",
    "\n",
    "# accuracy_score(label_test, val_preds)\n",
    "\n",
    "# prob_val = np.exp(prob_val)/(1+np.exp(prob_val))\n",
    "# predictions_val = [1 if x >= 0.5 else 0 for x in prob_val]\n",
    "# accuracy_score(Y_test, predictions_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "0addb1f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 12,\n",
       " 12,\n",
       " 12,\n",
       " 12,\n",
       " 12,\n",
       " 12,\n",
       " 12,\n",
       " 12,\n",
       " 12,\n",
       " 12,\n",
       " 12,\n",
       " 12,\n",
       " 12,\n",
       " 12,\n",
       " 12,\n",
       " 12,\n",
       " 12,\n",
       " 12,\n",
       " 12,\n",
       " 12,\n",
       " 12,\n",
       " 12,\n",
       " 12,\n",
       " 12,\n",
       " 12,\n",
       " 12,\n",
       " 12,\n",
       " 12,\n",
       " 12,\n",
       " 12,\n",
       " 12,\n",
       " 12,\n",
       " 12,\n",
       " 12,\n",
       " 12,\n",
       " 12,\n",
       " 12,\n",
       " 12,\n",
       " 12,\n",
       " 12,\n",
       " 12,\n",
       " 12,\n",
       " 12,\n",
       " 12,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 17,\n",
       " 17,\n",
       " 17,\n",
       " 17,\n",
       " 17,\n",
       " 17,\n",
       " 17,\n",
       " 17,\n",
       " 17,\n",
       " 17,\n",
       " 17,\n",
       " 17,\n",
       " 17,\n",
       " 17,\n",
       " 17,\n",
       " 17,\n",
       " 17,\n",
       " 17,\n",
       " 17,\n",
       " 17,\n",
       " 17,\n",
       " 17,\n",
       " 17,\n",
       " 17,\n",
       " 17,\n",
       " 17,\n",
       " 17,\n",
       " 17,\n",
       " 17,\n",
       " 17,\n",
       " 17,\n",
       " 17,\n",
       " 17,\n",
       " 17,\n",
       " 17,\n",
       " 17,\n",
       " 17,\n",
       " 17,\n",
       " 17,\n",
       " 17,\n",
       " 17,\n",
       " 17,\n",
       " 17,\n",
       " 17,\n",
       " 17,\n",
       " 17,\n",
       " 17,\n",
       " 17,\n",
       " 18,\n",
       " 18,\n",
       " 18,\n",
       " 18,\n",
       " 18,\n",
       " 18,\n",
       " 18,\n",
       " 18,\n",
       " 18,\n",
       " 18,\n",
       " 18,\n",
       " 18,\n",
       " 18,\n",
       " 18,\n",
       " 18,\n",
       " 18,\n",
       " 18,\n",
       " 18,\n",
       " 18,\n",
       " 18,\n",
       " 18,\n",
       " 18,\n",
       " 18,\n",
       " 18,\n",
       " 18,\n",
       " 18,\n",
       " 18,\n",
       " 18,\n",
       " 18,\n",
       " 18,\n",
       " 18,\n",
       " 18,\n",
       " 18,\n",
       " 18,\n",
       " 18,\n",
       " 18,\n",
       " 18,\n",
       " 18,\n",
       " 18,\n",
       " 18,\n",
       " 18,\n",
       " 18,\n",
       " 18,\n",
       " 18,\n",
       " 18,\n",
       " 18,\n",
       " 18,\n",
       " 18,\n",
       " 18,\n",
       " 19,\n",
       " 19,\n",
       " 19,\n",
       " 19,\n",
       " 19,\n",
       " 19,\n",
       " 19,\n",
       " 19,\n",
       " 19,\n",
       " 19,\n",
       " 19,\n",
       " 19,\n",
       " 19,\n",
       " 19,\n",
       " 19,\n",
       " 19,\n",
       " 19,\n",
       " 19,\n",
       " 19,\n",
       " 19,\n",
       " 19,\n",
       " 19,\n",
       " 19,\n",
       " 19,\n",
       " 19,\n",
       " 19,\n",
       " 19,\n",
       " 19,\n",
       " 19,\n",
       " 19,\n",
       " 19,\n",
       " 19,\n",
       " 19,\n",
       " 19,\n",
       " 19,\n",
       " 19,\n",
       " 19,\n",
       " 19,\n",
       " 19,\n",
       " 19,\n",
       " 19,\n",
       " 19,\n",
       " 19,\n",
       " 19,\n",
       " 19,\n",
       " 19,\n",
       " 19,\n",
       " 19,\n",
       " 19,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 21,\n",
       " 21,\n",
       " 21,\n",
       " 21,\n",
       " 21,\n",
       " 21,\n",
       " 21,\n",
       " 21,\n",
       " 21,\n",
       " 21,\n",
       " 21,\n",
       " 21,\n",
       " 21,\n",
       " 21,\n",
       " 21,\n",
       " 21,\n",
       " 21,\n",
       " 21,\n",
       " 21,\n",
       " 21,\n",
       " 21,\n",
       " 21,\n",
       " 21,\n",
       " 21,\n",
       " 21,\n",
       " 21,\n",
       " 21,\n",
       " 21,\n",
       " 21,\n",
       " 21,\n",
       " 21,\n",
       " 21,\n",
       " 21,\n",
       " 21,\n",
       " 21,\n",
       " 21,\n",
       " 21,\n",
       " 21,\n",
       " ...]"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cfbe8a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "beta.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87235c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(label_test),  val_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65aee6e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "confusion_matrix(Y_test, predictions_val), confusion_matrix(Y_train, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69c99ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "409/(409+200), 409/(409+857)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "979995fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "305/(305+133), 305/(305+961)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07698ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "2027/(2027+538), 2027/(2027+3343)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a5846e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "X_train_embedding = result[0][:, 1:embedding_dim+1]\n",
    "X_train_embedding = X_train_embedding.numpy()\n",
    "\n",
    "X_test_embedding = result_val[0].numpy()\n",
    "\n",
    "\n",
    "clf = LogisticRegression().fit(X_train_embedding, Y_train)\n",
    "pred_logistic = clf.predict(X_train_embedding)\n",
    "pred_logistic_val = clf.predict(X_test_embedding)\n",
    "accuracy_score(Y_train, pred_logistic), accuracy_score(Y_test, pred_logistic_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21d387d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(Y_test, pred_logistic_val), confusion_matrix(Y_train, pred_logistic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f17149e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "328/(328+236), 328/(328+938)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aedaa965",
   "metadata": {},
   "outputs": [],
   "source": [
    "1565/(1565+650), 1565/(1565+3805)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9377afd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "beta[5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6461d6e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1db71d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "A_prime[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17cabbd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c98a3071",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "prob_val[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9bc4471",
   "metadata": {},
   "outputs": [],
   "source": [
    "prob[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4508e482",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = tf.Variable([1,1,1])\n",
    "tf.where(temp!=1).shape[0]\n",
    "# tf.where([True, False, False, True]).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6dcba4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.min(val_pred,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ddcd2bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.min(result[0],axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2100c827",
   "metadata": {},
   "outputs": [],
   "source": [
    "beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ada90a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "result[0][3:10,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "e356869c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3,), dtype=float32, numpy=array([0.09003057, 0.24472848, 0.665241  ], dtype=float32)>"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_info = Y_regularization_loss_list[2]\n",
    "\n",
    "# A_00, A_10, A_01, A_11 = loss_info['A_start_row'], loss_info['A_end_row'], loss_info['A_start_col'], loss_info['A_end_col']\n",
    "X_00, X_10, X_01, X_11 = loss_info['X_start_row'], loss_info['X_end_row'], loss_info['X_start_col'], loss_info['X_end_col']\n",
    "# Y_00, Y_10, Y_01, Y_11 = loss_info['Y_start_row'], loss_info['Y_end_row'], loss_info['Y_start_col'], loss_info['Y_end_col']\n",
    "\n",
    "\n",
    "# A_prime[A_00:A_10, A_01:A_11]\n",
    "# X_prime[X_00:X_10, X_01:X_11]\n",
    "# Y_prime[Y_00:Y_10, Y_01:Y_11]\n",
    "Y_prime[X_00:X_10, X_01:X_11]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6d61232",
   "metadata": {},
   "outputs": [],
   "source": [
    "b = 1.\n",
    "a = 0.\n",
    "current_min = 1.\n",
    "current_max = 38.\n",
    "\n",
    "def scale_class(val):\n",
    "    return (b-a)*(val-current_min)/(current_max-current_min)\n",
    "\n",
    "def scale_inv(val):\n",
    "    return val*(current_max-current_min) / (b-a) +current_min\n",
    "label_train_arr = np.array(label_train).reshape(-1,1)\n",
    "vfunc = np.vectorize(scale_class)\n",
    "label_train_arr_scaled = vfunc(label_train_arr)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
