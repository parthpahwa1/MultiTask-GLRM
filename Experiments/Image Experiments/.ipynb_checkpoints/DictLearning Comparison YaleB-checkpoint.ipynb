{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8541f4bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from importlib import reload\n",
    "import scipy.io as sio\n",
    "import pkg_resources\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7d62e23c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "module_path = os.path.abspath(os.path.join('../../'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "import MultiLearn_GLRM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9c7ac73d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_mat(filename):\n",
    "    return sio.loadmat(filename)\n",
    "\n",
    "def label_to_range(label):\n",
    "    \"\"\"\n",
    "    Convert label to range\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    label: list of integers\n",
    "        must be in the form of [1, 1, ..., 1, 2, 2, ..., 2, ..., C, C, ..., C]\n",
    "        i.e. nondecreasing numbers starting from 1, each element is greater\n",
    "        than the previous element by at most 1\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    a list of intergers with C + 1 elements, start with 0\n",
    "    the i-th element is number of elements in label that equals to i\n",
    "        \n",
    "    \"\"\"\n",
    "    res = [0]\n",
    "    assert label[0] == 1, 'label must start with 1'\n",
    "    for i in range(1, len(label)):\n",
    "        if label[i] == label[i-1]:\n",
    "            continue\n",
    "        if label[i] == label[i-1] + 1:\n",
    "            res.append(i)\n",
    "        else:\n",
    "            assert False,\\\n",
    "                ('label[{}] and label[{}] must be equal or two consecutive '\n",
    "                 'integers, got {} and {}').format(\n",
    "                     i-1, i, label[i-1], label[i]\n",
    "                 )\n",
    "    res.append(len(label))\n",
    "    return res\n",
    "\n",
    "def get_block_row(matrix, block_indices, row_range):\n",
    "    \"\"\"\n",
    "    Extract a subset of rows from a matrix\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    matrix: 2-d numpy array\n",
    "        block matrix\n",
    "    block_indices: integer of list of integers\n",
    "        indices of extracted blocks, 0-indexed. If indices is a list, return\n",
    "        the concatenation of all blocks\n",
    "    row_range: list of intergers\n",
    "        in the form of [0, c_1, c_1 + c_2, ..., c_1 + c_2 + ... + c_N]\n",
    "        where c_i is the number of rows in the i-th block\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    a 2-d matrix\n",
    "    \"\"\"\n",
    "    assert matrix.ndim == 2, 'Expect to receive 2-d array input, got shape {}'.format(matrix.shape)\n",
    "    if isinstance(block_indices, int):\n",
    "        block_indices = [block_indices]\n",
    "    # if isinstance(block_indices, (list, np.ndarray, np.generic))\n",
    "    ids = []\n",
    "    for i in block_indices:\n",
    "        ids = ids + list(range(row_range[i], row_range[i+1]))\n",
    "    return matrix[ids, :].copy()\n",
    "\n",
    "def get_block_col(matrix, block_indices, col_range):\n",
    "    \"\"\"\n",
    "    Extract a subset of columns from a matrix\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    matrix: 2-d numpy array\n",
    "        block matrix\n",
    "    block_indices: integer of list of integers\n",
    "        indices of extracted blocks, 1-indexed. If indices is a list, return\n",
    "        the concatenation of all blocks\n",
    "    row_range: list of intergers\n",
    "        in the form of [0, c_1, c_1 + c_2, ..., c_1 + c_2 + ... + c_N]\n",
    "        where c_i is the number of columns in the i-th block\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    a 2-d matrix\n",
    "    \"\"\"\n",
    "    assert matrix.ndim == 2, 'Expect to receive 2-d array input, got shape {}'.format(matrix.shape)\n",
    "    assert matrix.shape[1] == col_range[-1]\n",
    "    return get_block_row(matrix.T, block_indices, col_range).T\n",
    "\n",
    "def get_block_col(matrix, block_indices, col_range):\n",
    "    \"\"\"\n",
    "    Extract a subset of columns from a matrix\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    matrix: 2-d numpy array\n",
    "        block matrix\n",
    "    block_indices: integer of list of integers\n",
    "        indices of extracted blocks, 1-indexed. If indices is a list, return\n",
    "        the concatenation of all blocks\n",
    "    row_range: list of intergers\n",
    "        in the form of [0, c_1, c_1 + c_2, ..., c_1 + c_2 + ... + c_N]\n",
    "        where c_i is the number of columns in the i-th block\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    a 2-d matrix\n",
    "    \"\"\"\n",
    "    assert matrix.ndim == 2, 'Expect to receive 2-d array input, got shape {}'.format(matrix.shape)\n",
    "    assert matrix.shape[1] == col_range[-1]\n",
    "    return get_block_row(matrix.T, block_indices, col_range).T\n",
    "\n",
    "def randperm(n):\n",
    "    \"\"\"\n",
    "    get a random permutation of range(n)\n",
    "    \"\"\"\n",
    "    return np.random.permutation(list(range(n)))\n",
    "\n",
    "def normc(A):\n",
    "    \"\"\"\n",
    "    normalize each column of A to have norm2 = 1\n",
    "    \"\"\"\n",
    "    return A / np.tile(np.sqrt(np.sum(A*A, axis=0)), (A.shape[0], 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "e8119194",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_fn = pkg_resources.resource_filename('data/myYaleB' + '.mat')\n",
    "vars_dict = load_mat('./DICTOL_python-master/dictol/data/myYaleB' + '.mat')\n",
    "\n",
    "N_train_c = 15\n",
    "\n",
    "Y = vars_dict['Y']\n",
    "d = Y.shape[0]\n",
    "if 'Y_range' not in vars_dict:\n",
    "    Y_range = label_to_range(vars_dict['label'].flatten(1)).astype(int)\n",
    "\n",
    "else:\n",
    "    Y_range = vars_dict['Y_range'].flatten('C').astype(int)\n",
    "\n",
    "C = Y_range.size - 1\n",
    "N_total     = Y_range[-1]\n",
    "N_train     = C*N_train_c\n",
    "N_test      = N_total - N_train\n",
    "\n",
    "Y_train     = np.zeros((d, N_train))\n",
    "Y_test      = np.zeros((d, N_test))\n",
    "label_train = [0]*N_train\n",
    "label_test = [0]*N_test\n",
    "cur_train   = 0\n",
    "cur_test    = 0\n",
    "for c in range(C):\n",
    "    Yc        = get_block_col(Y, c, Y_range)\n",
    "    N_total_c = Yc.shape[1]\n",
    "    N_test_c  = N_total_c - N_train_c\n",
    "    label_train[cur_train: cur_train + N_train_c] = [c+1]*N_train_c\n",
    "    label_test[cur_test:cur_test + N_test_c] = [c+1]*N_test_c\n",
    "\n",
    "    ids = randperm(N_total_c)\n",
    "\n",
    "    Y_train[:, cur_train: cur_train + N_train_c] = \\\n",
    "        Yc[:, np.sort(ids[:N_train_c])]\n",
    "\n",
    "    Y_test[:, cur_test: cur_test + N_test_c] = \\\n",
    "        Yc[:, np.sort(ids[N_train_c:])]\n",
    "\n",
    "    cur_train += N_train_c\n",
    "    cur_test += N_test_c\n",
    "\n",
    "Y_train = normc(Y_train)\n",
    "Y_test  = normc(Y_test)\n",
    "\n",
    "params = {\n",
    "    \"train_hyper_params\": None,\n",
    "    \"test_hyper_params\": None,\n",
    "    \"beta\": None,\n",
    "    \"embedding_matrix\": None,\n",
    "    \"train_error\": None,\n",
    "    \"test_error\": None,\n",
    "    \"embedding_dim\": None,\n",
    "    \"predictor_scaling_parmas\": None,\n",
    "    \"target_scaling_parmas\": None\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "0d2b722a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0015418425233250197"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(TRAIN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "9441941b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(542,), dtype=float64, numpy=\n",
       "array([ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  1.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  1.12270277e-02, -2.46604066e-02,\n",
       "        4.70339323e-02, -7.44765032e-02, -3.32322438e-02, -4.03381317e-02,\n",
       "       -1.21988572e-02, -1.59314676e-03,  3.03640297e-02,  1.67805817e-02,\n",
       "       -4.64715527e-02,  4.63147670e-03, -1.59070159e-02, -5.56375785e-03,\n",
       "        1.76238254e-02, -7.74105521e-03, -1.81363192e-03,  7.75840545e-02,\n",
       "        4.06391377e-03,  4.64863700e-02, -2.97242599e-02,  2.78664144e-02,\n",
       "        4.70576120e-02, -1.86788888e-02, -7.52117139e-04, -1.22356849e-02,\n",
       "       -4.42039666e-02, -2.47331391e-02, -4.78399324e-02,  1.97091716e-02,\n",
       "        2.52301108e-02,  2.56053322e-02,  3.16689636e-02, -2.62323439e-02,\n",
       "        5.57762645e-02, -3.24736863e-02,  5.17026712e-04, -1.45953750e-02,\n",
       "        1.24175975e-02, -5.49454497e-02, -3.36306700e-02,  7.36882876e-02,\n",
       "        4.93798807e-02,  1.63709510e-02, -8.48519040e-03,  4.17222873e-02,\n",
       "        6.59884442e-03, -1.92977008e-02,  7.34882912e-03, -1.19722107e-01,\n",
       "        3.67764854e-02,  3.86224107e-02, -3.47498616e-02,  5.22349262e-02,\n",
       "        1.14760259e-02,  9.91723838e-02,  4.07995540e-02, -2.85466400e-02,\n",
       "        9.05446174e-03,  8.31967931e-02, -2.07817553e-02, -1.06923744e-02,\n",
       "        2.57413733e-02,  7.79128762e-02,  1.95051641e-02, -1.56643884e-02,\n",
       "       -4.65729501e-02,  1.15783912e-01, -5.59522224e-02, -6.60315180e-03,\n",
       "       -4.04511867e-02, -7.26965632e-02,  3.99050720e-02, -2.64847510e-02,\n",
       "        1.14320848e-02,  3.63073016e-02,  6.60732776e-02,  7.59767113e-03,\n",
       "       -2.63334939e-02,  1.40475446e-02, -4.18719243e-03,  3.84347446e-03,\n",
       "        8.81233492e-02, -1.82844214e-02,  2.99451790e-02, -6.00380344e-02,\n",
       "       -2.84158668e-03,  1.43638755e-02,  6.30904255e-02, -1.13385403e-02,\n",
       "       -3.44042031e-02,  6.36272601e-02, -5.18064240e-02,  4.83354691e-02,\n",
       "        8.17826694e-02,  7.28773503e-02, -6.12774971e-03,  6.94402192e-02,\n",
       "       -5.17705011e-02, -1.19972419e-02, -7.04429690e-02, -2.03261921e-02,\n",
       "       -7.24743888e-02, -7.11610678e-03, -3.68629066e-02, -2.52218051e-02,\n",
       "       -5.44419432e-02,  1.16064443e-01,  1.94217538e-02, -7.06762505e-02,\n",
       "        2.30126911e-02, -6.53402116e-04, -6.20023714e-02,  7.97144033e-03,\n",
       "       -5.10978552e-02,  5.56098248e-02, -1.54928722e-03, -1.22581557e-01,\n",
       "        1.89371212e-02,  1.00313094e-02, -2.32204565e-02, -8.33065644e-02,\n",
       "       -4.29539946e-02, -5.79441592e-03,  1.56737037e-02, -1.58957337e-02,\n",
       "       -8.74915420e-02, -5.12439739e-02,  5.47896078e-02,  1.24419841e-02,\n",
       "       -6.85630330e-02,  4.02500379e-03,  3.45239974e-02, -4.32505381e-02,\n",
       "       -7.37636212e-02, -7.87286794e-02, -5.72994650e-02, -1.35129667e-02,\n",
       "       -2.21230763e-03,  1.34356601e-02,  2.57237191e-02, -8.58323122e-03,\n",
       "       -2.99731239e-02,  2.74893426e-02, -3.39693100e-02, -1.18402743e-02,\n",
       "        2.46144980e-02,  2.70466457e-02, -9.79767380e-05,  7.90393516e-03,\n",
       "       -4.31812192e-02,  6.40434786e-02, -7.74170464e-02, -3.41331208e-02,\n",
       "       -8.39486079e-02,  2.98850590e-02, -9.40895586e-02,  9.70937919e-02,\n",
       "        3.55302056e-02,  1.16174254e-02,  1.52785760e-02, -1.89323350e-02,\n",
       "       -2.26514715e-02, -3.94407900e-03, -4.77341291e-02, -3.08753083e-02,\n",
       "        3.35837440e-02,  4.19016085e-02, -5.66160353e-02,  1.51645382e-02,\n",
       "        2.91516059e-02, -6.74410074e-03,  8.02785925e-02,  1.73675222e-02,\n",
       "        4.90699340e-04, -6.08122346e-03, -9.04595048e-03,  9.63371912e-03,\n",
       "        5.59575012e-02, -2.87679314e-02,  4.00684150e-03,  1.09356108e-02,\n",
       "        1.57249779e-02, -4.91525832e-02,  5.61954422e-03, -2.28332944e-02,\n",
       "        3.09825620e-02, -1.16085970e-02,  3.00726971e-02, -4.48177352e-02,\n",
       "       -4.20712117e-02,  9.09128739e-02, -1.62668407e-03, -3.60819474e-03,\n",
       "       -4.91768796e-02, -4.19579296e-02, -1.25445478e-02, -2.37526550e-02,\n",
       "        7.01297939e-02, -4.37329492e-02,  1.42945929e-02, -1.87008505e-02,\n",
       "        4.48144045e-02, -4.59054086e-02,  3.38366566e-02, -3.06552505e-02,\n",
       "        1.65252740e-02, -1.66209951e-02, -6.38524555e-02, -5.26203844e-02,\n",
       "       -1.03367046e-01, -5.81473582e-02,  2.80489426e-02,  5.69832539e-04,\n",
       "       -1.12343596e-01,  3.72242533e-02, -7.97257390e-03,  2.32667501e-02,\n",
       "        7.88753213e-03,  5.47440639e-02, -1.50856028e-02, -6.04874725e-02,\n",
       "       -6.24722150e-02,  6.91974735e-03,  2.86264764e-02,  6.55474377e-03,\n",
       "       -1.85985268e-02, -9.98539390e-02,  2.14043201e-02,  5.94670679e-02,\n",
       "       -3.78213012e-02,  2.82098332e-02,  3.02462677e-02,  3.20103635e-02,\n",
       "       -5.61390085e-03, -3.04411809e-02, -1.57892765e-02,  1.41794344e-02,\n",
       "       -1.58853038e-02, -9.78038925e-02,  1.26123516e-02,  7.57020486e-02,\n",
       "       -9.44151598e-02,  6.45493179e-02, -9.48476004e-03, -2.55287909e-02,\n",
       "        1.10264173e-02, -3.54793434e-02,  2.03315415e-02, -1.26932187e-02,\n",
       "        3.20108545e-02, -9.20502126e-02, -3.39110253e-02, -3.55191149e-02,\n",
       "       -2.86413390e-03, -1.50695308e-02,  4.04002434e-02, -2.43525073e-02,\n",
       "        3.95953367e-02,  1.17200556e-02, -7.38452021e-03,  1.58346693e-02,\n",
       "       -5.08321898e-02, -2.12378705e-02,  2.88797977e-02,  4.05717428e-02,\n",
       "        2.30615496e-03, -4.66167658e-02, -2.01133063e-02,  4.20740301e-02,\n",
       "       -1.06740552e-02, -3.67550630e-02, -4.59563744e-02, -1.31442061e-02,\n",
       "        4.34839910e-03,  6.36150702e-04,  4.57729780e-02, -5.64840483e-03,\n",
       "       -6.56115626e-02,  9.34315719e-03, -7.11475365e-02,  2.62324993e-02,\n",
       "        2.43308771e-02,  1.65911551e-02, -4.05279168e-02,  5.99062206e-03,\n",
       "       -6.43282011e-02, -3.35613078e-02,  5.27926777e-02,  6.91456524e-03,\n",
       "        5.47205459e-02,  5.54092822e-02, -4.78205912e-04, -1.03793461e-02,\n",
       "       -2.72220317e-02,  2.93905941e-02,  1.10276946e-01, -3.92123395e-02,\n",
       "        2.52234931e-03, -3.02180748e-02,  5.92094380e-03,  6.14193783e-02,\n",
       "        7.56932143e-02,  2.58664013e-02, -7.13838445e-02,  7.17903362e-03,\n",
       "        6.01497722e-02,  1.50854424e-03,  1.36619621e-02,  1.19431623e-03,\n",
       "       -5.29258908e-02, -6.02907615e-02, -6.59291956e-02, -3.24931552e-02,\n",
       "       -8.54553728e-02,  4.75669851e-02, -1.88552339e-02, -5.99480064e-02,\n",
       "        7.04186339e-02, -3.00731889e-02, -8.97076866e-02,  9.73484867e-03,\n",
       "        2.02759204e-02, -4.90340518e-02,  2.19687195e-02,  4.32652808e-02,\n",
       "        2.03445232e-02, -1.63275872e-02, -3.37532802e-02,  5.34904494e-02,\n",
       "       -1.10282630e-01,  5.32863540e-02, -5.42019960e-02, -5.35269581e-02,\n",
       "       -1.60611265e-02,  4.45838443e-02,  1.18485889e-01,  5.43073120e-02,\n",
       "       -3.40444630e-02,  4.45293981e-02,  2.38025847e-02, -1.40529085e-02,\n",
       "        8.69377507e-03,  3.61604896e-02, -1.27508955e-03,  3.35874996e-02,\n",
       "       -1.07188398e-01, -4.21459274e-02,  3.71796265e-02,  6.84385417e-02,\n",
       "        5.29141981e-02,  1.25005281e-02,  4.56880733e-02,  8.59059942e-02,\n",
       "       -3.10381628e-02, -7.05557503e-02,  2.85545727e-02, -4.14218795e-02,\n",
       "       -3.10048846e-02, -3.23209223e-02, -5.29301642e-03, -1.37566780e-03,\n",
       "        8.70092470e-02,  4.69419245e-03, -3.50340070e-02,  1.39469411e-02,\n",
       "        3.40771173e-02,  8.11869904e-02, -1.44261855e-02, -4.05896462e-02,\n",
       "       -4.21302321e-02,  6.63393739e-03,  4.76397387e-03, -2.19376162e-02,\n",
       "        9.71421395e-03,  3.16972937e-02,  5.32974530e-03,  7.56164118e-02,\n",
       "       -6.02987190e-03,  4.70808125e-02,  2.59037026e-02, -1.24567840e-01,\n",
       "        4.49611278e-02, -7.47093317e-02, -2.85404220e-02,  3.17671523e-02,\n",
       "       -1.69871436e-02,  1.30223459e-02,  1.30177793e-01, -3.33277973e-03,\n",
       "       -7.18300194e-02,  1.00753358e-01,  3.92638442e-02,  1.13152775e-02,\n",
       "        1.31326295e-02, -6.49563706e-02, -1.28842552e-02,  2.48008741e-02,\n",
       "       -3.09315156e-02,  3.01146211e-02, -1.12762478e-01,  7.24977105e-02,\n",
       "        6.33204592e-03, -5.36451363e-02, -8.35680393e-02,  2.79404200e-02,\n",
       "        1.53356696e-02,  4.65941000e-02, -3.76593329e-02, -6.22967665e-02,\n",
       "       -3.11822800e-03, -4.16319227e-02,  2.15441500e-02, -6.75321723e-02,\n",
       "       -2.17900311e-03, -6.79827364e-02,  6.98302952e-02, -1.97467654e-02,\n",
       "       -4.03726102e-02, -2.32334046e-02,  4.06165902e-02, -8.38858855e-02,\n",
       "       -1.23055868e-02,  1.82379420e-03,  2.15105723e-02, -9.07211656e-03,\n",
       "        5.20533277e-03, -3.83400448e-02,  5.47297723e-03,  1.44789874e-02,\n",
       "       -1.06713706e-02,  6.91158144e-02,  3.69575400e-02, -1.20113683e-02,\n",
       "       -1.07578541e-02,  2.94117183e-02, -1.39546799e-02,  6.65553827e-03,\n",
       "        7.27336206e-03, -1.36782154e-02,  2.76536305e-02, -7.49970827e-02,\n",
       "        2.50655256e-02, -5.58966021e-02,  2.20987405e-02,  1.89612212e-02,\n",
       "       -5.28593907e-02, -5.38957850e-02, -5.04497585e-02,  8.65741253e-03,\n",
       "       -2.60025922e-02, -1.08397318e-01, -1.03550522e-01,  3.88938981e-02,\n",
       "        6.98787511e-03, -6.24327470e-02, -1.08805807e-04, -1.61116119e-02,\n",
       "        1.51717074e-02,  4.74558107e-02,  4.15740999e-02, -2.90230204e-02,\n",
       "       -4.05974452e-02,  1.88600628e-02, -4.53665049e-02, -1.11549143e-02,\n",
       "       -1.81143918e-02, -1.31193934e-03,  2.95092124e-02, -3.88092614e-02,\n",
       "        1.86186813e-02, -2.35386876e-02,  5.19480692e-02, -5.87815872e-02,\n",
       "        1.57690476e-02, -1.60220386e-02,  2.46737105e-02, -5.08351938e-02,\n",
       "        9.99992921e-03, -3.40426641e-02, -4.08622744e-02, -4.06604142e-03,\n",
       "       -5.53813078e-03, -4.10899027e-02,  1.49125624e-02, -1.17365733e-02,\n",
       "       -7.90915571e-03, -1.88083948e-02,  1.16238225e-02,  8.89175274e-02,\n",
       "       -4.87426571e-02, -3.79703355e-02,  1.17424029e-02,  2.83633456e-02,\n",
       "        1.20943760e-02,  2.05973558e-02,  2.05574660e-02,  3.27271440e-02,\n",
       "       -7.53651833e-03, -4.64288734e-02,  8.63568922e-03, -7.80562046e-02,\n",
       "       -1.57162452e-02,  2.08342513e-02])>"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A_prime[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "684a69e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(570, 1844, (504, 570), (504, 1844))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(label_train), len(label_test), Y_train.shape, Y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "ab34ad3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_classes = max(max(label_train), max(label_test))\n",
    "label_train\n",
    "one_hot_targets = np.eye(nb_classes)[np.array(label_train)-1]\n",
    "TRAIN = np.hstack((Y_train.T, one_hot_targets))\n",
    "np.random.shuffle(TRAIN)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "917d0cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle(A_prime, X_prime, Y_prime, n_class, embedding_dim):\n",
    "#     A_new = tf.Variable(A_prime)\n",
    "#     X_new = tf.Variable(X_prime)\n",
    "    indices = tf.range(start=0, limit=tf.shape(A_prime)[0], dtype=tf.int64)\n",
    "    \n",
    "    \n",
    "    shuffled_indices = tf.random.shuffle(indices)\n",
    "    print(shuffled_indices)\n",
    "    A_new = tf.gather(A_prime, shuffled_indices)\n",
    "    X_new = tf.Variable(tf.gather(tf.constant(X_prime), shuffled_indices))\n",
    "    \n",
    "    return A_new, X_new, Y_prime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "7ca81bab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([151 102 339 ... 771 119 472], shape=(1844,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "A_new, X_new, Y_prime = shuffle(A_prime, X_prime, Y_prime, n_class, embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "96597f86",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0\n",
      "iter: 0 Total loss: tf.Tensor(3361.4404081793214, shape=(), dtype=float64) [7.477485220714377, 98.61724484134783]\n",
      "iter: 50 Total loss: tf.Tensor(2511.7151507109206, shape=(), dtype=float64)\n",
      "iter: 0 Total loss: tf.Tensor(2499.3644394844864, shape=(), dtype=float64)\n",
      "iter: 83 Total loss: tf.Tensor(964.0002518895806, shape=(), dtype=float64)\n",
      "epoch: 1\n",
      "iter: 0 Total loss: tf.Tensor(959.3454328893148, shape=(), dtype=float64) [5.122064910688501, 3.1854881728866093]\n",
      "iter: 100 Total loss: tf.Tensor(389.0265958855387, shape=(), dtype=float64) [3.5217422028721583, 0.7565578505882475]\n",
      "iter: 141 Total loss: tf.Tensor(303.7649796206168, shape=(), dtype=float64)\n",
      "iter: 0 Total loss: tf.Tensor(302.16480819159034, shape=(), dtype=float64)\n",
      "iter: 100 Total loss: tf.Tensor(117.53886740519924, shape=(), dtype=float64)\n",
      "iter: 200 Total loss: tf.Tensor(115.62090582130686, shape=(), dtype=float64)\n",
      "epoch: 2\n",
      "iter: 0 Total loss: tf.Tensor(115.41031378818718, shape=(), dtype=float64) [0.009611994657220176, 0.0019960969590797346]\n",
      "iter: 3 Total loss: tf.Tensor(111.2063655766391, shape=(), dtype=float64)\n",
      "iter: 0 Total loss: tf.Tensor(108.50044736468818, shape=(), dtype=float64)\n",
      "iter: 100 Total loss: tf.Tensor(107.28262102022799, shape=(), dtype=float64)\n",
      "iter: 200 Total loss: tf.Tensor(107.333634004452, shape=(), dtype=float64)\n",
      "epoch: 3\n",
      "iter: 0 Total loss: tf.Tensor(107.29181601121975, shape=(), dtype=float64) [-5.963695089062155e-05, 0.0019842601059608185]\n",
      "iter: 1 Total loss: tf.Tensor(107.70460905022682, shape=(), dtype=float64)\n",
      "iter: 0 Total loss: tf.Tensor(104.92732066590895, shape=(), dtype=float64)\n",
      "iter: 100 Total loss: tf.Tensor(104.79381046752164, shape=(), dtype=float64)\n",
      "iter: 200 Total loss: tf.Tensor(104.88252065912832, shape=(), dtype=float64)\n",
      "epoch: 4\n",
      "iter: 0 Total loss: tf.Tensor(104.86854328247588, shape=(), dtype=float64) [-5.9636953908545865e-05, 0.0019842531964099175]\n",
      "iter: 1 Total loss: tf.Tensor(104.44928540446895, shape=(), dtype=float64)\n",
      "iter: 0 Total loss: tf.Tensor(102.29663911893836, shape=(), dtype=float64)\n",
      "iter: 100 Total loss: tf.Tensor(102.1630952937777, shape=(), dtype=float64)\n",
      "iter: 200 Total loss: tf.Tensor(102.2304388792083, shape=(), dtype=float64)\n",
      "epoch: 5\n",
      "iter: 0 Total loss: tf.Tensor(102.23079595139225, shape=(), dtype=float64) [-5.9636953908545865e-05, 0.001984419791552617]\n",
      "iter: 1 Total loss: tf.Tensor(102.59668779717134, shape=(), dtype=float64)\n",
      "iter: 0 Total loss: tf.Tensor(100.84107964899589, shape=(), dtype=float64)\n",
      "iter: 100 Total loss: tf.Tensor(100.73406885308599, shape=(), dtype=float64)\n",
      "iter: 200 Total loss: tf.Tensor(100.77873143500784, shape=(), dtype=float64)\n",
      "epoch: 6\n",
      "iter: 0 Total loss: tf.Tensor(100.7867427793668, shape=(), dtype=float64) [-5.9636953908545865e-05, 0.0019842850426884345]\n",
      "iter: 1 Total loss: tf.Tensor(100.49138475164288, shape=(), dtype=float64)\n",
      "iter: 0 Total loss: tf.Tensor(99.17851362276207, shape=(), dtype=float64)\n",
      "iter: 100 Total loss: tf.Tensor(99.09345891475994, shape=(), dtype=float64)\n",
      "iter: 200 Total loss: tf.Tensor(99.12981222070314, shape=(), dtype=float64)\n",
      "epoch: 7\n",
      "iter: 0 Total loss: tf.Tensor(99.13317655532286, shape=(), dtype=float64) [-5.9636953908545865e-05, 0.0019842500454494236]\n",
      "iter: 1 Total loss: tf.Tensor(99.33849189354964, shape=(), dtype=float64)\n",
      "iter: 0 Total loss: tf.Tensor(98.2315269096165, shape=(), dtype=float64)\n",
      "iter: 100 Total loss: tf.Tensor(98.16568179997178, shape=(), dtype=float64)\n",
      "iter: 200 Total loss: tf.Tensor(98.18902979196926, shape=(), dtype=float64)\n",
      "epoch: 8\n",
      "iter: 0 Total loss: tf.Tensor(98.19286282908263, shape=(), dtype=float64) [-5.9636953908545865e-05, 0.0019841729110586246]\n",
      "iter: 1 Total loss: tf.Tensor(98.04724866793987, shape=(), dtype=float64)\n",
      "iter: 0 Total loss: tf.Tensor(97.18140646083087, shape=(), dtype=float64)\n",
      "iter: 100 Total loss: tf.Tensor(97.13345375253394, shape=(), dtype=float64)\n",
      "iter: 200 Total loss: tf.Tensor(97.15291471252588, shape=(), dtype=float64)\n",
      "epoch: 9\n",
      "iter: 0 Total loss: tf.Tensor(97.15013974301569, shape=(), dtype=float64) [-5.9636953908545865e-05, 0.0019841912010253155]\n",
      "iter: 1 Total loss: tf.Tensor(97.3183334692134, shape=(), dtype=float64)\n",
      "iter: 0 Total loss: tf.Tensor(96.61006386746455, shape=(), dtype=float64)\n",
      "iter: 100 Total loss: tf.Tensor(96.57590807607072, shape=(), dtype=float64)\n",
      "iter: 200 Total loss: tf.Tensor(96.59591945018676, shape=(), dtype=float64)\n",
      "Final loss: tf.Tensor(98.0132651065891, shape=(), dtype=float64) best loss: tf.Tensor(98.0132651065891, shape=(), dtype=float64)\n"
     ]
    }
   ],
   "source": [
    "reload(MultiLearn_GLRM)\n",
    "from MultiLearn_GLRM import Multi_Learn, generate_AXY\n",
    "from MultiLearn_GLRM.Params.semisupervised_params import Multilearn_GLRM_Semisupervised_Train_Params, Multilearn_GLRM_Semisupervised_Test_Params\n",
    "\n",
    "embedding_dim = 100\n",
    "params[\"embedding_dim\"] = embedding_dim\n",
    "n_class = nb_classes\n",
    "\n",
    "A_prime, X_prime, Y_prime = generate_AXY.get_semisupervised_glrm_train_form(TRAIN[:,:-nb_classes], TRAIN[:,-nb_classes:], n_class, embedding_dim, init_distribution='std_normal')\n",
    "[GLRM_loss_list, X_regulariation_list, Y_regulariation_list, X_grad_restrictions, Y_grad_restrictions] = Multilearn_GLRM_Semisupervised_Train_Params(A_prime, X_prime, Y_prime, n_class)\n",
    "\n",
    "params[\"train_hyper_params\"] = [GLRM_loss_list, X_regulariation_list, Y_regulariation_list]\n",
    "\n",
    "\n",
    "num_iterations=300\n",
    "learning_rate=0.01\n",
    "for i in range(0, 1):\n",
    "    result = Multi_Learn.alternating_minimization(A_prime, X_prime, Y_prime, GLRM_loss_list, X_regulariation_list, Y_regulariation_list, X_grad_restrictions, Y_grad_restrictions, num_iterations, learning_rate, n_class)\n",
    "#     A_prime, X_prime, Y_prime = shuffle(A_prime, X_prime, Y_prime, n_class, embedding_dim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "3465848d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.02631578947368421"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "beta = result[1][1:,0:n_class]\n",
    "Y_final = result[1][1:embedding_dim+1,n_class:]\n",
    "params[\"beta\"] = beta.numpy().tolist()\n",
    "params[\"embedding_matrix\"] = Y_final.numpy().tolist()\n",
    "\n",
    "train_prob = tf.nn.softmax(np.matmul(result[0], result[1][:,0:n_class])).numpy()\n",
    "label_train_shuffled =  np.argmax(TRAIN[:,-nb_classes:], axis=1)\n",
    "params[\"train_accuracy\"] = accuracy_score(label_train_shuffled, np.argmax(train_prob, axis=1))\n",
    "params[\"train_accuracy\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "92fd283e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(542,), dtype=float64, numpy=\n",
       "array([ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  1.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  1.12270277e-02, -2.46604066e-02,\n",
       "        4.70339323e-02, -7.44765032e-02, -3.32322438e-02, -4.03381317e-02,\n",
       "       -1.21988572e-02, -1.59314676e-03,  3.03640297e-02,  1.67805817e-02,\n",
       "       -4.64715527e-02,  4.63147670e-03, -1.59070159e-02, -5.56375785e-03,\n",
       "        1.76238254e-02, -7.74105521e-03, -1.81363192e-03,  7.75840545e-02,\n",
       "        4.06391377e-03,  4.64863700e-02, -2.97242599e-02,  2.78664144e-02,\n",
       "        4.70576120e-02, -1.86788888e-02, -7.52117139e-04, -1.22356849e-02,\n",
       "       -4.42039666e-02, -2.47331391e-02, -4.78399324e-02,  1.97091716e-02,\n",
       "        2.52301108e-02,  2.56053322e-02,  3.16689636e-02, -2.62323439e-02,\n",
       "        5.57762645e-02, -3.24736863e-02,  5.17026712e-04, -1.45953750e-02,\n",
       "        1.24175975e-02, -5.49454497e-02, -3.36306700e-02,  7.36882876e-02,\n",
       "        4.93798807e-02,  1.63709510e-02, -8.48519040e-03,  4.17222873e-02,\n",
       "        6.59884442e-03, -1.92977008e-02,  7.34882912e-03, -1.19722107e-01,\n",
       "        3.67764854e-02,  3.86224107e-02, -3.47498616e-02,  5.22349262e-02,\n",
       "        1.14760259e-02,  9.91723838e-02,  4.07995540e-02, -2.85466400e-02,\n",
       "        9.05446174e-03,  8.31967931e-02, -2.07817553e-02, -1.06923744e-02,\n",
       "        2.57413733e-02,  7.79128762e-02,  1.95051641e-02, -1.56643884e-02,\n",
       "       -4.65729501e-02,  1.15783912e-01, -5.59522224e-02, -6.60315180e-03,\n",
       "       -4.04511867e-02, -7.26965632e-02,  3.99050720e-02, -2.64847510e-02,\n",
       "        1.14320848e-02,  3.63073016e-02,  6.60732776e-02,  7.59767113e-03,\n",
       "       -2.63334939e-02,  1.40475446e-02, -4.18719243e-03,  3.84347446e-03,\n",
       "        8.81233492e-02, -1.82844214e-02,  2.99451790e-02, -6.00380344e-02,\n",
       "       -2.84158668e-03,  1.43638755e-02,  6.30904255e-02, -1.13385403e-02,\n",
       "       -3.44042031e-02,  6.36272601e-02, -5.18064240e-02,  4.83354691e-02,\n",
       "        8.17826694e-02,  7.28773503e-02, -6.12774971e-03,  6.94402192e-02,\n",
       "       -5.17705011e-02, -1.19972419e-02, -7.04429690e-02, -2.03261921e-02,\n",
       "       -7.24743888e-02, -7.11610678e-03, -3.68629066e-02, -2.52218051e-02,\n",
       "       -5.44419432e-02,  1.16064443e-01,  1.94217538e-02, -7.06762505e-02,\n",
       "        2.30126911e-02, -6.53402116e-04, -6.20023714e-02,  7.97144033e-03,\n",
       "       -5.10978552e-02,  5.56098248e-02, -1.54928722e-03, -1.22581557e-01,\n",
       "        1.89371212e-02,  1.00313094e-02, -2.32204565e-02, -8.33065644e-02,\n",
       "       -4.29539946e-02, -5.79441592e-03,  1.56737037e-02, -1.58957337e-02,\n",
       "       -8.74915420e-02, -5.12439739e-02,  5.47896078e-02,  1.24419841e-02,\n",
       "       -6.85630330e-02,  4.02500379e-03,  3.45239974e-02, -4.32505381e-02,\n",
       "       -7.37636212e-02, -7.87286794e-02, -5.72994650e-02, -1.35129667e-02,\n",
       "       -2.21230763e-03,  1.34356601e-02,  2.57237191e-02, -8.58323122e-03,\n",
       "       -2.99731239e-02,  2.74893426e-02, -3.39693100e-02, -1.18402743e-02,\n",
       "        2.46144980e-02,  2.70466457e-02, -9.79767380e-05,  7.90393516e-03,\n",
       "       -4.31812192e-02,  6.40434786e-02, -7.74170464e-02, -3.41331208e-02,\n",
       "       -8.39486079e-02,  2.98850590e-02, -9.40895586e-02,  9.70937919e-02,\n",
       "        3.55302056e-02,  1.16174254e-02,  1.52785760e-02, -1.89323350e-02,\n",
       "       -2.26514715e-02, -3.94407900e-03, -4.77341291e-02, -3.08753083e-02,\n",
       "        3.35837440e-02,  4.19016085e-02, -5.66160353e-02,  1.51645382e-02,\n",
       "        2.91516059e-02, -6.74410074e-03,  8.02785925e-02,  1.73675222e-02,\n",
       "        4.90699340e-04, -6.08122346e-03, -9.04595048e-03,  9.63371912e-03,\n",
       "        5.59575012e-02, -2.87679314e-02,  4.00684150e-03,  1.09356108e-02,\n",
       "        1.57249779e-02, -4.91525832e-02,  5.61954422e-03, -2.28332944e-02,\n",
       "        3.09825620e-02, -1.16085970e-02,  3.00726971e-02, -4.48177352e-02,\n",
       "       -4.20712117e-02,  9.09128739e-02, -1.62668407e-03, -3.60819474e-03,\n",
       "       -4.91768796e-02, -4.19579296e-02, -1.25445478e-02, -2.37526550e-02,\n",
       "        7.01297939e-02, -4.37329492e-02,  1.42945929e-02, -1.87008505e-02,\n",
       "        4.48144045e-02, -4.59054086e-02,  3.38366566e-02, -3.06552505e-02,\n",
       "        1.65252740e-02, -1.66209951e-02, -6.38524555e-02, -5.26203844e-02,\n",
       "       -1.03367046e-01, -5.81473582e-02,  2.80489426e-02,  5.69832539e-04,\n",
       "       -1.12343596e-01,  3.72242533e-02, -7.97257390e-03,  2.32667501e-02,\n",
       "        7.88753213e-03,  5.47440639e-02, -1.50856028e-02, -6.04874725e-02,\n",
       "       -6.24722150e-02,  6.91974735e-03,  2.86264764e-02,  6.55474377e-03,\n",
       "       -1.85985268e-02, -9.98539390e-02,  2.14043201e-02,  5.94670679e-02,\n",
       "       -3.78213012e-02,  2.82098332e-02,  3.02462677e-02,  3.20103635e-02,\n",
       "       -5.61390085e-03, -3.04411809e-02, -1.57892765e-02,  1.41794344e-02,\n",
       "       -1.58853038e-02, -9.78038925e-02,  1.26123516e-02,  7.57020486e-02,\n",
       "       -9.44151598e-02,  6.45493179e-02, -9.48476004e-03, -2.55287909e-02,\n",
       "        1.10264173e-02, -3.54793434e-02,  2.03315415e-02, -1.26932187e-02,\n",
       "        3.20108545e-02, -9.20502126e-02, -3.39110253e-02, -3.55191149e-02,\n",
       "       -2.86413390e-03, -1.50695308e-02,  4.04002434e-02, -2.43525073e-02,\n",
       "        3.95953367e-02,  1.17200556e-02, -7.38452021e-03,  1.58346693e-02,\n",
       "       -5.08321898e-02, -2.12378705e-02,  2.88797977e-02,  4.05717428e-02,\n",
       "        2.30615496e-03, -4.66167658e-02, -2.01133063e-02,  4.20740301e-02,\n",
       "       -1.06740552e-02, -3.67550630e-02, -4.59563744e-02, -1.31442061e-02,\n",
       "        4.34839910e-03,  6.36150702e-04,  4.57729780e-02, -5.64840483e-03,\n",
       "       -6.56115626e-02,  9.34315719e-03, -7.11475365e-02,  2.62324993e-02,\n",
       "        2.43308771e-02,  1.65911551e-02, -4.05279168e-02,  5.99062206e-03,\n",
       "       -6.43282011e-02, -3.35613078e-02,  5.27926777e-02,  6.91456524e-03,\n",
       "        5.47205459e-02,  5.54092822e-02, -4.78205912e-04, -1.03793461e-02,\n",
       "       -2.72220317e-02,  2.93905941e-02,  1.10276946e-01, -3.92123395e-02,\n",
       "        2.52234931e-03, -3.02180748e-02,  5.92094380e-03,  6.14193783e-02,\n",
       "        7.56932143e-02,  2.58664013e-02, -7.13838445e-02,  7.17903362e-03,\n",
       "        6.01497722e-02,  1.50854424e-03,  1.36619621e-02,  1.19431623e-03,\n",
       "       -5.29258908e-02, -6.02907615e-02, -6.59291956e-02, -3.24931552e-02,\n",
       "       -8.54553728e-02,  4.75669851e-02, -1.88552339e-02, -5.99480064e-02,\n",
       "        7.04186339e-02, -3.00731889e-02, -8.97076866e-02,  9.73484867e-03,\n",
       "        2.02759204e-02, -4.90340518e-02,  2.19687195e-02,  4.32652808e-02,\n",
       "        2.03445232e-02, -1.63275872e-02, -3.37532802e-02,  5.34904494e-02,\n",
       "       -1.10282630e-01,  5.32863540e-02, -5.42019960e-02, -5.35269581e-02,\n",
       "       -1.60611265e-02,  4.45838443e-02,  1.18485889e-01,  5.43073120e-02,\n",
       "       -3.40444630e-02,  4.45293981e-02,  2.38025847e-02, -1.40529085e-02,\n",
       "        8.69377507e-03,  3.61604896e-02, -1.27508955e-03,  3.35874996e-02,\n",
       "       -1.07188398e-01, -4.21459274e-02,  3.71796265e-02,  6.84385417e-02,\n",
       "        5.29141981e-02,  1.25005281e-02,  4.56880733e-02,  8.59059942e-02,\n",
       "       -3.10381628e-02, -7.05557503e-02,  2.85545727e-02, -4.14218795e-02,\n",
       "       -3.10048846e-02, -3.23209223e-02, -5.29301642e-03, -1.37566780e-03,\n",
       "        8.70092470e-02,  4.69419245e-03, -3.50340070e-02,  1.39469411e-02,\n",
       "        3.40771173e-02,  8.11869904e-02, -1.44261855e-02, -4.05896462e-02,\n",
       "       -4.21302321e-02,  6.63393739e-03,  4.76397387e-03, -2.19376162e-02,\n",
       "        9.71421395e-03,  3.16972937e-02,  5.32974530e-03,  7.56164118e-02,\n",
       "       -6.02987190e-03,  4.70808125e-02,  2.59037026e-02, -1.24567840e-01,\n",
       "        4.49611278e-02, -7.47093317e-02, -2.85404220e-02,  3.17671523e-02,\n",
       "       -1.69871436e-02,  1.30223459e-02,  1.30177793e-01, -3.33277973e-03,\n",
       "       -7.18300194e-02,  1.00753358e-01,  3.92638442e-02,  1.13152775e-02,\n",
       "        1.31326295e-02, -6.49563706e-02, -1.28842552e-02,  2.48008741e-02,\n",
       "       -3.09315156e-02,  3.01146211e-02, -1.12762478e-01,  7.24977105e-02,\n",
       "        6.33204592e-03, -5.36451363e-02, -8.35680393e-02,  2.79404200e-02,\n",
       "        1.53356696e-02,  4.65941000e-02, -3.76593329e-02, -6.22967665e-02,\n",
       "       -3.11822800e-03, -4.16319227e-02,  2.15441500e-02, -6.75321723e-02,\n",
       "       -2.17900311e-03, -6.79827364e-02,  6.98302952e-02, -1.97467654e-02,\n",
       "       -4.03726102e-02, -2.32334046e-02,  4.06165902e-02, -8.38858855e-02,\n",
       "       -1.23055868e-02,  1.82379420e-03,  2.15105723e-02, -9.07211656e-03,\n",
       "        5.20533277e-03, -3.83400448e-02,  5.47297723e-03,  1.44789874e-02,\n",
       "       -1.06713706e-02,  6.91158144e-02,  3.69575400e-02, -1.20113683e-02,\n",
       "       -1.07578541e-02,  2.94117183e-02, -1.39546799e-02,  6.65553827e-03,\n",
       "        7.27336206e-03, -1.36782154e-02,  2.76536305e-02, -7.49970827e-02,\n",
       "        2.50655256e-02, -5.58966021e-02,  2.20987405e-02,  1.89612212e-02,\n",
       "       -5.28593907e-02, -5.38957850e-02, -5.04497585e-02,  8.65741253e-03,\n",
       "       -2.60025922e-02, -1.08397318e-01, -1.03550522e-01,  3.88938981e-02,\n",
       "        6.98787511e-03, -6.24327470e-02, -1.08805807e-04, -1.61116119e-02,\n",
       "        1.51717074e-02,  4.74558107e-02,  4.15740999e-02, -2.90230204e-02,\n",
       "       -4.05974452e-02,  1.88600628e-02, -4.53665049e-02, -1.11549143e-02,\n",
       "       -1.81143918e-02, -1.31193934e-03,  2.95092124e-02, -3.88092614e-02,\n",
       "        1.86186813e-02, -2.35386876e-02,  5.19480692e-02, -5.87815872e-02,\n",
       "        1.57690476e-02, -1.60220386e-02,  2.46737105e-02, -5.08351938e-02,\n",
       "        9.99992921e-03, -3.40426641e-02, -4.08622744e-02, -4.06604142e-03,\n",
       "       -5.53813078e-03, -4.10899027e-02,  1.49125624e-02, -1.17365733e-02,\n",
       "       -7.90915571e-03, -1.88083948e-02,  1.16238225e-02,  8.89175274e-02,\n",
       "       -4.87426571e-02, -3.79703355e-02,  1.17424029e-02,  2.83633456e-02,\n",
       "        1.20943760e-02,  2.05973558e-02,  2.05574660e-02,  3.27271440e-02,\n",
       "       -7.53651833e-03, -4.64288734e-02,  8.63568922e-03, -7.80562046e-02,\n",
       "       -1.57162452e-02,  2.08342513e-02])>"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A_prime[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "07df830b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([31, 36, 32, 28, 24,  1, 10, 17, 32, 20,  1, 32,  3, 12, 27, 29, 18,\n",
       "       29, 16, 33, 26, 12,  5, 20, 37,  1, 36, 13, 19,  3,  4,  3, 15, 36,\n",
       "       30,  9, 21, 16, 36, 17, 29, 35,  1, 30, 25, 27, 11, 31,  6, 12,  2,\n",
       "       15, 31, 18,  4, 27, 28, 11,  7, 21,  9, 19, 20, 27, 35, 23,  9, 20,\n",
       "       16, 20, 13, 10,  9,  6, 21, 14, 24, 27,  3, 28, 19, 15, 24,  1, 25,\n",
       "       25, 25, 23, 21, 17,  0, 12,  6,  2,  7, 32, 10, 30, 29, 36,  1,  4,\n",
       "       14,  5, 34, 17, 28,  0, 20, 18, 26,  7,  3,  8, 21, 10, 18,  0, 33,\n",
       "       35, 11, 32, 20, 16, 33, 18,  2, 29, 18, 12, 34, 14, 16,  0, 26, 37,\n",
       "        4, 30, 22, 35, 35, 18, 36, 27, 35, 15,  9, 15, 24, 12, 10, 23, 14,\n",
       "       28, 31, 29, 20, 19, 25, 33, 13, 14,  7, 15,  4,  8,  9, 21,  4,  6,\n",
       "       15, 30,  6, 23,  8,  6, 11, 16, 26, 14, 10, 20, 21, 32, 15, 15, 19,\n",
       "        2, 26,  6, 10,  0, 32,  8, 14, 27, 12, 31, 28, 27, 34, 13, 35, 31,\n",
       "       35, 36, 25, 25, 21, 16, 24,  5, 15,  3, 31,  4,  2, 19,  9,  0, 17,\n",
       "        0,  2, 32, 30, 22, 16, 24, 16,  0, 12,  5,  7,  8, 13, 30, 18, 25,\n",
       "       18, 19,  4, 30, 35, 37, 11, 31, 17, 22,  5, 19, 20,  0, 23, 33,  6,\n",
       "       11, 22, 28,  0,  1,  5, 33, 10, 29,  4, 36, 18, 26, 12, 20, 24, 33,\n",
       "       30,  5, 35, 24, 21, 35,  5, 37, 16, 37, 27, 18,  7,  7, 19, 30, 23,\n",
       "       35, 31, 14,  8, 31,  7,  4,  9, 16, 22, 10, 31, 13, 25, 15, 17, 13,\n",
       "       37, 23,  2, 31,  5,  2,  8,  3, 36,  3,  7, 19, 11, 17,  7,  1, 26,\n",
       "        1,  2, 15, 17, 37,  1, 18, 33, 19, 19, 24, 32, 24, 37, 11, 14, 11,\n",
       "       27, 21,  6, 23, 33, 28,  7,  6, 24, 13, 33, 17,  8, 26, 20, 15, 14,\n",
       "       29, 26,  7, 26,  5, 30,  9, 11, 34, 13, 16, 24, 19, 32, 16,  0,  7,\n",
       "       28,  3,  5,  3, 16, 22, 17,  3, 22, 34, 37, 21,  9, 27, 32, 16,  9,\n",
       "       28, 13, 36, 34,  4, 34, 25, 23,  8,  2, 33, 21,  0,  5, 14,  1, 26,\n",
       "       17, 14, 35, 28, 13,  8, 11, 23, 24, 25, 11, 21,  6, 15, 22,  3, 22,\n",
       "       15, 29, 36, 25, 34, 22, 19, 10, 29,  6,  8, 14, 12, 25, 27, 12, 17,\n",
       "        0, 27, 11,  9, 22, 10, 23,  2,  1,  8, 20, 37, 28, 11, 22, 28, 24,\n",
       "       34, 32,  5,  2,  6,  5, 27, 33, 36, 31, 20,  7, 32,  3, 25,  5, 13,\n",
       "       37, 21, 12, 27, 35, 23, 33,  3, 29, 18,  4, 30, 23,  4, 10, 12,  9,\n",
       "       34, 10, 28, 20, 25, 13,  9, 26, 10, 12,  8, 22, 34,  2,  2, 34, 17,\n",
       "       30, 21, 17,  9,  8, 23, 31, 26, 11, 18, 12,  2, 22,  6, 37,  4, 26,\n",
       "       34,  8, 30,  1, 34, 31, 37, 13, 29, 37, 34, 33,  1, 22, 33, 30,  0,\n",
       "       29,  6,  0, 36, 28, 19, 10, 35, 29, 29, 14, 26,  1, 18, 24, 32,  7,\n",
       "       36, 36,  4, 14, 37, 32, 23, 13,  3])"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_train_shuffled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "1b27b723",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19,\n",
       "       19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19,\n",
       "       19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19,\n",
       "       19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19,\n",
       "       19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19,\n",
       "       19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19,\n",
       "       19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19,\n",
       "       19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19,\n",
       "       19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19,\n",
       "       19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19,\n",
       "       19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19,\n",
       "       19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19,\n",
       "       19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19,\n",
       "       19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19,\n",
       "       19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19,\n",
       "       19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19,\n",
       "       19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19,\n",
       "       19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19,\n",
       "       19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19,\n",
       "       19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19,\n",
       "       19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19,\n",
       "       19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19,\n",
       "       19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19,\n",
       "       19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19,\n",
       "       19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19,\n",
       "       19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19,\n",
       "       19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19,\n",
       "       19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19,\n",
       "       19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19,\n",
       "       19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19,\n",
       "       19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19,\n",
       "       19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19,\n",
       "       19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19,\n",
       "       19, 19, 19, 19, 19, 19, 19, 19, 19])"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(train_prob, axis=1)\n",
    "# np.avg(train_prob, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "918b960c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter: 0 Total loss: tf.Tensor(20.617666247881132, shape=(), dtype=float64)\n",
      "iter: 100 Total loss: tf.Tensor(0.7016107615570075, shape=(), dtype=float64)\n",
      "iter: 184 Total loss: tf.Tensor(0.009704540528992995, shape=(), dtype=float64)\n",
      "0.07863340563991324\n"
     ]
    }
   ],
   "source": [
    "A = Y_test.T\n",
    "A_prime =tf.constant(A)\n",
    "\n",
    "X = np.random.uniform(-1,1, (A.shape[0], embedding_dim))\n",
    "X_prime = X\n",
    "X_prime = tf.Variable(X_prime)\n",
    "\n",
    "Y_prime = Y_final\n",
    "\n",
    "GLRM_loss_list, X_regularization_loss_list, Y_regularization_loss_list, X_grad_restrictions, Y_grad_restrictions =  Multilearn_GLRM_Semisupervised_Test_Params(A_prime, X_prime, Y_prime)\n",
    "params[\"test_hyper_params\"] = [GLRM_loss_list, X_regularization_loss_list, Y_regularization_loss_list]\n",
    "\n",
    "num_iterations=10000\n",
    "learning_rate=0.01\n",
    "results_log = []\n",
    "for i in range(0,1):\n",
    "    A = Y_test.T\n",
    "    A_prime =tf.constant(A)\n",
    "\n",
    "    X = np.random.uniform(-1,1, (A.shape[0], embedding_dim))\n",
    "    X_prime = X\n",
    "    X_prime = tf.Variable(X_prime)\n",
    "\n",
    "    Y_prime = Y_final\n",
    "\n",
    "    result_val = Multi_Learn.predict(A_prime, X_prime, Y_prime, GLRM_loss_list, X_regularization_loss_list, Y_regularization_loss_list, num_iterations, learning_rate)\n",
    "    val_pred = np.hstack((result_val[0], A_prime))\n",
    "    val_pred = np.hstack((val_pred, np.ones(shape=(val_pred.shape[0],1))))\n",
    "    val_pred = np.matmul(val_pred, beta)\n",
    "    val_pred = tf.nn.softmax(val_pred).numpy()\n",
    "    accuracy = accuracy_score(np.array(label_test)-1, np.argmax(val_pred, axis=1))\n",
    "\n",
    "    print(accuracy)\n",
    "    results_log.append(accuracy)\n",
    "\n",
    "params[\"test_accuracy\"] = results_log\n",
    "\n",
    "# with open('./logs/YaleB_accuracy_'+ str(round(np.mean(results_log),6))+'_MSE.json', 'w') as f:\n",
    "#     f.write(json.dumps(params, sort_keys=True, indent=4, separators=(',', ': ')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "695adfb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.07863340563991324"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_pred = np.hstack((result_val[0], A_prime))\n",
    "val_pred = np.hstack((val_pred, np.ones(shape=(val_pred.shape[0],1))))\n",
    "val_pred = np.matmul(val_pred, beta)\n",
    "val_pred = tf.nn.softmax(val_pred).numpy()\n",
    "accuracy = accuracy_score(np.array(label_test)-1, np.argmax(val_pred, axis=1))\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "b7f89cc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28\n",
      "28\n",
      "28\n",
      "28\n",
      "28\n",
      "28\n",
      "28\n",
      "28\n",
      "28\n",
      "28\n",
      "28\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "29\n",
      "27\n",
      "35\n",
      "28\n",
      "28\n",
      "28\n",
      "28\n",
      "28\n",
      "28\n",
      "28\n",
      "28\n",
      "26\n",
      "28\n",
      "28\n",
      "28\n",
      "28\n",
      "28\n",
      "28\n",
      "34\n",
      "28\n",
      "34\n",
      "35\n",
      "35\n",
      "35\n",
      "15\n",
      "35\n",
      "35\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "35\n",
      "35\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "34\n",
      "34\n",
      "4\n",
      "34\n",
      "34\n",
      "4\n",
      "34\n",
      "4\n",
      "34\n",
      "34\n",
      "34\n",
      "35\n",
      "28\n",
      "4\n",
      "4\n",
      "29\n",
      "28\n",
      "28\n",
      "28\n",
      "28\n",
      "4\n",
      "28\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "29\n",
      "27\n",
      "13\n",
      "13\n",
      "28\n",
      "28\n",
      "28\n",
      "28\n",
      "28\n",
      "28\n",
      "28\n",
      "28\n",
      "4\n",
      "28\n",
      "28\n",
      "28\n",
      "4\n",
      "34\n",
      "4\n",
      "34\n",
      "35\n",
      "35\n",
      "35\n",
      "34\n",
      "35\n",
      "35\n",
      "28\n",
      "28\n",
      "28\n",
      "28\n",
      "28\n",
      "28\n",
      "28\n",
      "28\n",
      "28\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "29\n",
      "28\n",
      "28\n",
      "28\n",
      "28\n",
      "28\n",
      "28\n",
      "28\n",
      "28\n",
      "26\n",
      "28\n",
      "28\n",
      "28\n",
      "28\n",
      "28\n",
      "28\n",
      "15\n",
      "28\n",
      "15\n",
      "15\n",
      "28\n",
      "15\n",
      "15\n",
      "34\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "27\n",
      "4\n",
      "27\n",
      "4\n",
      "27\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "28\n",
      "28\n",
      "26\n",
      "4\n",
      "28\n",
      "4\n",
      "4\n",
      "4\n",
      "28\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "13\n",
      "4\n",
      "4\n",
      "28\n",
      "28\n",
      "4\n",
      "4\n",
      "4\n",
      "28\n",
      "26\n",
      "4\n",
      "28\n",
      "15\n",
      "15\n",
      "28\n",
      "15\n",
      "15\n",
      "15\n",
      "15\n",
      "15\n",
      "35\n",
      "15\n",
      "35\n",
      "29\n",
      "28\n",
      "28\n",
      "28\n",
      "28\n",
      "27\n",
      "28\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "26\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "29\n",
      "27\n",
      "27\n",
      "13\n",
      "28\n",
      "28\n",
      "28\n",
      "28\n",
      "28\n",
      "28\n",
      "28\n",
      "28\n",
      "28\n",
      "28\n",
      "28\n",
      "28\n",
      "15\n",
      "15\n",
      "28\n",
      "28\n",
      "15\n",
      "15\n",
      "28\n",
      "28\n",
      "15\n",
      "15\n",
      "28\n",
      "28\n",
      "28\n",
      "26\n",
      "28\n",
      "28\n",
      "28\n",
      "28\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "29\n",
      "27\n",
      "27\n",
      "28\n",
      "28\n",
      "28\n",
      "28\n",
      "28\n",
      "28\n",
      "28\n",
      "28\n",
      "28\n",
      "26\n",
      "26\n",
      "28\n",
      "28\n",
      "28\n",
      "28\n",
      "28\n",
      "4\n",
      "15\n",
      "28\n",
      "15\n",
      "34\n",
      "34\n",
      "15\n",
      "34\n",
      "26\n",
      "26\n",
      "28\n",
      "28\n",
      "26\n",
      "28\n",
      "28\n",
      "27\n",
      "27\n",
      "26\n",
      "27\n",
      "27\n",
      "15\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "29\n",
      "27\n",
      "27\n",
      "29\n",
      "28\n",
      "28\n",
      "28\n",
      "28\n",
      "26\n",
      "28\n",
      "28\n",
      "28\n",
      "28\n",
      "28\n",
      "26\n",
      "26\n",
      "28\n",
      "28\n",
      "28\n",
      "28\n",
      "26\n",
      "28\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "15\n",
      "34\n",
      "28\n",
      "28\n",
      "29\n",
      "4\n",
      "4\n",
      "28\n",
      "28\n",
      "27\n",
      "28\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "28\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "13\n",
      "28\n",
      "28\n",
      "28\n",
      "28\n",
      "28\n",
      "28\n",
      "28\n",
      "28\n",
      "26\n",
      "28\n",
      "28\n",
      "28\n",
      "28\n",
      "28\n",
      "28\n",
      "28\n",
      "34\n",
      "34\n",
      "35\n",
      "35\n",
      "34\n",
      "34\n",
      "35\n",
      "28\n",
      "26\n",
      "26\n",
      "28\n",
      "28\n",
      "28\n",
      "28\n",
      "28\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "29\n",
      "28\n",
      "28\n",
      "28\n",
      "28\n",
      "28\n",
      "28\n",
      "28\n",
      "28\n",
      "26\n",
      "26\n",
      "28\n",
      "28\n",
      "28\n",
      "28\n",
      "28\n",
      "28\n",
      "15\n",
      "15\n",
      "35\n",
      "15\n",
      "28\n",
      "28\n",
      "4\n",
      "13\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "28\n",
      "4\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "4\n",
      "4\n",
      "28\n",
      "28\n",
      "4\n",
      "28\n",
      "4\n",
      "4\n",
      "28\n",
      "28\n",
      "15\n",
      "15\n",
      "4\n",
      "15\n",
      "28\n",
      "35\n",
      "35\n",
      "15\n",
      "35\n",
      "4\n",
      "4\n",
      "4\n",
      "28\n",
      "28\n",
      "4\n",
      "4\n",
      "4\n",
      "29\n",
      "4\n",
      "27\n",
      "4\n",
      "4\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "35\n",
      "28\n",
      "28\n",
      "4\n",
      "28\n",
      "28\n",
      "4\n",
      "4\n",
      "28\n",
      "28\n",
      "34\n",
      "28\n",
      "4\n",
      "28\n",
      "15\n",
      "15\n",
      "15\n",
      "4\n",
      "4\n",
      "34\n",
      "4\n",
      "28\n",
      "28\n",
      "26\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "27\n",
      "28\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "4\n",
      "4\n",
      "4\n",
      "28\n",
      "4\n",
      "4\n",
      "34\n",
      "34\n",
      "28\n",
      "34\n",
      "34\n",
      "28\n",
      "28\n",
      "15\n",
      "28\n",
      "15\n",
      "15\n",
      "35\n",
      "35\n",
      "15\n",
      "35\n",
      "28\n",
      "26\n",
      "26\n",
      "28\n",
      "28\n",
      "28\n",
      "28\n",
      "26\n",
      "26\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "26\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "35\n",
      "28\n",
      "28\n",
      "28\n",
      "26\n",
      "28\n",
      "28\n",
      "28\n",
      "28\n",
      "26\n",
      "28\n",
      "28\n",
      "28\n",
      "28\n",
      "28\n",
      "28\n",
      "28\n",
      "15\n",
      "15\n",
      "35\n",
      "35\n",
      "35\n",
      "4\n",
      "4\n",
      "28\n",
      "26\n",
      "4\n",
      "4\n",
      "28\n",
      "4\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "29\n",
      "28\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "15\n",
      "26\n",
      "28\n",
      "15\n",
      "15\n",
      "15\n",
      "15\n",
      "15\n",
      "15\n",
      "15\n",
      "15\n",
      "15\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "34\n",
      "4\n",
      "34\n",
      "34\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "15\n",
      "15\n",
      "28\n",
      "28\n",
      "29\n",
      "28\n",
      "28\n",
      "27\n",
      "28\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "26\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "13\n",
      "28\n",
      "28\n",
      "28\n",
      "28\n",
      "28\n",
      "28\n",
      "28\n",
      "28\n",
      "28\n",
      "28\n",
      "28\n",
      "28\n",
      "28\n",
      "28\n",
      "15\n",
      "28\n",
      "15\n",
      "34\n",
      "35\n",
      "35\n",
      "15\n",
      "35\n",
      "28\n",
      "26\n",
      "26\n",
      "4\n",
      "26\n",
      "28\n",
      "27\n",
      "27\n",
      "26\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "28\n",
      "27\n",
      "28\n",
      "28\n",
      "28\n",
      "28\n",
      "26\n",
      "28\n",
      "28\n",
      "28\n",
      "28\n",
      "28\n",
      "26\n",
      "26\n",
      "28\n",
      "28\n",
      "28\n",
      "28\n",
      "28\n",
      "28\n",
      "15\n",
      "28\n",
      "35\n",
      "35\n",
      "28\n",
      "28\n",
      "28\n",
      "27\n",
      "28\n",
      "28\n",
      "28\n",
      "27\n",
      "28\n",
      "27\n",
      "27\n",
      "27\n",
      "26\n",
      "26\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "29\n",
      "29\n",
      "27\n",
      "29\n",
      "28\n",
      "28\n",
      "28\n",
      "28\n",
      "28\n",
      "4\n",
      "28\n",
      "28\n",
      "28\n",
      "28\n",
      "28\n",
      "15\n",
      "28\n",
      "15\n",
      "28\n",
      "15\n",
      "34\n",
      "35\n",
      "4\n",
      "15\n",
      "35\n",
      "35\n",
      "26\n",
      "26\n",
      "26\n",
      "26\n",
      "26\n",
      "26\n",
      "26\n",
      "26\n",
      "26\n",
      "26\n",
      "26\n",
      "27\n",
      "4\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "29\n",
      "27\n",
      "27\n",
      "26\n",
      "26\n",
      "26\n",
      "26\n",
      "26\n",
      "26\n",
      "26\n",
      "26\n",
      "26\n",
      "26\n",
      "26\n",
      "28\n",
      "26\n",
      "28\n",
      "26\n",
      "28\n",
      "4\n",
      "28\n",
      "35\n",
      "35\n",
      "4\n",
      "35\n",
      "35\n",
      "28\n",
      "28\n",
      "26\n",
      "26\n",
      "28\n",
      "28\n",
      "28\n",
      "28\n",
      "27\n",
      "4\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "26\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "28\n",
      "28\n",
      "28\n",
      "28\n",
      "28\n",
      "28\n",
      "28\n",
      "28\n",
      "28\n",
      "28\n",
      "26\n",
      "28\n",
      "28\n",
      "15\n",
      "15\n",
      "15\n",
      "15\n",
      "28\n",
      "35\n",
      "15\n",
      "15\n",
      "15\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "26\n",
      "4\n",
      "34\n",
      "4\n",
      "15\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "15\n",
      "15\n",
      "4\n",
      "28\n",
      "26\n",
      "26\n",
      "28\n",
      "28\n",
      "26\n",
      "28\n",
      "28\n",
      "28\n",
      "26\n",
      "27\n",
      "27\n",
      "27\n",
      "4\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "26\n",
      "28\n",
      "28\n",
      "26\n",
      "28\n",
      "28\n",
      "28\n",
      "28\n",
      "28\n",
      "4\n",
      "4\n",
      "28\n",
      "15\n",
      "4\n",
      "15\n",
      "4\n",
      "15\n",
      "4\n",
      "15\n",
      "4\n",
      "4\n",
      "35\n",
      "15\n",
      "15\n",
      "35\n",
      "26\n",
      "26\n",
      "26\n",
      "26\n",
      "4\n",
      "4\n",
      "26\n",
      "4\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "35\n",
      "26\n",
      "28\n",
      "28\n",
      "26\n",
      "28\n",
      "4\n",
      "28\n",
      "28\n",
      "26\n",
      "28\n",
      "28\n",
      "15\n",
      "28\n",
      "15\n",
      "15\n",
      "15\n",
      "35\n",
      "28\n",
      "28\n",
      "15\n",
      "35\n",
      "26\n",
      "26\n",
      "4\n",
      "4\n",
      "26\n",
      "4\n",
      "26\n",
      "4\n",
      "4\n",
      "4\n",
      "27\n",
      "4\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "35\n",
      "29\n",
      "29\n",
      "27\n",
      "29\n",
      "28\n",
      "28\n",
      "28\n",
      "28\n",
      "28\n",
      "28\n",
      "28\n",
      "28\n",
      "28\n",
      "28\n",
      "28\n",
      "4\n",
      "28\n",
      "28\n",
      "28\n",
      "4\n",
      "35\n",
      "35\n",
      "34\n",
      "4\n",
      "35\n",
      "34\n",
      "26\n",
      "26\n",
      "26\n",
      "26\n",
      "28\n",
      "26\n",
      "26\n",
      "26\n",
      "26\n",
      "27\n",
      "27\n",
      "27\n",
      "26\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "29\n",
      "29\n",
      "26\n",
      "26\n",
      "26\n",
      "26\n",
      "26\n",
      "26\n",
      "26\n",
      "26\n",
      "26\n",
      "26\n",
      "26\n",
      "26\n",
      "26\n",
      "26\n",
      "26\n",
      "26\n",
      "26\n",
      "26\n",
      "26\n",
      "26\n",
      "26\n",
      "35\n",
      "26\n",
      "4\n",
      "28\n",
      "4\n",
      "4\n",
      "28\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "28\n",
      "4\n",
      "28\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "28\n",
      "28\n",
      "28\n",
      "4\n",
      "15\n",
      "4\n",
      "15\n",
      "15\n",
      "28\n",
      "15\n",
      "15\n",
      "15\n",
      "15\n",
      "15\n",
      "15\n",
      "28\n",
      "15\n",
      "35\n",
      "28\n",
      "26\n",
      "26\n",
      "28\n",
      "28\n",
      "28\n",
      "28\n",
      "28\n",
      "28\n",
      "28\n",
      "27\n",
      "27\n",
      "27\n",
      "26\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "29\n",
      "27\n",
      "27\n",
      "28\n",
      "28\n",
      "28\n",
      "28\n",
      "28\n",
      "28\n",
      "28\n",
      "26\n",
      "28\n",
      "28\n",
      "28\n",
      "28\n",
      "28\n",
      "28\n",
      "28\n",
      "28\n",
      "28\n",
      "35\n",
      "34\n",
      "15\n",
      "35\n",
      "28\n",
      "28\n",
      "26\n",
      "4\n",
      "4\n",
      "28\n",
      "28\n",
      "28\n",
      "4\n",
      "28\n",
      "28\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "29\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "29\n",
      "29\n",
      "27\n",
      "28\n",
      "28\n",
      "28\n",
      "28\n",
      "28\n",
      "28\n",
      "28\n",
      "28\n",
      "26\n",
      "28\n",
      "28\n",
      "28\n",
      "34\n",
      "28\n",
      "28\n",
      "34\n",
      "34\n",
      "35\n",
      "35\n",
      "15\n",
      "34\n",
      "35\n",
      "4\n",
      "4\n",
      "26\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "27\n",
      "27\n",
      "4\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "4\n",
      "4\n",
      "27\n",
      "29\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "13\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "28\n",
      "28\n",
      "34\n",
      "28\n",
      "34\n",
      "15\n",
      "15\n",
      "28\n",
      "15\n",
      "15\n",
      "15\n",
      "15\n",
      "15\n",
      "35\n",
      "35\n",
      "15\n",
      "15\n",
      "35\n",
      "4\n",
      "4\n",
      "4\n",
      "34\n",
      "4\n",
      "28\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "27\n",
      "4\n",
      "4\n",
      "4\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "4\n",
      "27\n",
      "4\n",
      "27\n",
      "35\n",
      "35\n",
      "4\n",
      "4\n",
      "4\n",
      "28\n",
      "4\n",
      "4\n",
      "28\n",
      "34\n",
      "4\n",
      "4\n",
      "34\n",
      "34\n",
      "34\n",
      "34\n",
      "34\n",
      "34\n",
      "34\n",
      "34\n",
      "4\n",
      "4\n",
      "34\n",
      "34\n",
      "26\n",
      "26\n",
      "26\n",
      "4\n",
      "4\n",
      "28\n",
      "4\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "15\n",
      "27\n",
      "27\n",
      "29\n",
      "27\n",
      "35\n",
      "4\n",
      "4\n",
      "28\n",
      "4\n",
      "26\n",
      "28\n",
      "28\n",
      "28\n",
      "26\n",
      "26\n",
      "28\n",
      "28\n",
      "28\n",
      "28\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "26\n",
      "4\n",
      "15\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "4\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "29\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "34\n",
      "34\n",
      "34\n",
      "34\n",
      "4\n",
      "34\n",
      "34\n",
      "28\n",
      "35\n",
      "34\n",
      "15\n",
      "35\n",
      "26\n",
      "26\n",
      "35\n",
      "4\n",
      "28\n",
      "28\n",
      "4\n",
      "26\n",
      "26\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "26\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "35\n",
      "35\n",
      "35\n",
      "28\n",
      "28\n",
      "28\n",
      "28\n",
      "26\n",
      "26\n",
      "28\n",
      "4\n",
      "28\n",
      "28\n",
      "28\n",
      "28\n",
      "28\n",
      "28\n",
      "28\n",
      "28\n",
      "35\n",
      "28\n",
      "35\n",
      "35\n",
      "35\n",
      "4\n",
      "35\n",
      "28\n",
      "26\n",
      "26\n",
      "35\n",
      "4\n",
      "4\n",
      "26\n",
      "4\n",
      "26\n",
      "27\n",
      "4\n",
      "4\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "35\n",
      "27\n",
      "27\n",
      "13\n",
      "27\n",
      "27\n",
      "13\n",
      "4\n",
      "28\n",
      "28\n",
      "26\n",
      "28\n",
      "28\n",
      "28\n",
      "34\n",
      "26\n",
      "26\n",
      "28\n",
      "34\n",
      "34\n",
      "34\n",
      "4\n",
      "4\n",
      "15\n",
      "4\n",
      "35\n",
      "35\n",
      "35\n",
      "28\n",
      "28\n",
      "26\n",
      "29\n",
      "4\n",
      "28\n",
      "27\n",
      "27\n",
      "27\n",
      "28\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "35\n",
      "27\n",
      "29\n",
      "27\n",
      "13\n",
      "29\n",
      "28\n",
      "28\n",
      "4\n",
      "28\n",
      "28\n",
      "28\n",
      "28\n",
      "4\n",
      "28\n",
      "28\n",
      "4\n",
      "28\n",
      "4\n",
      "28\n",
      "4\n",
      "4\n",
      "35\n",
      "28\n",
      "29\n",
      "4\n",
      "35\n",
      "35\n"
     ]
    }
   ],
   "source": [
    "for x in np.argmax(val_pred, axis=1):\n",
    "    print (x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "86ae8385",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  0,  0, ..., 37, 37, 37])"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(label_test)-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "59c2edb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.42424019e-03,  1.33224469e-03,  5.53522913e-04,  1.06521894e-03,\n",
       "        1.12348003e-03, -5.83529233e-04,  8.90350804e-04,  4.11846315e-04,\n",
       "        2.94052558e-03, -5.52908272e-04, -8.61529738e-04,  1.21510516e-03,\n",
       "        4.71752987e-04,  8.34627801e-04,  9.95672773e-04,  2.34777495e-03,\n",
       "       -1.08469896e-03,  1.86895498e-03,  8.04937219e-04,  1.05603756e-03,\n",
       "        1.57745371e-03, -6.25450117e-04,  9.20439871e-04, -9.23603756e-04,\n",
       "       -5.48255215e-04, -7.35537521e-07,  1.20186117e-03,  1.72628932e-03,\n",
       "        5.93192642e-04,  6.51027304e-04,  1.04056089e-03,  2.02205476e-03,\n",
       "        1.61060854e-04,  2.22730049e-03,  9.74439770e-04, -5.66723422e-04,\n",
       "        2.09441387e-03,  2.61401423e-03])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(beta, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "f3e66ca5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([30, 30, 30, ..., 19, 19, 19])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b5f6089",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_pred = np.hstack((result_val[0], A_prime))\n",
    "val_pred = np.hstack((val_pred, np.ones(shape=(val_pred.shape[0],1))))\n",
    "val_pred = np.matmul(val_pred, beta)\n",
    "val_pred = tf.nn.softmax(val_pred).numpy()\n",
    "accuracy_score(np.array(label_test)-1, np.argmax(val_pred, axis=1))\n",
    "\n",
    "# accuracy_score(label_test, val_preds)\n",
    "\n",
    "# prob_val = np.exp(prob_val)/(1+np.exp(prob_val))\n",
    "# predictions_val = [1 if x >= 0.5 else 0 for x in prob_val]\n",
    "# accuracy_score(Y_test, predictions_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5637260a",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d5a6ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "beta.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "570de168",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(label_test),  val_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65aee6e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "confusion_matrix(Y_test, predictions_val), confusion_matrix(Y_train, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69c99ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "409/(409+200), 409/(409+857)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "979995fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "305/(305+133), 305/(305+961)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07698ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "2027/(2027+538), 2027/(2027+3343)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a5846e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "X_train_embedding = result[0][:, 1:embedding_dim+1]\n",
    "X_train_embedding = X_train_embedding.numpy()\n",
    "\n",
    "X_test_embedding = result_val[0].numpy()\n",
    "\n",
    "\n",
    "clf = LogisticRegression().fit(X_train_embedding, Y_train)\n",
    "pred_logistic = clf.predict(X_train_embedding)\n",
    "pred_logistic_val = clf.predict(X_test_embedding)\n",
    "accuracy_score(Y_train, pred_logistic), accuracy_score(Y_test, pred_logistic_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21d387d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(Y_test, pred_logistic_val), confusion_matrix(Y_train, pred_logistic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f17149e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "328/(328+236), 328/(328+938)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aedaa965",
   "metadata": {},
   "outputs": [],
   "source": [
    "1565/(1565+650), 1565/(1565+3805)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9377afd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "beta[5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6461d6e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1db71d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "A_prime[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17cabbd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c98a3071",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "prob_val[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9bc4471",
   "metadata": {},
   "outputs": [],
   "source": [
    "prob[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4508e482",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = tf.Variable([1,1,1])\n",
    "tf.where(temp!=1).shape[0]\n",
    "# tf.where([True, False, False, True]).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6dcba4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.min(val_pred,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ddcd2bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.min(result[0],axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2100c827",
   "metadata": {},
   "outputs": [],
   "source": [
    "beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ada90a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "result[0][3:10,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a65a10ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_info = Y_regularization_loss_list[2]\n",
    "\n",
    "# A_00, A_10, A_01, A_11 = loss_info['A_start_row'], loss_info['A_end_row'], loss_info['A_start_col'], loss_info['A_end_col']\n",
    "X_00, X_10, X_01, X_11 = loss_info['X_start_row'], loss_info['X_end_row'], loss_info['X_start_col'], loss_info['X_end_col']\n",
    "# Y_00, Y_10, Y_01, Y_11 = loss_info['Y_start_row'], loss_info['Y_end_row'], loss_info['Y_start_col'], loss_info['Y_end_col']\n",
    "\n",
    "\n",
    "# A_prime[A_00:A_10, A_01:A_11]\n",
    "# X_prime[X_00:X_10, X_01:X_11]\n",
    "# Y_prime[Y_00:Y_10, Y_01:Y_11]\n",
    "Y_prime[X_00:X_10, X_01:X_11]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b861b2ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "b = 1.\n",
    "a = 0.\n",
    "current_min = 1.\n",
    "current_max = 38.\n",
    "\n",
    "def scale_class(val):\n",
    "    return (b-a)*(val-current_min)/(current_max-current_min)\n",
    "\n",
    "def scale_inv(val):\n",
    "    return val*(current_max-current_min) / (b-a) +current_min\n",
    "label_train_arr = np.array(label_train).reshape(-1,1)\n",
    "vfunc = np.vectorize(scale_class)\n",
    "label_train_arr_scaled = vfunc(label_train_arr)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
