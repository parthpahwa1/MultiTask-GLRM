{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8541f4bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from importlib import reload\n",
    "import json\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f18b8c23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "module_path = os.path.abspath(os.path.join('../../'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "import MultiLearn_GLRM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3356f8d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('./microarray/xtrain.data', sep='\\s+', header=None)\n",
    "df_label_train = pd.read_csv('./microarray/ytrain.data', sep='\\s+', header=None)\n",
    "df_test = pd.read_csv('./microarray/xtest.data', sep='\\s+', header=None)\n",
    "df_label_test = pd.read_csv('./microarray/ytest.data', sep='\\s+', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2239092b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2308, 63), (1, 63))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.shape,df_label_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9bf28c0e",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "cannot set a row with mismatched columns",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [42]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df_train\u001b[38;5;241m.\u001b[39mloc[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m=\u001b[39m df_label_train\u001b[38;5;241m.\u001b[39mloc[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mto_list()\n\u001b[1;32m      2\u001b[0m df_train\u001b[38;5;241m.\u001b[39mindex \u001b[38;5;241m=\u001b[39m df_train\u001b[38;5;241m.\u001b[39mindex \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m  \n\u001b[1;32m      3\u001b[0m df_train \u001b[38;5;241m=\u001b[39m df_train\u001b[38;5;241m.\u001b[39msort_index()\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.9/site-packages/pandas/core/indexing.py:716\u001b[0m, in \u001b[0;36m_LocationIndexer.__setitem__\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m    713\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_valid_setitem_indexer(key)\n\u001b[1;32m    715\u001b[0m iloc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miloc\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39miloc\n\u001b[0;32m--> 716\u001b[0m \u001b[43miloc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_setitem_with_indexer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.9/site-packages/pandas/core/indexing.py:1682\u001b[0m, in \u001b[0;36m_iLocIndexer._setitem_with_indexer\u001b[0;34m(self, indexer, value, name)\u001b[0m\n\u001b[1;32m   1679\u001b[0m     indexer, missing \u001b[38;5;241m=\u001b[39m convert_missing_indexer(indexer)\n\u001b[1;32m   1681\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m missing:\n\u001b[0;32m-> 1682\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_setitem_with_indexer_missing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1683\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m   1685\u001b[0m \u001b[38;5;66;03m# align and set the values\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.9/site-packages/pandas/core/indexing.py:1998\u001b[0m, in \u001b[0;36m_iLocIndexer._setitem_with_indexer_missing\u001b[0;34m(self, indexer, value)\u001b[0m\n\u001b[1;32m   1995\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_list_like_indexer(value):\n\u001b[1;32m   1996\u001b[0m         \u001b[38;5;66;03m# must have conforming columns\u001b[39;00m\n\u001b[1;32m   1997\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(value) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39mcolumns):\n\u001b[0;32m-> 1998\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcannot set a row with mismatched columns\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   2000\u001b[0m     value \u001b[38;5;241m=\u001b[39m Series(value, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39mcolumns, name\u001b[38;5;241m=\u001b[39mindexer)\n\u001b[1;32m   2002\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj):\n\u001b[1;32m   2003\u001b[0m     \u001b[38;5;66;03m# We will ignore the existing dtypes instead of using\u001b[39;00m\n\u001b[1;32m   2004\u001b[0m     \u001b[38;5;66;03m#  internals.concat logic\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: cannot set a row with mismatched columns"
     ]
    }
   ],
   "source": [
    "df_train.loc[-1] = df_label_train.loc[0].to_list()\n",
    "df_train.index = df_train.index + 1  \n",
    "df_train = df_train.sort_index()\n",
    "df_train = df_train.T\n",
    "\n",
    "df_test.loc[-1] = df_label_test.loc[0].to_list()\n",
    "df_test.index = df_test.index + 1  \n",
    "df_test = df_test.sort_index()\n",
    "df_test = df_test.T\n",
    "df_test = df_test.dropna()\n",
    "\n",
    "X_train = df_train.drop(columns=[0])\n",
    "Y_train = df_train[0]\n",
    "Y_train = np.array(Y_train, dtype=np.int64).reshape(-1,1)\n",
    "\n",
    "X_test = df_test.drop(columns=[0])\n",
    "Y_test = df_test[0]\n",
    "Y_test = np.array(Y_test, dtype=np.int64).reshape(-1,1)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.fit_transform(X_test)\n",
    "\n",
    "\n",
    "params = {\n",
    "    \"train_hyper_params\": None,\n",
    "    \"test_hyper_params\": None,\n",
    "    \"beta\": None,\n",
    "    \"embedding_matrix\": None,\n",
    "    \"train_error\": None,\n",
    "    \"test_error\": None,\n",
    "    \"embedding_dim\": None,\n",
    "    \"predictor_scaling_parmas\": None,\n",
    "    \"target_scaling_parmas\": None\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "10f83e11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>2299</th>\n",
       "      <th>2300</th>\n",
       "      <th>2301</th>\n",
       "      <th>2302</th>\n",
       "      <th>2303</th>\n",
       "      <th>2304</th>\n",
       "      <th>2305</th>\n",
       "      <th>2306</th>\n",
       "      <th>2307</th>\n",
       "      <th>2308</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.0</td>\n",
       "      <td>0.139501</td>\n",
       "      <td>-1.168927</td>\n",
       "      <td>0.564973</td>\n",
       "      <td>-3.366796</td>\n",
       "      <td>-1.323132</td>\n",
       "      <td>-0.692547</td>\n",
       "      <td>2.327395</td>\n",
       "      <td>0.923703</td>\n",
       "      <td>0.112167</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.180803</td>\n",
       "      <td>-0.942635</td>\n",
       "      <td>-1.210662</td>\n",
       "      <td>-0.588787</td>\n",
       "      <td>-0.070422</td>\n",
       "      <td>-2.783852</td>\n",
       "      <td>-2.840439</td>\n",
       "      <td>-1.160913</td>\n",
       "      <td>-0.343054</td>\n",
       "      <td>-0.055513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.0</td>\n",
       "      <td>1.164275</td>\n",
       "      <td>-2.018158</td>\n",
       "      <td>1.103533</td>\n",
       "      <td>-2.165435</td>\n",
       "      <td>-1.440117</td>\n",
       "      <td>-0.437420</td>\n",
       "      <td>2.661587</td>\n",
       "      <td>1.224011</td>\n",
       "      <td>0.210504</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.709480</td>\n",
       "      <td>-1.532940</td>\n",
       "      <td>-2.385967</td>\n",
       "      <td>-0.389641</td>\n",
       "      <td>0.422781</td>\n",
       "      <td>-2.816750</td>\n",
       "      <td>-2.422495</td>\n",
       "      <td>-1.722607</td>\n",
       "      <td>-1.703749</td>\n",
       "      <td>-1.699910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.0</td>\n",
       "      <td>0.841093</td>\n",
       "      <td>0.254720</td>\n",
       "      <td>-0.208748</td>\n",
       "      <td>-2.148149</td>\n",
       "      <td>-1.512765</td>\n",
       "      <td>-1.263723</td>\n",
       "      <td>2.946642</td>\n",
       "      <td>0.087828</td>\n",
       "      <td>0.482920</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.067958</td>\n",
       "      <td>-1.854060</td>\n",
       "      <td>-1.541312</td>\n",
       "      <td>-1.773723</td>\n",
       "      <td>-1.879935</td>\n",
       "      <td>-2.265289</td>\n",
       "      <td>-2.405726</td>\n",
       "      <td>-0.176379</td>\n",
       "      <td>-0.128743</td>\n",
       "      <td>-0.996417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2.0</td>\n",
       "      <td>0.685065</td>\n",
       "      <td>-1.927579</td>\n",
       "      <td>-0.233068</td>\n",
       "      <td>-1.640413</td>\n",
       "      <td>-1.008954</td>\n",
       "      <td>0.774451</td>\n",
       "      <td>1.617168</td>\n",
       "      <td>-0.567925</td>\n",
       "      <td>0.036621</td>\n",
       "      <td>...</td>\n",
       "      <td>1.077559</td>\n",
       "      <td>-0.263966</td>\n",
       "      <td>-1.966113</td>\n",
       "      <td>-1.086190</td>\n",
       "      <td>0.885914</td>\n",
       "      <td>-0.248590</td>\n",
       "      <td>0.385874</td>\n",
       "      <td>-0.508163</td>\n",
       "      <td>-0.626985</td>\n",
       "      <td>-0.699366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.956163</td>\n",
       "      <td>-2.234926</td>\n",
       "      <td>0.281563</td>\n",
       "      <td>-2.695628</td>\n",
       "      <td>-1.214697</td>\n",
       "      <td>-1.059872</td>\n",
       "      <td>2.498070</td>\n",
       "      <td>0.780196</td>\n",
       "      <td>1.041583</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.209320</td>\n",
       "      <td>-0.693147</td>\n",
       "      <td>-1.846427</td>\n",
       "      <td>-0.993442</td>\n",
       "      <td>-3.294138</td>\n",
       "      <td>-3.332605</td>\n",
       "      <td>-2.282782</td>\n",
       "      <td>-0.656622</td>\n",
       "      <td>-2.012157</td>\n",
       "      <td>-1.668657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>3.0</td>\n",
       "      <td>-0.258641</td>\n",
       "      <td>-1.684700</td>\n",
       "      <td>0.175800</td>\n",
       "      <td>-2.323809</td>\n",
       "      <td>-1.692276</td>\n",
       "      <td>-0.008637</td>\n",
       "      <td>2.302135</td>\n",
       "      <td>0.455778</td>\n",
       "      <td>-0.342490</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.049494</td>\n",
       "      <td>-1.223835</td>\n",
       "      <td>-1.140372</td>\n",
       "      <td>-0.952436</td>\n",
       "      <td>0.294012</td>\n",
       "      <td>-1.205307</td>\n",
       "      <td>-1.457576</td>\n",
       "      <td>-0.655081</td>\n",
       "      <td>-0.060493</td>\n",
       "      <td>-0.980563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>4.0</td>\n",
       "      <td>-1.109875</td>\n",
       "      <td>-1.046969</td>\n",
       "      <td>-0.853786</td>\n",
       "      <td>-2.607752</td>\n",
       "      <td>-1.770781</td>\n",
       "      <td>-1.259133</td>\n",
       "      <td>1.426380</td>\n",
       "      <td>-0.743599</td>\n",
       "      <td>0.613129</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.963120</td>\n",
       "      <td>-1.016940</td>\n",
       "      <td>-0.991553</td>\n",
       "      <td>-0.643405</td>\n",
       "      <td>-2.323809</td>\n",
       "      <td>-1.435485</td>\n",
       "      <td>-0.484995</td>\n",
       "      <td>0.479335</td>\n",
       "      <td>-1.006489</td>\n",
       "      <td>-0.778487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2.0</td>\n",
       "      <td>1.471485</td>\n",
       "      <td>-1.751578</td>\n",
       "      <td>-0.256700</td>\n",
       "      <td>-1.899122</td>\n",
       "      <td>-1.364924</td>\n",
       "      <td>-1.198654</td>\n",
       "      <td>2.489878</td>\n",
       "      <td>-0.035006</td>\n",
       "      <td>0.833474</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000900</td>\n",
       "      <td>-1.020263</td>\n",
       "      <td>-1.599488</td>\n",
       "      <td>-2.273998</td>\n",
       "      <td>-0.559141</td>\n",
       "      <td>-1.985587</td>\n",
       "      <td>-1.044124</td>\n",
       "      <td>-1.064211</td>\n",
       "      <td>-1.179605</td>\n",
       "      <td>-0.499556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>3.0</td>\n",
       "      <td>-0.396159</td>\n",
       "      <td>-1.191386</td>\n",
       "      <td>0.696691</td>\n",
       "      <td>-1.862397</td>\n",
       "      <td>-1.312672</td>\n",
       "      <td>0.744980</td>\n",
       "      <td>1.762708</td>\n",
       "      <td>1.034038</td>\n",
       "      <td>0.802943</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.104429</td>\n",
       "      <td>-0.955811</td>\n",
       "      <td>-0.438505</td>\n",
       "      <td>-0.845366</td>\n",
       "      <td>-0.372659</td>\n",
       "      <td>0.059118</td>\n",
       "      <td>-1.209320</td>\n",
       "      <td>-0.220148</td>\n",
       "      <td>0.452031</td>\n",
       "      <td>-1.119325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-2.136224</td>\n",
       "      <td>-2.236797</td>\n",
       "      <td>-0.946492</td>\n",
       "      <td>-2.777400</td>\n",
       "      <td>-1.822631</td>\n",
       "      <td>-0.455233</td>\n",
       "      <td>2.547514</td>\n",
       "      <td>-0.270366</td>\n",
       "      <td>1.437866</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.552585</td>\n",
       "      <td>-0.649513</td>\n",
       "      <td>-1.159318</td>\n",
       "      <td>-0.809456</td>\n",
       "      <td>-2.993734</td>\n",
       "      <td>-2.431555</td>\n",
       "      <td>-0.589148</td>\n",
       "      <td>-0.790319</td>\n",
       "      <td>-0.536485</td>\n",
       "      <td>-1.935168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>3.0</td>\n",
       "      <td>-0.190676</td>\n",
       "      <td>-1.513219</td>\n",
       "      <td>0.824439</td>\n",
       "      <td>-2.391416</td>\n",
       "      <td>-1.112610</td>\n",
       "      <td>0.591945</td>\n",
       "      <td>2.101032</td>\n",
       "      <td>0.405532</td>\n",
       "      <td>0.545111</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.167472</td>\n",
       "      <td>-1.143191</td>\n",
       "      <td>0.014002</td>\n",
       "      <td>-0.831950</td>\n",
       "      <td>-0.484184</td>\n",
       "      <td>-0.234963</td>\n",
       "      <td>-0.935728</td>\n",
       "      <td>0.018233</td>\n",
       "      <td>0.316051</td>\n",
       "      <td>-0.800064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>4.0</td>\n",
       "      <td>1.321409</td>\n",
       "      <td>0.495367</td>\n",
       "      <td>0.041526</td>\n",
       "      <td>-1.541779</td>\n",
       "      <td>-1.627093</td>\n",
       "      <td>-1.184170</td>\n",
       "      <td>2.187814</td>\n",
       "      <td>0.474245</td>\n",
       "      <td>0.340322</td>\n",
       "      <td>...</td>\n",
       "      <td>0.022348</td>\n",
       "      <td>-1.136626</td>\n",
       "      <td>-0.798285</td>\n",
       "      <td>-0.752473</td>\n",
       "      <td>-1.789163</td>\n",
       "      <td>-1.421300</td>\n",
       "      <td>-2.144727</td>\n",
       "      <td>-0.961550</td>\n",
       "      <td>-0.141333</td>\n",
       "      <td>-0.766148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.682009</td>\n",
       "      <td>-2.439551</td>\n",
       "      <td>0.684813</td>\n",
       "      <td>-2.102096</td>\n",
       "      <td>-1.694996</td>\n",
       "      <td>-0.911801</td>\n",
       "      <td>1.682205</td>\n",
       "      <td>0.467124</td>\n",
       "      <td>0.684359</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.884533</td>\n",
       "      <td>-0.651621</td>\n",
       "      <td>-1.065952</td>\n",
       "      <td>-0.742758</td>\n",
       "      <td>-2.815079</td>\n",
       "      <td>-2.241490</td>\n",
       "      <td>-1.808499</td>\n",
       "      <td>-1.409768</td>\n",
       "      <td>-0.850035</td>\n",
       "      <td>-1.557320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2.0</td>\n",
       "      <td>-0.332261</td>\n",
       "      <td>-2.200029</td>\n",
       "      <td>-0.463465</td>\n",
       "      <td>-2.600991</td>\n",
       "      <td>-1.204640</td>\n",
       "      <td>-1.030019</td>\n",
       "      <td>1.969194</td>\n",
       "      <td>-0.244750</td>\n",
       "      <td>0.319326</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.727739</td>\n",
       "      <td>-0.187173</td>\n",
       "      <td>-1.719811</td>\n",
       "      <td>-0.757366</td>\n",
       "      <td>0.389065</td>\n",
       "      <td>-0.705220</td>\n",
       "      <td>-1.263015</td>\n",
       "      <td>-0.907083</td>\n",
       "      <td>-0.415819</td>\n",
       "      <td>-1.910543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2.0</td>\n",
       "      <td>1.111232</td>\n",
       "      <td>-1.981952</td>\n",
       "      <td>0.223543</td>\n",
       "      <td>-1.465771</td>\n",
       "      <td>-1.203306</td>\n",
       "      <td>1.506186</td>\n",
       "      <td>1.584551</td>\n",
       "      <td>0.415679</td>\n",
       "      <td>0.114845</td>\n",
       "      <td>...</td>\n",
       "      <td>0.487414</td>\n",
       "      <td>-0.128857</td>\n",
       "      <td>-1.206977</td>\n",
       "      <td>-0.934200</td>\n",
       "      <td>-1.909868</td>\n",
       "      <td>-2.064356</td>\n",
       "      <td>-1.025827</td>\n",
       "      <td>-0.966006</td>\n",
       "      <td>-0.707652</td>\n",
       "      <td>-0.837711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2.0</td>\n",
       "      <td>0.648725</td>\n",
       "      <td>-1.386694</td>\n",
       "      <td>-0.562645</td>\n",
       "      <td>-2.006191</td>\n",
       "      <td>-1.098112</td>\n",
       "      <td>0.944645</td>\n",
       "      <td>1.994564</td>\n",
       "      <td>0.295873</td>\n",
       "      <td>0.560986</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.084469</td>\n",
       "      <td>-0.543693</td>\n",
       "      <td>-1.868857</td>\n",
       "      <td>-1.135069</td>\n",
       "      <td>-0.991284</td>\n",
       "      <td>-1.053543</td>\n",
       "      <td>-1.175709</td>\n",
       "      <td>-1.334841</td>\n",
       "      <td>-0.511826</td>\n",
       "      <td>-1.323132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>4.0</td>\n",
       "      <td>-0.271415</td>\n",
       "      <td>-1.125470</td>\n",
       "      <td>-1.538048</td>\n",
       "      <td>-2.649310</td>\n",
       "      <td>-1.148538</td>\n",
       "      <td>-0.355390</td>\n",
       "      <td>1.811693</td>\n",
       "      <td>0.344440</td>\n",
       "      <td>0.339895</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.307477</td>\n",
       "      <td>-1.137873</td>\n",
       "      <td>-0.962073</td>\n",
       "      <td>-0.592036</td>\n",
       "      <td>-1.977607</td>\n",
       "      <td>-1.232715</td>\n",
       "      <td>-1.881903</td>\n",
       "      <td>0.061283</td>\n",
       "      <td>0.556926</td>\n",
       "      <td>-0.835402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>3.0</td>\n",
       "      <td>-0.445818</td>\n",
       "      <td>-1.380312</td>\n",
       "      <td>0.299289</td>\n",
       "      <td>-2.739541</td>\n",
       "      <td>-1.468807</td>\n",
       "      <td>0.991658</td>\n",
       "      <td>2.057784</td>\n",
       "      <td>0.295055</td>\n",
       "      <td>0.766955</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.546626</td>\n",
       "      <td>-1.763172</td>\n",
       "      <td>-0.375421</td>\n",
       "      <td>-1.492099</td>\n",
       "      <td>-0.467287</td>\n",
       "      <td>-0.982965</td>\n",
       "      <td>-0.991284</td>\n",
       "      <td>0.209612</td>\n",
       "      <td>-0.194313</td>\n",
       "      <td>-1.136626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>4.0</td>\n",
       "      <td>0.259437</td>\n",
       "      <td>-0.782634</td>\n",
       "      <td>-1.277978</td>\n",
       "      <td>-1.889813</td>\n",
       "      <td>-0.659132</td>\n",
       "      <td>0.587620</td>\n",
       "      <td>1.427556</td>\n",
       "      <td>-0.237877</td>\n",
       "      <td>-0.007327</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.112610</td>\n",
       "      <td>-0.869169</td>\n",
       "      <td>-1.705398</td>\n",
       "      <td>-0.854255</td>\n",
       "      <td>-1.893128</td>\n",
       "      <td>-1.461880</td>\n",
       "      <td>-1.702650</td>\n",
       "      <td>-1.393520</td>\n",
       "      <td>-1.011976</td>\n",
       "      <td>-1.160913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>3.0</td>\n",
       "      <td>-0.192493</td>\n",
       "      <td>-1.063052</td>\n",
       "      <td>0.203186</td>\n",
       "      <td>-2.058072</td>\n",
       "      <td>-1.285906</td>\n",
       "      <td>0.663821</td>\n",
       "      <td>2.358871</td>\n",
       "      <td>0.304318</td>\n",
       "      <td>0.978514</td>\n",
       "      <td>...</td>\n",
       "      <td>0.452667</td>\n",
       "      <td>-2.149006</td>\n",
       "      <td>-0.113953</td>\n",
       "      <td>-1.994365</td>\n",
       "      <td>-0.659906</td>\n",
       "      <td>-0.721341</td>\n",
       "      <td>-1.907170</td>\n",
       "      <td>0.160587</td>\n",
       "      <td>-0.349274</td>\n",
       "      <td>-1.273323</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20 rows × 2309 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    0         1         2         3         4         5         6     \\\n",
       "0    3.0  0.139501 -1.168927  0.564973 -3.366796 -1.323132 -0.692547   \n",
       "1    2.0  1.164275 -2.018158  1.103533 -2.165435 -1.440117 -0.437420   \n",
       "3    4.0  0.841093  0.254720 -0.208748 -2.148149 -1.512765 -1.263723   \n",
       "5    2.0  0.685065 -1.927579 -0.233068 -1.640413 -1.008954  0.774451   \n",
       "6    1.0 -1.956163 -2.234926  0.281563 -2.695628 -1.214697 -1.059872   \n",
       "7    3.0 -0.258641 -1.684700  0.175800 -2.323809 -1.692276 -0.008637   \n",
       "9    4.0 -1.109875 -1.046969 -0.853786 -2.607752 -1.770781 -1.259133   \n",
       "11   2.0  1.471485 -1.751578 -0.256700 -1.899122 -1.364924 -1.198654   \n",
       "13   3.0 -0.396159 -1.191386  0.696691 -1.862397 -1.312672  0.744980   \n",
       "14   1.0 -2.136224 -2.236797 -0.946492 -2.777400 -1.822631 -0.455233   \n",
       "15   3.0 -0.190676 -1.513219  0.824439 -2.391416 -1.112610  0.591945   \n",
       "16   4.0  1.321409  0.495367  0.041526 -1.541779 -1.627093 -1.184170   \n",
       "17   1.0 -1.682009 -2.439551  0.684813 -2.102096 -1.694996 -0.911801   \n",
       "18   2.0 -0.332261 -2.200029 -0.463465 -2.600991 -1.204640 -1.030019   \n",
       "19   2.0  1.111232 -1.981952  0.223543 -1.465771 -1.203306  1.506186   \n",
       "20   2.0  0.648725 -1.386694 -0.562645 -2.006191 -1.098112  0.944645   \n",
       "21   4.0 -0.271415 -1.125470 -1.538048 -2.649310 -1.148538 -0.355390   \n",
       "22   3.0 -0.445818 -1.380312  0.299289 -2.739541 -1.468807  0.991658   \n",
       "23   4.0  0.259437 -0.782634 -1.277978 -1.889813 -0.659132  0.587620   \n",
       "24   3.0 -0.192493 -1.063052  0.203186 -2.058072 -1.285906  0.663821   \n",
       "\n",
       "        7         8         9     ...      2299      2300      2301      2302  \\\n",
       "0   2.327395  0.923703  0.112167  ... -0.180803 -0.942635 -1.210662 -0.588787   \n",
       "1   2.661587  1.224011  0.210504  ... -0.709480 -1.532940 -2.385967 -0.389641   \n",
       "3   2.946642  0.087828  0.482920  ... -0.067958 -1.854060 -1.541312 -1.773723   \n",
       "5   1.617168 -0.567925  0.036621  ...  1.077559 -0.263966 -1.966113 -1.086190   \n",
       "6   2.498070  0.780196  1.041583  ... -1.209320 -0.693147 -1.846427 -0.993442   \n",
       "7   2.302135  0.455778 -0.342490  ... -2.049494 -1.223835 -1.140372 -0.952436   \n",
       "9   1.426380 -0.743599  0.613129  ... -0.963120 -1.016940 -0.991553 -0.643405   \n",
       "11  2.489878 -0.035006  0.833474  ... -0.000900 -1.020263 -1.599488 -2.273998   \n",
       "13  1.762708  1.034038  0.802943  ... -1.104429 -0.955811 -0.438505 -0.845366   \n",
       "14  2.547514 -0.270366  1.437866  ... -1.552585 -0.649513 -1.159318 -0.809456   \n",
       "15  2.101032  0.405532  0.545111  ... -0.167472 -1.143191  0.014002 -0.831950   \n",
       "16  2.187814  0.474245  0.340322  ...  0.022348 -1.136626 -0.798285 -0.752473   \n",
       "17  1.682205  0.467124  0.684359  ... -1.884533 -0.651621 -1.065952 -0.742758   \n",
       "18  1.969194 -0.244750  0.319326  ... -0.727739 -0.187173 -1.719811 -0.757366   \n",
       "19  1.584551  0.415679  0.114845  ...  0.487414 -0.128857 -1.206977 -0.934200   \n",
       "20  1.994564  0.295873  0.560986  ... -0.084469 -0.543693 -1.868857 -1.135069   \n",
       "21  1.811693  0.344440  0.339895  ... -0.307477 -1.137873 -0.962073 -0.592036   \n",
       "22  2.057784  0.295055  0.766955  ... -0.546626 -1.763172 -0.375421 -1.492099   \n",
       "23  1.427556 -0.237877 -0.007327  ... -1.112610 -0.869169 -1.705398 -0.854255   \n",
       "24  2.358871  0.304318  0.978514  ...  0.452667 -2.149006 -0.113953 -1.994365   \n",
       "\n",
       "        2303      2304      2305      2306      2307      2308  \n",
       "0  -0.070422 -2.783852 -2.840439 -1.160913 -0.343054 -0.055513  \n",
       "1   0.422781 -2.816750 -2.422495 -1.722607 -1.703749 -1.699910  \n",
       "3  -1.879935 -2.265289 -2.405726 -0.176379 -0.128743 -0.996417  \n",
       "5   0.885914 -0.248590  0.385874 -0.508163 -0.626985 -0.699366  \n",
       "6  -3.294138 -3.332605 -2.282782 -0.656622 -2.012157 -1.668657  \n",
       "7   0.294012 -1.205307 -1.457576 -0.655081 -0.060493 -0.980563  \n",
       "9  -2.323809 -1.435485 -0.484995  0.479335 -1.006489 -0.778487  \n",
       "11 -0.559141 -1.985587 -1.044124 -1.064211 -1.179605 -0.499556  \n",
       "13 -0.372659  0.059118 -1.209320 -0.220148  0.452031 -1.119325  \n",
       "14 -2.993734 -2.431555 -0.589148 -0.790319 -0.536485 -1.935168  \n",
       "15 -0.484184 -0.234963 -0.935728  0.018233  0.316051 -0.800064  \n",
       "16 -1.789163 -1.421300 -2.144727 -0.961550 -0.141333 -0.766148  \n",
       "17 -2.815079 -2.241490 -1.808499 -1.409768 -0.850035 -1.557320  \n",
       "18  0.389065 -0.705220 -1.263015 -0.907083 -0.415819 -1.910543  \n",
       "19 -1.909868 -2.064356 -1.025827 -0.966006 -0.707652 -0.837711  \n",
       "20 -0.991284 -1.053543 -1.175709 -1.334841 -0.511826 -1.323132  \n",
       "21 -1.977607 -1.232715 -1.881903  0.061283  0.556926 -0.835402  \n",
       "22 -0.467287 -0.982965 -0.991284  0.209612 -0.194313 -1.136626  \n",
       "23 -1.893128 -1.461880 -1.702650 -1.393520 -1.011976 -1.160913  \n",
       "24 -0.659906 -0.721341 -1.907170  0.160587 -0.349274 -1.273323  \n",
       "\n",
       "[20 rows x 2309 columns]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "84527605",
   "metadata": {},
   "outputs": [],
   "source": [
    "params[\"predictor_scaling_parmas\"] =  {\n",
    "    \"mean\" : scaler.mean_.tolist(),\n",
    "    \"var\" : scaler.mean_.tolist()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e736a691",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_class = max(max(Y_train), max(Y_test))[0]\n",
    "\n",
    "one_hot_targets = np.eye(n_class)[Y_train.reshape(-1,)-1]\n",
    "Train = np.hstack((X_train, one_hot_targets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fb122cb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((63, 2312), (63, 2308))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Train.shape, X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "96597f86",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal device set to: Apple M1\n",
      "epoch: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-05 16:34:32.055729: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2022-04-05 16:34:32.055984: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter: 0 Total loss: tf.Tensor(830.8701866755964, shape=(), dtype=float64) [<tf.Tensor: shape=(), dtype=float64, numpy=733.3538683248473>, <tf.Tensor: shape=(), dtype=float64, numpy=30.416232554158693>]\n",
      "iter: 3 Total loss: tf.Tensor(808.8918278177705, shape=(), dtype=float64)\n",
      "iter: 0 Total loss: tf.Tensor(805.888398816908, shape=(), dtype=float64)\n",
      "iter: 50 Total loss: tf.Tensor(123.40431425533149, shape=(), dtype=float64)\n",
      "iter: 71 Total loss: tf.Tensor(186.02879732436548, shape=(), dtype=float64)\n",
      "epoch: 1\n",
      "iter: 0 Total loss: tf.Tensor(185.80146993722798, shape=(), dtype=float64) [<tf.Tensor: shape=(), dtype=float64, numpy=257.8949835667271>, <tf.Tensor: shape=(), dtype=float64, numpy=1.032579348995594>]\n",
      "iter: 1 Total loss: tf.Tensor(185.40116597606664, shape=(), dtype=float64)\n",
      "iter: 0 Total loss: tf.Tensor(185.1214517811676, shape=(), dtype=float64)\n",
      "iter: 50 Total loss: tf.Tensor(45.35630909089813, shape=(), dtype=float64)\n",
      "iter: 77 Total loss: tf.Tensor(35.282666199863115, shape=(), dtype=float64)\n",
      "epoch: 2\n",
      "iter: 0 Total loss: tf.Tensor(33.96816830808928, shape=(), dtype=float64) [<tf.Tensor: shape=(), dtype=float64, numpy=16.12177546651634>, <tf.Tensor: shape=(), dtype=float64, numpy=1.0012405422587305>]\n",
      "iter: 3 Total loss: tf.Tensor(33.42388433209236, shape=(), dtype=float64)\n",
      "iter: 0 Total loss: tf.Tensor(33.27988180276912, shape=(), dtype=float64)\n",
      "iter: 50 Total loss: tf.Tensor(9.240589324588736, shape=(), dtype=float64)\n",
      "iter: 89 Total loss: tf.Tensor(67.13403817818053, shape=(), dtype=float64)\n",
      "epoch: 3\n",
      "iter: 0 Total loss: tf.Tensor(66.0136686666285, shape=(), dtype=float64) [<tf.Tensor: shape=(), dtype=float64, numpy=64.4723885038336>, <tf.Tensor: shape=(), dtype=float64, numpy=0.9978071324205372>]\n",
      "iter: 1 Total loss: tf.Tensor(65.895540669366, shape=(), dtype=float64)\n",
      "iter: 0 Total loss: tf.Tensor(65.78408518852709, shape=(), dtype=float64)\n",
      "iter: 50 Total loss: tf.Tensor(14.973659367942691, shape=(), dtype=float64)\n",
      "epoch: 4\n",
      "iter: 0 Total loss: tf.Tensor(41.422180157406004, shape=(), dtype=float64) [<tf.Tensor: shape=(), dtype=float64, numpy=32.240312688060015>, <tf.Tensor: shape=(), dtype=float64, numpy=1.0007369087649054>]\n",
      "iter: 1 Total loss: tf.Tensor(41.34699209863054, shape=(), dtype=float64)\n",
      "iter: 0 Total loss: tf.Tensor(41.272219991478664, shape=(), dtype=float64)\n",
      "iter: 1 Total loss: tf.Tensor(41.114115025442516, shape=(), dtype=float64)\n",
      "epoch: 5\n",
      "iter: 0 Total loss: tf.Tensor(35.6233630852256, shape=(), dtype=float64) [<tf.Tensor: shape=(), dtype=float64, numpy=32.236331133931685>, <tf.Tensor: shape=(), dtype=float64, numpy=1.0027867016294736>]\n",
      "iter: 1 Total loss: tf.Tensor(35.576980530593, shape=(), dtype=float64)\n",
      "iter: 0 Total loss: tf.Tensor(35.52590224244782, shape=(), dtype=float64)\n",
      "iter: 1 Total loss: tf.Tensor(35.526094945230284, shape=(), dtype=float64)\n",
      "epoch: 6\n",
      "iter: 0 Total loss: tf.Tensor(31.543778197648688, shape=(), dtype=float64) [<tf.Tensor: shape=(), dtype=float64, numpy=32.23660366799574>, <tf.Tensor: shape=(), dtype=float64, numpy=1.0002904560966943>]\n",
      "iter: 1 Total loss: tf.Tensor(31.511615801257385, shape=(), dtype=float64)\n",
      "iter: 0 Total loss: tf.Tensor(31.476946509434402, shape=(), dtype=float64)\n",
      "iter: 50 Total loss: tf.Tensor(7.216092224708184, shape=(), dtype=float64)\n",
      "epoch: 7\n",
      "iter: 0 Total loss: tf.Tensor(4.704044278594406, shape=(), dtype=float64) [<tf.Tensor: shape=(), dtype=float64, numpy=0.0016643449144137774>, <tf.Tensor: shape=(), dtype=float64, numpy=0.9983334269488429>]\n",
      "iter: 1 Total loss: tf.Tensor(4.683634683727927, shape=(), dtype=float64)\n",
      "iter: 0 Total loss: tf.Tensor(4.65956519506281, shape=(), dtype=float64)\n",
      "iter: 50 Total loss: tf.Tensor(4.408492496046752, shape=(), dtype=float64)\n",
      "epoch: 8\n",
      "iter: 0 Total loss: tf.Tensor(3.684871281017686, shape=(), dtype=float64) [<tf.Tensor: shape=(), dtype=float64, numpy=0.02100498408736024>, <tf.Tensor: shape=(), dtype=float64, numpy=0.9993335084654013>]\n",
      "iter: 1 Total loss: tf.Tensor(3.6702073752764908, shape=(), dtype=float64)\n",
      "iter: 0 Total loss: tf.Tensor(3.653658190599511, shape=(), dtype=float64)\n",
      "iter: 50 Total loss: tf.Tensor(2.5954039863983303, shape=(), dtype=float64)\n",
      "epoch: 9\n",
      "iter: 0 Total loss: tf.Tensor(4.776618686132959, shape=(), dtype=float64) [<tf.Tensor: shape=(), dtype=float64, numpy=0.0002676777525837113>, <tf.Tensor: shape=(), dtype=float64, numpy=0.9995585300088492>]\n",
      "iter: 1 Total loss: tf.Tensor(4.767074636790797, shape=(), dtype=float64)\n",
      "iter: 0 Total loss: tf.Tensor(4.755466441808739, shape=(), dtype=float64)\n",
      "iter: 50 Total loss: tf.Tensor(2.941949416341024, shape=(), dtype=float64)\n",
      "epoch: 10\n",
      "iter: 0 Total loss: tf.Tensor(2.771854314601989, shape=(), dtype=float64) [<tf.Tensor: shape=(), dtype=float64, numpy=0.02500431164720428>, <tf.Tensor: shape=(), dtype=float64, numpy=0.9995374323249908>]\n",
      "iter: 1 Total loss: tf.Tensor(2.764977965697623, shape=(), dtype=float64)\n",
      "iter: 0 Total loss: tf.Tensor(2.7569380490254356, shape=(), dtype=float64)\n",
      "iter: 50 Total loss: tf.Tensor(2.365700645257299, shape=(), dtype=float64)\n",
      "epoch: 11\n",
      "iter: 0 Total loss: tf.Tensor(2.3865422013613973, shape=(), dtype=float64) [<tf.Tensor: shape=(), dtype=float64, numpy=0.05514449053514525>, <tf.Tensor: shape=(), dtype=float64, numpy=0.9998173083603661>]\n",
      "iter: 1 Total loss: tf.Tensor(2.3820167411904043, shape=(), dtype=float64)\n",
      "iter: 0 Total loss: tf.Tensor(2.37637659138523, shape=(), dtype=float64)\n",
      "iter: 50 Total loss: tf.Tensor(2.2604572870261124, shape=(), dtype=float64)\n",
      "epoch: 12\n",
      "iter: 0 Total loss: tf.Tensor(2.3087175938959135, shape=(), dtype=float64) [<tf.Tensor: shape=(), dtype=float64, numpy=0.059980543634559104>, <tf.Tensor: shape=(), dtype=float64, numpy=0.9997738759491208>]\n",
      "iter: 1 Total loss: tf.Tensor(2.305400798427361, shape=(), dtype=float64)\n",
      "iter: 0 Total loss: tf.Tensor(2.30148365468463, shape=(), dtype=float64)\n",
      "iter: 50 Total loss: tf.Tensor(2.2132712353139596, shape=(), dtype=float64)\n",
      "epoch: 13\n",
      "iter: 0 Total loss: tf.Tensor(2.248563098630298, shape=(), dtype=float64) [<tf.Tensor: shape=(), dtype=float64, numpy=0.05728568061224934>, <tf.Tensor: shape=(), dtype=float64, numpy=0.9998772595589112>]\n",
      "iter: 1 Total loss: tf.Tensor(2.2463881220439887, shape=(), dtype=float64)\n",
      "iter: 0 Total loss: tf.Tensor(2.2436312326145043, shape=(), dtype=float64)\n",
      "iter: 50 Total loss: tf.Tensor(2.18165543114041, shape=(), dtype=float64)\n",
      "epoch: 14\n",
      "iter: 0 Total loss: tf.Tensor(2.2062029278962854, shape=(), dtype=float64) [<tf.Tensor: shape=(), dtype=float64, numpy=0.06050028779221471>, <tf.Tensor: shape=(), dtype=float64, numpy=0.9999226279680993>]\n",
      "iter: 1 Total loss: tf.Tensor(2.204593627659607, shape=(), dtype=float64)\n",
      "iter: 0 Total loss: tf.Tensor(2.202682308600755, shape=(), dtype=float64)\n",
      "iter: 50 Total loss: tf.Tensor(2.1598403757739457, shape=(), dtype=float64)\n",
      "epoch: 15\n",
      "iter: 0 Total loss: tf.Tensor(2.177501133896726, shape=(), dtype=float64) [<tf.Tensor: shape=(), dtype=float64, numpy=0.058599688363182564>, <tf.Tensor: shape=(), dtype=float64, numpy=0.9999386556286214>]\n",
      "iter: 1 Total loss: tf.Tensor(2.1764414539738777, shape=(), dtype=float64)\n",
      "iter: 0 Total loss: tf.Tensor(2.17509371085931, shape=(), dtype=float64)\n",
      "iter: 50 Total loss: tf.Tensor(2.1452146071956912, shape=(), dtype=float64)\n",
      "epoch: 16\n",
      "iter: 0 Total loss: tf.Tensor(2.1574959133383373, shape=(), dtype=float64) [<tf.Tensor: shape=(), dtype=float64, numpy=0.0603587925718727>, <tf.Tensor: shape=(), dtype=float64, numpy=0.9999517190284478>]\n",
      "iter: 1 Total loss: tf.Tensor(2.156714652576705, shape=(), dtype=float64)\n",
      "iter: 0 Total loss: tf.Tensor(2.1557780087326393, shape=(), dtype=float64)\n",
      "iter: 50 Total loss: tf.Tensor(2.13515701852629, shape=(), dtype=float64)\n",
      "epoch: 17\n",
      "iter: 0 Total loss: tf.Tensor(2.143816099127309, shape=(), dtype=float64) [<tf.Tensor: shape=(), dtype=float64, numpy=0.06006831246792026>, <tf.Tensor: shape=(), dtype=float64, numpy=0.9999727731594606>]\n",
      "iter: 1 Total loss: tf.Tensor(2.1433002028438404, shape=(), dtype=float64)\n",
      "iter: 0 Total loss: tf.Tensor(2.1426392157350573, shape=(), dtype=float64)\n",
      "iter: 50 Total loss: tf.Tensor(2.1277892161828342, shape=(), dtype=float64)\n",
      "iter: 92 Total loss: tf.Tensor(2.1330500071690777, shape=(), dtype=float64)\n",
      "epoch: 18\n",
      "iter: 0 Total loss: tf.Tensor(2.1436022910840324, shape=(), dtype=float64) [<tf.Tensor: shape=(), dtype=float64, numpy=0.06100865653161374>, <tf.Tensor: shape=(), dtype=float64, numpy=1.000092118208212>]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter: 1 Total loss: tf.Tensor(2.1432226156481895, shape=(), dtype=float64)\n",
      "iter: 0 Total loss: tf.Tensor(2.142762794439807, shape=(), dtype=float64)\n",
      "iter: 1 Total loss: tf.Tensor(2.148873870591464, shape=(), dtype=float64)\n",
      "Final loss: tf.Tensor(2.142762794439807, shape=(), dtype=float64) best loss: tf.Tensor(2.142762794439807, shape=(), dtype=float64)\n"
     ]
    }
   ],
   "source": [
    "reload(MultiLearn_GLRM)\n",
    "from MultiLearn_GLRM import Multi_Learn, generate_AXY\n",
    "from MultiLearn_GLRM.Params.semisupervised_params import Multilearn_GLRM_Semisupervised_Train_Params, Multilearn_GLRM_Semisupervised_Test_Params\n",
    "\n",
    "embedding_dim = 30\n",
    "params[\"embedding_dim\"] = embedding_dim\n",
    "\n",
    "n_class = n_class\n",
    "\n",
    "A_prime, X_prime, Y_prime = generate_AXY.get_semisupervised_glrm_train_form(Train[:,:-n_class], Train[:,-n_class:], n_class, embedding_dim)\n",
    "GLRM_loss_list, X_regulariation_list, Y_regulariation_list, X_grad_restrictions, Y_grad_restrictions = Multilearn_GLRM_Semisupervised_Train_Params(A_prime, X_prime, Y_prime, n_class)\n",
    "\n",
    "params[\"train_hyper_params\"] = [GLRM_loss_list, X_regulariation_list, Y_regulariation_list]\n",
    "\n",
    "num_iterations=1000\n",
    "learning_rate=0.01\n",
    "result = Multi_Learn.alternating_minimization(A_prime, X_prime, Y_prime, GLRM_loss_list, X_regulariation_list, Y_regulariation_list, X_grad_restrictions, Y_grad_restrictions, num_iterations, learning_rate, n_class)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5c229acb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "beta = result[1][1:,0:n_class]\n",
    "Y_final = result[1][1:embedding_dim+1,n_class:]\n",
    "params[\"beta\"] = beta.numpy().tolist()\n",
    "params[\"embedding_matrix\"] = Y_final.numpy().tolist()\n",
    "\n",
    "train_prob = tf.nn.softmax(np.matmul(result[0], result[1][:,0:n_class])).numpy()\n",
    "label_train_shuffled =  np.argmax(Train[:,-n_class:], axis=1)\n",
    "params[\"train_accuracy\"] = accuracy_score(label_train_shuffled, np.argmax(train_prob, axis=1))\n",
    "params[\"train_accuracy\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "22f1319d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_train_shuffled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7e858e3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(train_prob, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c2d60e42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 1., 0.],\n",
       "       [0., 1., 0., 0.],\n",
       "       [0., 0., 0., 1.],\n",
       "       [0., 0., 0., 1.],\n",
       "       [0., 0., 0., 1.],\n",
       "       [0., 1., 0., 0.],\n",
       "       [1., 0., 0., 0.],\n",
       "       [0., 0., 1., 0.],\n",
       "       [0., 0., 0., 1.],\n",
       "       [0., 0., 0., 1.],\n",
       "       [0., 0., 0., 1.],\n",
       "       [0., 1., 0., 0.],\n",
       "       [0., 0., 0., 1.],\n",
       "       [0., 0., 1., 0.],\n",
       "       [1., 0., 0., 0.],\n",
       "       [0., 0., 1., 0.],\n",
       "       [0., 0., 0., 1.],\n",
       "       [1., 0., 0., 0.],\n",
       "       [0., 1., 0., 0.],\n",
       "       [0., 1., 0., 0.],\n",
       "       [0., 1., 0., 0.],\n",
       "       [0., 0., 0., 1.],\n",
       "       [0., 0., 1., 0.],\n",
       "       [0., 0., 0., 1.],\n",
       "       [0., 0., 1., 0.]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot_targets_val = np.eye(n_class)[Y_test.reshape(-1,)-1]\n",
    "one_hot_targets_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "558196dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# res = np.matmul(result[0], result[1][:,0:n_class])\n",
    "# np.mean((Y_train - res)**2)\n",
    "# 0.4495674496068231\n",
    "\n",
    "# res = np.matmul(result[0], result[1][:,0:n_class])\n",
    "# np.mean((Y_train - res)**2)\n",
    "# 0.4011543372347976\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1e7f876",
   "metadata": {},
   "outputs": [],
   "source": [
    "A_prime.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6db0fa4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "<tf.Tensor: shape=(11, 1), dtype=float64, numpy=\n",
    "array([[-0.02956712],\n",
    "       [-0.43500185],\n",
    "       [ 0.46663761],\n",
    "       [ 0.28479513],\n",
    "       [-0.04591487],\n",
    "       [ 0.22732162],\n",
    "       [ 0.19878154],\n",
    "       [-0.21300145],\n",
    "       [ 0.0071286 ],\n",
    "       [ 0.11102323],\n",
    "       [-0.00936144]])>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6761625",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(0, Y_train.shape[0]):\n",
    "    print(Y_train[i], res[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1d34aeea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter: 0 Total loss: tf.Tensor(1.8520780811379758, shape=(), dtype=float64)\n",
      "iter: 50 Total loss: tf.Tensor(1.5131978231099836, shape=(), dtype=float64)\n",
      "iter: 100 Total loss: tf.Tensor(1.284693330670677, shape=(), dtype=float64)\n",
      "iter: 112 Total loss: tf.Tensor(1.2450941069886405, shape=(), dtype=float64)\n",
      "0.8\n",
      "iter: 0 Total loss: tf.Tensor(1.8401827135992528, shape=(), dtype=float64)\n",
      "iter: 50 Total loss: tf.Tensor(1.5077413562286706, shape=(), dtype=float64)\n",
      "iter: 100 Total loss: tf.Tensor(1.2849028562162204, shape=(), dtype=float64)\n",
      "iter: 110 Total loss: tf.Tensor(1.252135190541121, shape=(), dtype=float64)\n",
      "0.8\n",
      "iter: 0 Total loss: tf.Tensor(1.8778441075452257, shape=(), dtype=float64)\n",
      "iter: 50 Total loss: tf.Tensor(1.5339951863352344, shape=(), dtype=float64)\n",
      "iter: 100 Total loss: tf.Tensor(1.2983136076051385, shape=(), dtype=float64)\n",
      "iter: 117 Total loss: tf.Tensor(1.2410321981213004, shape=(), dtype=float64)\n",
      "0.8\n",
      "iter: 0 Total loss: tf.Tensor(1.8793256277360608, shape=(), dtype=float64)\n",
      "iter: 50 Total loss: tf.Tensor(1.5343327210419977, shape=(), dtype=float64)\n",
      "iter: 100 Total loss: tf.Tensor(1.3006021042078015, shape=(), dtype=float64)\n",
      "iter: 116 Total loss: tf.Tensor(1.2470943970405146, shape=(), dtype=float64)\n",
      "0.8\n",
      "iter: 0 Total loss: tf.Tensor(1.8565853753321497, shape=(), dtype=float64)\n",
      "iter: 50 Total loss: tf.Tensor(1.5165342794770078, shape=(), dtype=float64)\n",
      "iter: 100 Total loss: tf.Tensor(1.2880205206214486, shape=(), dtype=float64)\n",
      "iter: 114 Total loss: tf.Tensor(1.241682825442059, shape=(), dtype=float64)\n",
      "0.8\n",
      "iter: 0 Total loss: tf.Tensor(1.902745840894287, shape=(), dtype=float64)\n",
      "iter: 50 Total loss: tf.Tensor(1.549588585622093, shape=(), dtype=float64)\n",
      "iter: 100 Total loss: tf.Tensor(1.308669824866093, shape=(), dtype=float64)\n",
      "iter: 121 Total loss: tf.Tensor(1.237023864075088, shape=(), dtype=float64)\n",
      "0.8\n",
      "iter: 0 Total loss: tf.Tensor(1.896589675413841, shape=(), dtype=float64)\n",
      "iter: 50 Total loss: tf.Tensor(1.548231645560622, shape=(), dtype=float64)\n",
      "iter: 100 Total loss: tf.Tensor(1.3102876027734487, shape=(), dtype=float64)\n",
      "iter: 118 Total loss: tf.Tensor(1.2491368673978174, shape=(), dtype=float64)\n",
      "0.8\n",
      "iter: 0 Total loss: tf.Tensor(1.8911899452031342, shape=(), dtype=float64)\n",
      "iter: 50 Total loss: tf.Tensor(1.5435932041370541, shape=(), dtype=float64)\n",
      "iter: 100 Total loss: tf.Tensor(1.3066924954939925, shape=(), dtype=float64)\n",
      "iter: 117 Total loss: tf.Tensor(1.2492630585631088, shape=(), dtype=float64)\n",
      "0.8\n",
      "iter: 0 Total loss: tf.Tensor(1.9163614455305584, shape=(), dtype=float64)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [35]\u001b[0m, in \u001b[0;36m<cell line: 21>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m X_prime \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mVariable(X_prime)\n\u001b[1;32m     30\u001b[0m Y_prime \u001b[38;5;241m=\u001b[39m Y_final\n\u001b[0;32m---> 32\u001b[0m result_val \u001b[38;5;241m=\u001b[39m \u001b[43mMulti_Learn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mA_prime\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_prime\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_prime\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mGLRM_loss_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_regularization_loss_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_regularization_loss_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_iterations\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m val_pred \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mhstack((result_val[\u001b[38;5;241m0\u001b[39m], A_prime))\n\u001b[1;32m     34\u001b[0m val_pred \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mhstack((val_pred, np\u001b[38;5;241m.\u001b[39mones(shape\u001b[38;5;241m=\u001b[39m(val_pred\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m],\u001b[38;5;241m1\u001b[39m))))\n",
      "File \u001b[0;32m~/Documents/GLRM-MTL/MultiLearn_GLRM/Multi_Learn.py:191\u001b[0m, in \u001b[0;36mpredict\u001b[0;34m(A, X, Y, GLRM_loss_list, X_regulariation_list, Y_regulariation_list, num_iterations, learning_rate)\u001b[0m\n\u001b[1;32m    189\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, num_iterations):\n\u001b[1;32m    190\u001b[0m     gradients_dictionary \u001b[38;5;241m=\u001b[39m get_gradients(A, X, Y, GLRM_loss_list, X_regulariation_list, Y_regulariation_list)\n\u001b[0;32m--> 191\u001b[0m     \u001b[43mopt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_gradients\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mzip\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mVariable\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgradients_dictionary\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mgradients\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mX\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mX\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    193\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m(i\u001b[38;5;241m%\u001b[39m\u001b[38;5;241m50\u001b[39m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    194\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124miter:\u001b[39m\u001b[38;5;124m'\u001b[39m, i, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTotal loss:\u001b[39m\u001b[38;5;124m'\u001b[39m, gradients_dictionary[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtotal_loss\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.9/site-packages/keras/optimizer_v2/optimizer_v2.py:665\u001b[0m, in \u001b[0;36mOptimizerV2.apply_gradients\u001b[0;34m(self, grads_and_vars, name, experimental_aggregate_gradients)\u001b[0m\n\u001b[1;32m    654\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m experimental_aggregate_gradients \u001b[38;5;129;01mand\u001b[39;00m strategy \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    655\u001b[0m     \u001b[38;5;28misinstance\u001b[39m(strategy,\n\u001b[1;32m    656\u001b[0m                (tf\u001b[38;5;241m.\u001b[39mcompat\u001b[38;5;241m.\u001b[39mv1\u001b[38;5;241m.\u001b[39mdistribute\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mParameterServerStrategy,\n\u001b[1;32m    657\u001b[0m                 tf\u001b[38;5;241m.\u001b[39mdistribute\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mParameterServerStrategy,\n\u001b[1;32m    658\u001b[0m                 tf\u001b[38;5;241m.\u001b[39mdistribute\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mCentralStorageStrategy,\n\u001b[1;32m    659\u001b[0m                 tf\u001b[38;5;241m.\u001b[39mcompat\u001b[38;5;241m.\u001b[39mv1\u001b[38;5;241m.\u001b[39mdistribute\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mCentralStorageStrategy))):\n\u001b[1;32m    660\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[1;32m    661\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`experimental_aggregate_gradients=False is not supported for \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    662\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParameterServerStrategy and CentralStorageStrategy. Used: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    663\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstrategy=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstrategy\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 665\u001b[0m apply_state \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_prepare\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvar_list\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    666\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m experimental_aggregate_gradients:\n\u001b[1;32m    667\u001b[0m   grads_and_vars \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transform_unaggregated_gradients(grads_and_vars)\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.9/site-packages/keras/optimizer_v2/nadam.py:139\u001b[0m, in \u001b[0;36mNadam._prepare\u001b[0;34m(self, var_list)\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_prepare\u001b[39m(\u001b[38;5;28mself\u001b[39m, var_list):\n\u001b[1;32m    137\u001b[0m   \u001b[38;5;66;03m# Get the value of the momentum cache before starting to apply gradients.\u001b[39;00m\n\u001b[1;32m    138\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_m_cache_read \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39midentity(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_m_cache)\n\u001b[0;32m--> 139\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mNadam\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_prepare\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvar_list\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.9/site-packages/keras/optimizer_v2/optimizer_v2.py:944\u001b[0m, in \u001b[0;36mOptimizerV2._prepare\u001b[0;34m(self, var_list)\u001b[0m\n\u001b[1;32m    942\u001b[0m   apply_state[(var_device, var_dtype)] \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m    943\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mdevice(var_device):\n\u001b[0;32m--> 944\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_prepare_local\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvar_device\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvar_dtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mapply_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    946\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m apply_state\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.9/site-packages/keras/optimizer_v2/nadam.py:102\u001b[0m, in \u001b[0;36mNadam._prepare_local\u001b[0;34m(self, var_device, var_dtype, apply_state)\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_prepare_local\u001b[39m(\u001b[38;5;28mself\u001b[39m, var_device, var_dtype, apply_state):\n\u001b[1;32m    101\u001b[0m   lr_t \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39midentity(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_hyper(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlearning_rate\u001b[39m\u001b[38;5;124m'\u001b[39m, var_dtype))\n\u001b[0;32m--> 102\u001b[0m   beta_1_t \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43midentity\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_hyper\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbeta_1\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvar_dtype\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    103\u001b[0m   beta_2_t \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39midentity(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_hyper(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbeta_2\u001b[39m\u001b[38;5;124m'\u001b[39m, var_dtype))\n\u001b[1;32m    104\u001b[0m   local_step \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mcast(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miterations \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m, var_dtype)\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.9/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.9/site-packages/tensorflow/python/util/dispatch.py:1082\u001b[0m, in \u001b[0;36madd_dispatch_support.<locals>.decorator.<locals>.op_dispatch_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1080\u001b[0m \u001b[38;5;66;03m# Fallback dispatch system (dispatch v1):\u001b[39;00m\n\u001b[1;32m   1081\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1082\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdispatch_target\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1083\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mTypeError\u001b[39;00m, \u001b[38;5;167;01mValueError\u001b[39;00m):\n\u001b[1;32m   1084\u001b[0m   \u001b[38;5;66;03m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[39;00m\n\u001b[1;32m   1085\u001b[0m   \u001b[38;5;66;03m# TypeError, when given unexpected types.  So we need to catch both.\u001b[39;00m\n\u001b[1;32m   1086\u001b[0m   result \u001b[38;5;241m=\u001b[39m dispatch(op_dispatch_handler, args, kwargs)\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.9/site-packages/tensorflow/python/ops/array_ops.py:287\u001b[0m, in \u001b[0;36midentity\u001b[0;34m(input, name)\u001b[0m\n\u001b[1;32m    283\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m context\u001b[38;5;241m.\u001b[39mexecuting_eagerly() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgraph\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    284\u001b[0m   \u001b[38;5;66;03m# Make sure we get an input with handle data attached from resource\u001b[39;00m\n\u001b[1;32m    285\u001b[0m   \u001b[38;5;66;03m# variables. Variables have correct handle data when graph building.\u001b[39;00m\n\u001b[1;32m    286\u001b[0m   \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m ops\u001b[38;5;241m.\u001b[39mconvert_to_tensor(\u001b[38;5;28minput\u001b[39m)\n\u001b[0;32m--> 287\u001b[0m ret \u001b[38;5;241m=\u001b[39m \u001b[43mgen_array_ops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43midentity\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;66;03m# Propagate handle data for happier shape inference for resource variables.\u001b[39;00m\n\u001b[1;32m    289\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_handle_data\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.9/site-packages/tensorflow/python/ops/gen_array_ops.py:4064\u001b[0m, in \u001b[0;36midentity\u001b[0;34m(input, name)\u001b[0m\n\u001b[1;32m   4062\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tld\u001b[38;5;241m.\u001b[39mis_eager:\n\u001b[1;32m   4063\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 4064\u001b[0m     _result \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_FastPathExecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4065\u001b[0m \u001b[43m      \u001b[49m\u001b[43m_ctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mIdentity\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4066\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _result\n\u001b[1;32m   4067\u001b[0m   \u001b[38;5;28;01mexcept\u001b[39;00m _core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "reload(MultiLearn_GLRM)\n",
    "from MultiLearn_GLRM import Multi_Learn, generate_AXY\n",
    "from MultiLearn_GLRM.Params.semisupervised_params import Multilearn_GLRM_Semisupervised_Train_Params, Multilearn_GLRM_Semisupervised_Test_Params\n",
    "\n",
    "A = X_test\n",
    "A_prime =tf.constant(A)\n",
    "\n",
    "\n",
    "X = np.random.standard_normal((A.shape[0], embedding_dim))\n",
    "X_prime = X\n",
    "X_prime = tf.Variable(X_prime)\n",
    "\n",
    "Y_prime = Y_final\n",
    "\n",
    "GLRM_loss_list, X_regularization_loss_list, Y_regularization_loss_list, X_grad_restrictions, Y_grad_restrictions =  Multilearn_GLRM_Semisupervised_Test_Params(A_prime, X_prime, Y_prime)\n",
    "params[\"test_hyper_params\"] = [GLRM_loss_list, X_regularization_loss_list, Y_regularization_loss_list]\n",
    "\n",
    "num_iterations=10000\n",
    "learning_rate=0.004\n",
    "results_log = []\n",
    "for i in range(0,10):\n",
    "    A = X_test\n",
    "    A_prime =tf.constant(A)\n",
    "\n",
    "\n",
    "    X = np.random.uniform(-1,1, (A.shape[0], embedding_dim))\n",
    "    X_prime = X\n",
    "    X_prime = tf.Variable(X_prime)\n",
    "\n",
    "    Y_prime = Y_final\n",
    "\n",
    "    result_val = Multi_Learn.predict(A_prime, X_prime, Y_prime, GLRM_loss_list, X_regularization_loss_list, Y_regularization_loss_list, num_iterations, learning_rate)\n",
    "    val_pred = np.hstack((result_val[0], A_prime))\n",
    "    val_pred = np.hstack((val_pred, np.ones(shape=(val_pred.shape[0],1))))\n",
    "    val_pred = np.matmul(val_pred, beta)\n",
    "    val_pred = tf.nn.softmax(val_pred).numpy()\n",
    "    accuracy = accuracy_score(np.argmax(one_hot_targets_val, axis=1), np.argmax(val_pred, axis=1))\n",
    "\n",
    "    print(accuracy)\n",
    "    results_log.append(accuracy)\n",
    "\n",
    "\n",
    "params[\"accuracy\"] = results_log\n",
    "\n",
    "# with open('./logs/Prostate_'+ str(round(np.mean(results_log),5))+'_MSE.json', 'w') as f:\n",
    "#     f.write(json.dumps(params, sort_keys=True, indent=4, separators=(',', ': ')))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7697b4a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3],\n",
       "       [2],\n",
       "       [0],\n",
       "       [4],\n",
       "       [0],\n",
       "       [2],\n",
       "       [1],\n",
       "       [3],\n",
       "       [0],\n",
       "       [4],\n",
       "       [0],\n",
       "       [2],\n",
       "       [0],\n",
       "       [3],\n",
       "       [1],\n",
       "       [3],\n",
       "       [4],\n",
       "       [1],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [4],\n",
       "       [3],\n",
       "       [4],\n",
       "       [3]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8cc1d4e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34f25389",
   "metadata": {},
   "outputs": [],
   "source": [
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34fb8398",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_pred = np.hstack((result_val[0], A_prime))\n",
    "val_pred = np.hstack((val_pred, np.ones(shape=(val_pred.shape[0],1))))\n",
    "regress_val = np.matmul(val_pred, beta)\n",
    "np.mean((scaler2.inverse_transform(Y_test) - scaler2.inverse_transform(regress_val))**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d376c42e",
   "metadata": {},
   "outputs": [],
   "source": [
    "regress_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e30e421",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65aee6e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "confusion_matrix(Y_test, predictions_val), confusion_matrix(Y_train, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69c99ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "409/(409+200), 409/(409+857)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "979995fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "305/(305+133), 305/(305+961)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07698ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "2027/(2027+538), 2027/(2027+3343)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a5846e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "X_train_embedding = result[0][:, 1:embedding_dim+1]\n",
    "X_train_embedding = X_train_embedding.numpy()\n",
    "\n",
    "X_test_embedding = result_val[0].numpy()\n",
    "\n",
    "\n",
    "clf = LogisticRegression().fit(X_train_embedding, Y_train)\n",
    "pred_logistic = clf.predict(X_train_embedding)\n",
    "pred_logistic_val = clf.predict(X_test_embedding)\n",
    "accuracy_score(Y_train, pred_logistic), accuracy_score(Y_test, pred_logistic_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21d387d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(Y_test, pred_logistic_val), confusion_matrix(Y_train, pred_logistic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f17149e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "328/(328+236), 328/(328+938)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aedaa965",
   "metadata": {},
   "outputs": [],
   "source": [
    "1565/(1565+650), 1565/(1565+3805)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9377afd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "beta[5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6461d6e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1db71d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "A_prime[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17cabbd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c98a3071",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "prob_val[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9bc4471",
   "metadata": {},
   "outputs": [],
   "source": [
    "prob[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4508e482",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = tf.Variable([1,1,1])\n",
    "tf.where(temp!=1).shape[0]\n",
    "# tf.where([True, False, False, True]).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6dcba4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.min(val_pred,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ddcd2bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.min(result[0],axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2100c827",
   "metadata": {},
   "outputs": [],
   "source": [
    "beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ada90a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "result[0][3:10,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e00b9b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "'beta' in params.keys()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
