{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8541f4bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from importlib import reload\n",
    "import json\n",
    "from matplotlib import pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f18b8c23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "module_path = os.path.abspath(os.path.join('../../'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "import MultiLearn_GLRM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9bf28c0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('./microarray/xtrain.data', sep='\\s+', header=None)\n",
    "df_label_train = pd.read_csv('./microarray/ytrain.data', sep='\\s+', header=None)\n",
    "df_test = pd.read_csv('./microarray/xtest.data', sep='\\s+', header=None)\n",
    "df_label_test = pd.read_csv('./microarray/ytest.data', sep='\\s+', header=None)\n",
    "\n",
    "df_train.loc[-1] = df_label_train.loc[0].to_list()\n",
    "df_train.index = df_train.index + 1  \n",
    "df_train = df_train.sort_index()\n",
    "df_train = df_train.T\n",
    "\n",
    "df_test.loc[-1] = df_label_test.loc[0].to_list()\n",
    "df_test.index = df_test.index + 1  \n",
    "df_test = df_test.sort_index()\n",
    "df_test = df_test.T\n",
    "df_test = df_test.dropna()\n",
    "\n",
    "X_train = df_train.drop(columns=[0])\n",
    "Y_train = df_train[0]\n",
    "Y_train = np.array(Y_train, dtype=np.int64).reshape(-1,1)\n",
    "\n",
    "X_test = df_test.drop(columns=[0])\n",
    "Y_test = df_test[0]\n",
    "Y_test = np.array(Y_test, dtype=np.int64).reshape(-1,1)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.fit_transform(X_test)\n",
    "\n",
    "\n",
    "params = {\n",
    "    \"train_hyper_params\": None,\n",
    "    \"test_hyper_params\": None,\n",
    "    \"beta\": None,\n",
    "    \"embedding_matrix\": None,\n",
    "    \"train_error\": None,\n",
    "    \"test_error\": None,\n",
    "    \"embedding_dim\": None,\n",
    "    \"predictor_scaling_parmas\": None,\n",
    "    \"target_scaling_parmas\": None\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "84527605",
   "metadata": {},
   "outputs": [],
   "source": [
    "params[\"predictor_scaling_parmas\"] =  {\n",
    "    \"mean\" : scaler.mean_.tolist(),\n",
    "    \"var\" : scaler.mean_.tolist()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "07764b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_class = max(max(Y_train), max(Y_test))[0]\n",
    "\n",
    "one_hot_targets = np.eye(n_class)[Y_train.reshape(-1,)-1]\n",
    "Train = np.hstack((X_train, one_hot_targets))\n",
    "np.random.shuffle(Train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "96597f86",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0\n",
      "iter: 1 Total loss: tf.Tensor(36.62215408239004, shape=(), dtype=float64) [5.743335858064075, 19.864183746985347]\n",
      "iter: 74 Total loss: tf.Tensor(12.390405261895328, shape=(), dtype=float64)\n",
      "iter: 1 Total loss: tf.Tensor(12.384689218367178, shape=(), dtype=float64)\n",
      "iter: 40 Total loss: tf.Tensor(1.7325083277127236, shape=(), dtype=float64)\n",
      "epoch: 1\n",
      "iter: 1 Total loss: tf.Tensor(1.7315156782121157, shape=(), dtype=float64) [0.3879187318552999, 0.9734478524246398]\n",
      "iter: 32 Total loss: tf.Tensor(1.305510954433335, shape=(), dtype=float64)\n",
      "iter: 1 Total loss: tf.Tensor(1.3049088102595632, shape=(), dtype=float64)\n",
      "iter: 13 Total loss: tf.Tensor(1.0873576952219672, shape=(), dtype=float64)\n",
      "epoch: 2\n",
      "iter: 1 Total loss: tf.Tensor(1.0866749894770191, shape=(), dtype=float64) [0.0532117578898473, 0.7712534130903165]\n",
      "iter: 41 Total loss: tf.Tensor(0.9483927354521597, shape=(), dtype=float64)\n",
      "iter: 1 Total loss: tf.Tensor(0.9479732229304852, shape=(), dtype=float64)\n",
      "iter: 14 Total loss: tf.Tensor(0.8741472471466416, shape=(), dtype=float64)\n",
      "epoch: 3\n",
      "iter: 1 Total loss: tf.Tensor(0.8736938945132704, shape=(), dtype=float64) [0.03230119866738518, 0.49730840005340377]\n",
      "iter: 26 Total loss: tf.Tensor(0.8236921414907961, shape=(), dtype=float64)\n",
      "iter: 1 Total loss: tf.Tensor(0.8233614476127047, shape=(), dtype=float64)\n",
      "iter: 13 Total loss: tf.Tensor(0.8043972202270152, shape=(), dtype=float64)\n",
      "epoch: 4\n",
      "iter: 1 Total loss: tf.Tensor(0.8040368704552585, shape=(), dtype=float64) [0.026302229307308102, 0.3754528077493334]\n",
      "iter: 13 Total loss: tf.Tensor(0.7923498410329191, shape=(), dtype=float64)\n",
      "iter: 1 Total loss: tf.Tensor(0.7920472354630324, shape=(), dtype=float64)\n",
      "iter: 11 Total loss: tf.Tensor(0.7864266897613288, shape=(), dtype=float64)\n",
      "epoch: 5\n",
      "iter: 1 Total loss: tf.Tensor(0.7860187297846941, shape=(), dtype=float64) [0.024667800238644167, 0.3346429391405121]\n",
      "iter: 7 Total loss: tf.Tensor(0.7816139579578898, shape=(), dtype=float64)\n",
      "iter: 1 Total loss: tf.Tensor(0.7812791858786637, shape=(), dtype=float64)\n",
      "iter: 6 Total loss: tf.Tensor(0.780003358717456, shape=(), dtype=float64)\n",
      "epoch: 6\n",
      "iter: 1 Total loss: tf.Tensor(0.7801342212732119, shape=(), dtype=float64) [0.02345899215320314, 0.3189277212089614]\n",
      "iter: 5 Total loss: tf.Tensor(0.7778993051573251, shape=(), dtype=float64)\n",
      "iter: 1 Total loss: tf.Tensor(0.7776206646558561, shape=(), dtype=float64)\n",
      "iter: 5 Total loss: tf.Tensor(0.7755086887665942, shape=(), dtype=float64)\n",
      "epoch: 7\n",
      "iter: 1 Total loss: tf.Tensor(0.7754218358742074, shape=(), dtype=float64) [0.02328260330499233, 0.30913913097402507]\n",
      "iter: 4 Total loss: tf.Tensor(0.774095174821809, shape=(), dtype=float64)\n",
      "iter: 1 Total loss: tf.Tensor(0.7738716690290663, shape=(), dtype=float64)\n",
      "iter: 5 Total loss: tf.Tensor(0.7730864750388019, shape=(), dtype=float64)\n",
      "epoch: 8\n",
      "iter: 1 Total loss: tf.Tensor(0.7732640980938068, shape=(), dtype=float64) [0.023221980841672096, 0.3036190490471468]\n",
      "iter: 3 Total loss: tf.Tensor(0.7724431720592848, shape=(), dtype=float64)\n",
      "iter: 1 Total loss: tf.Tensor(0.772233248632231, shape=(), dtype=float64)\n",
      "iter: 5 Total loss: tf.Tensor(0.7710520398450033, shape=(), dtype=float64)\n",
      "epoch: 9\n",
      "iter: 1 Total loss: tf.Tensor(0.7712139127749195, shape=(), dtype=float64) [0.02377353447964592, 0.3001377204930403]\n",
      "iter: 3 Total loss: tf.Tensor(0.7705229908955826, shape=(), dtype=float64)\n",
      "iter: 1 Total loss: tf.Tensor(0.7703482232744987, shape=(), dtype=float64)\n",
      "iter: 5 Total loss: tf.Tensor(0.7696886178170439, shape=(), dtype=float64)\n",
      "epoch: 10\n",
      "iter: 1 Total loss: tf.Tensor(0.7698079846336139, shape=(), dtype=float64) [0.02397090944166424, 0.2976940795197388]\n",
      "iter: 2 Total loss: tf.Tensor(0.7694476778693878, shape=(), dtype=float64)\n",
      "iter: 1 Total loss: tf.Tensor(0.7692359415933685, shape=(), dtype=float64)\n",
      "iter: 5 Total loss: tf.Tensor(0.7687320912423664, shape=(), dtype=float64)\n",
      "epoch: 11\n",
      "iter: 1 Total loss: tf.Tensor(0.7688947045339662, shape=(), dtype=float64) [0.024664232075810713, 0.2962114775976183]\n",
      "iter: 2 Total loss: tf.Tensor(0.7685602329027165, shape=(), dtype=float64)\n",
      "iter: 1 Total loss: tf.Tensor(0.7683660374551886, shape=(), dtype=float64)\n",
      "iter: 5 Total loss: tf.Tensor(0.7679519295341044, shape=(), dtype=float64)\n",
      "epoch: 12\n",
      "iter: 1 Total loss: tf.Tensor(0.7679964460542884, shape=(), dtype=float64) [0.021978569229651565, 0.29507068822841725]\n",
      "iter: 2 Total loss: tf.Tensor(0.7677311665265105, shape=(), dtype=float64)\n",
      "iter: 1 Total loss: tf.Tensor(0.7675789994245089, shape=(), dtype=float64)\n",
      "iter: 5 Total loss: tf.Tensor(0.7672724545407625, shape=(), dtype=float64)\n",
      "epoch: 13\n",
      "iter: 1 Total loss: tf.Tensor(0.7674328341680703, shape=(), dtype=float64) [0.02398942192137567, 0.2941618424242542]\n",
      "iter: 2 Total loss: tf.Tensor(0.7671863859634911, shape=(), dtype=float64)\n",
      "iter: 1 Total loss: tf.Tensor(0.7670441759442039, shape=(), dtype=float64)\n",
      "iter: 5 Total loss: tf.Tensor(0.7667345186345464, shape=(), dtype=float64)\n",
      "epoch: 14\n",
      "iter: 1 Total loss: tf.Tensor(0.7667670880719344, shape=(), dtype=float64) [0.02331520804796578, 0.2934829231562396]\n",
      "iter: 2 Total loss: tf.Tensor(0.7665527335013793, shape=(), dtype=float64)\n",
      "iter: 1 Total loss: tf.Tensor(0.7664282066397905, shape=(), dtype=float64)\n",
      "iter: 5 Total loss: tf.Tensor(0.7663611824563356, shape=(), dtype=float64)\n",
      "Final loss: tf.Tensor(0.7663055000356567, shape=(), dtype=float64) best loss: tf.Tensor(0.7663055000356567, shape=(), dtype=float64)\n"
     ]
    }
   ],
   "source": [
    "reload(MultiLearn_GLRM)\n",
    "from MultiLearn_GLRM import Multi_Learn, generate_AXY\n",
    "from MultiLearn_GLRM.Params.supervised_embedding_params import Multilearn_GLRM_Supervised_Embeddings_Train_Params, Multilearn_GLRM_Supervised_Embeddings_Test_Params\n",
    "\n",
    "embedding_dim = 20\n",
    "functional_loss='WLSE'\n",
    "params[\"embedding_dim\"] = embedding_dim\n",
    "\n",
    "n_class = n_class\n",
    "\n",
    "A_prime, X_prime, Y_prime = generate_AXY.get_supervised_embedding_glrm_train_form(Train[:,:-n_class], Train[:,-n_class:], n_class, embedding_dim)\n",
    "GLRM_loss_list, X_regulariation_list, Y_regulariation_list, X_grad_restrictions, Y_grad_restrictions = Multilearn_GLRM_Supervised_Embeddings_Train_Params(A_prime, X_prime, Y_prime, n_class)\n",
    "\n",
    "params[\"train_hyper_params\"] = [GLRM_loss_list, X_regulariation_list, Y_regulariation_list]\n",
    "\n",
    "num_iterations= 200\n",
    "learning_rate=0.05\n",
    "result = Multi_Learn.alternating_minimization(A_prime, X_prime, Y_prime, GLRM_loss_list, X_regulariation_list, Y_regulariation_list, X_grad_restrictions, Y_grad_restrictions, num_iterations, learning_rate, n_class, functional_loss=functional_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "103876e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "beta = result[1][1:,0:n_class]\n",
    "Y_final = result[1][1:embedding_dim+1,n_class:]\n",
    "params[\"beta\"] = beta.numpy().tolist()\n",
    "params[\"embedding_matrix\"] = Y_final.numpy().tolist()\n",
    "\n",
    "train_prob = tf.nn.softmax(np.matmul(result[0], result[1][:,0:n_class])).numpy()\n",
    "label_train_shuffled =  np.argmax(Train[:,-n_class:], axis=1)\n",
    "params[\"train_accuracy\"] = accuracy_score(label_train_shuffled, np.argmax(train_prob, axis=1))\n",
    "params[\"train_accuracy\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ae1b7322",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "0.95\n",
      "0.95\n",
      "0.95\n",
      "0.95\n",
      "0.95\n",
      "0.95\n",
      "0.95\n",
      "0.95\n",
      "0.95\n",
      "0.95\n",
      "0.95\n",
      "0.95\n",
      "0.95\n",
      "0.95\n",
      "0.95\n",
      "0.95\n",
      "0.95\n",
      "0.95\n",
      "0.95\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0475000000000001"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reload(MultiLearn_GLRM)\n",
    "from MultiLearn_GLRM import Multi_Learn, generate_AXY\n",
    "from MultiLearn_GLRM.Params.supervised_embedding_params import Multilearn_GLRM_Supervised_Embeddings_Train_Params, Multilearn_GLRM_Supervised_Embeddings_Test_Params\n",
    "\n",
    "A = X_test\n",
    "A_prime =tf.constant(A)\n",
    "\n",
    "\n",
    "X = np.random.standard_normal((A.shape[0], embedding_dim))\n",
    "X_prime = X\n",
    "X_prime = tf.Variable(X_prime)\n",
    "\n",
    "Y_prime = Y_final\n",
    "\n",
    "one_hot_targets_val = np.eye(n_class)[Y_test.reshape(-1,)-1]\n",
    "one_hot_targets_val\n",
    "\n",
    "GLRM_loss_list, X_regularization_loss_list, Y_regularization_loss_list, X_grad_restrictions, Y_grad_restrictions =  Multilearn_GLRM_Supervised_Embeddings_Test_Params(A_prime, X_prime, Y_prime)\n",
    "params[\"test_hyper_params\"] = [GLRM_loss_list, X_regularization_loss_list, Y_regularization_loss_list]\n",
    "\n",
    "num_iterations=10000\n",
    "learning_rate=0.1\n",
    "results_log = []\n",
    "for i in range(0,20):\n",
    "    A = X_test\n",
    "    A_prime =tf.constant(A)\n",
    "\n",
    "\n",
    "    X = np.random.standard_normal((A.shape[0], embedding_dim))\n",
    "    X_prime = X\n",
    "    X_prime = tf.Variable(X_prime)\n",
    "\n",
    "    Y_prime = Y_final\n",
    "\n",
    "    result_val = Multi_Learn.predict(A_prime, X_prime, Y_prime, GLRM_loss_list, X_regularization_loss_list, Y_regularization_loss_list, num_iterations, learning_rate)\n",
    "    val_pred = result_val[0]\n",
    "    val_pred = np.hstack((val_pred, np.ones(shape=(val_pred.shape[0],1))))\n",
    "    val_pred = np.matmul(val_pred, beta)\n",
    "    val_pred = tf.nn.softmax(val_pred).numpy()\n",
    "    accuracy = accuracy_score(Y_test-1, np.argmax(val_pred, axis=1))\n",
    "\n",
    "    print(accuracy)\n",
    "    results_log.append(accuracy)\n",
    "\n",
    "\n",
    "params[\"accuracy\"] = results_log\n",
    "1 - np.mean(results_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c229acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "beta = result[1][1:,0:n_class]\n",
    "Y_final = result[1][1:embedding_dim+1,n_class:]\n",
    "params[\"beta\"] = beta.numpy().tolist()\n",
    "params[\"embedding_matrix\"] = Y_final.numpy().tolist()\n",
    "\n",
    "train_prob = tf.nn.softmax(np.matmul(result[0], result[1][:,0:n_class])).numpy()\n",
    "label_train_shuffled =  np.argmax(Train[:,-n_class:], axis=1)\n",
    "params[\"train_accuracy\"] = accuracy_score(label_train_shuffled, np.argmax(train_prob, axis=1))\n",
    "params[\"train_accuracy\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9c26c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_train_shuffled =  np.argmax(Train[:,-n_class:], axis=1)\n",
    "label_train_shuffled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c067f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.argmax(train_prob, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b70b2c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(beta, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7f69bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(train_prob, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eebbba8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dict_keys(['SupervisedEmbeddings', 'Factorization', 'WLSE'])\n",
    "# result[-1].keys()\n",
    "len(result[-1]['SupervisedEmbeddings']), len(result[-1]['Factorization'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be09a8ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_values = list(range(0, len(result[-1]['SupervisedEmbeddings'])))\n",
    "len(x_values[::20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d7ea820",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "x_values = list(range(0, len(result[-1]['SupervisedEmbeddings'])))\n",
    "\n",
    "plt.plot(x_values[::20], result[-1]['SupervisedEmbeddings'][::20],'-.', label=\"Supervised Learning (w: 0.5)\")\n",
    "plt.plot(x_values[::20], result[-1]['Factorization'][::20], '-.', label=\"Matrix Factorization (w: 0.5)\")\n",
    "plt.plot(x_values[::20], result[-1]['weighted_average'][::20],'-x', label=\"Functional: Weighted Average\")\n",
    "plt.yscale('log')\n",
    "# plt.xticks( x_values[::20])\n",
    "plt.ylabel('Loss (Log Scale)')\n",
    "plt.xlabel('Iteration')\n",
    "plt.title('Task Loss vs Iteration (alpha=0.1)')\n",
    "plt.legend()\n",
    "plt.savefig('./microarray/plots/MSE_wavg_microarray_loss_vs_iteration_alpha_1e-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "300bc778",
   "metadata": {},
   "outputs": [],
   "source": [
    "# alpha = 0.1\n",
    "# w1 = 0.5\n",
    "# w2= 0.5\n",
    "\n",
    "# SupervisedEmbeddings_weights = w1*np.exp(alpha*np.array(result[-1]['SupervisedEmbeddings']))\n",
    "# Factorization_weights = w2*np.exp(alpha*np.array(result[-1]['Factorization']))\n",
    "# SupervisedEmbeddings_weights_normalized = SupervisedEmbeddings_weights/(SupervisedEmbeddings_weights+Factorization_weights)\n",
    "# Factorization_weights_normalized = Factorization_weights/(SupervisedEmbeddings_weights+Factorization_weights)\n",
    "\n",
    "# plt.figure(figsize=(10, 6))\n",
    "# x_values = list(range(0, len(result[-1]['SupervisedEmbeddings'])))\n",
    "\n",
    "# plt.plot(x_values[::20], SupervisedEmbeddings_weights_normalized[::20],'-s', label=\"Supervised Learning\")\n",
    "# plt.plot(x_values[::20], Factorization_weights_normalized[::20], '-s', label=\"Matrix Factorization\")\n",
    "\n",
    "# # plt.xticks( x_values[::20])\n",
    "# plt.ylabel('Task Weights')\n",
    "# plt.xlabel('Iteration')\n",
    "# plt.title('Task Weights vs Iteration (alpha=0.1)')\n",
    "# plt.legend()\n",
    "\n",
    "# plt.savefig('./microarray/plots/MSE_max_microarray_task_weight_vs_iteration_alpha_1e-1')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "558196dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# res = np.matmul(result[0], result[1][:,0:n_class])\n",
    "# np.mean((Y_train - res)**2)\n",
    "# 0.4495674496068231\n",
    "\n",
    "# res = np.matmul(result[0], result[1][:,0:n_class])\n",
    "# np.mean((Y_train - res)**2)\n",
    "# 0.4011543372347976\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1e7f876",
   "metadata": {},
   "outputs": [],
   "source": [
    "A_prime.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6db0fa4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "<tf.Tensor: shape=(11, 1), dtype=float64, numpy=\n",
    "array([[-0.02956712],\n",
    "       [-0.43500185],\n",
    "       [ 0.46663761],\n",
    "       [ 0.28479513],\n",
    "       [-0.04591487],\n",
    "       [ 0.22732162],\n",
    "       [ 0.19878154],\n",
    "       [-0.21300145],\n",
    "       [ 0.0071286 ],\n",
    "       [ 0.11102323],\n",
    "       [-0.00936144]])>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5f49308",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "reload(MultiLearn_GLRM)\n",
    "from MultiLearn_GLRM import Multi_Learn, generate_AXY\n",
    "from MultiLearn_GLRM.Params.supervised_embedding_params import Multilearn_GLRM_Supervised_Embeddings_Train_Params, Multilearn_GLRM_Supervised_Embeddings_Test_Params\n",
    "\n",
    "A = X_test\n",
    "A_prime =tf.constant(A)\n",
    "\n",
    "\n",
    "X = np.random.standard_normal((A.shape[0], embedding_dim))\n",
    "X_prime = X\n",
    "X_prime = tf.Variable(X_prime)\n",
    "\n",
    "Y_prime = Y_final\n",
    "\n",
    "one_hot_targets_val = np.eye(n_class)[Y_test.reshape(-1,)-1]\n",
    "one_hot_targets_val\n",
    "\n",
    "GLRM_loss_list, X_regularization_loss_list, Y_regularization_loss_list, X_grad_restrictions, Y_grad_restrictions =  Multilearn_GLRM_Supervised_Embeddings_Test_Params(A_prime, X_prime, Y_prime)\n",
    "params[\"test_hyper_params\"] = [GLRM_loss_list, X_regularization_loss_list, Y_regularization_loss_list]\n",
    "\n",
    "num_iterations=10000\n",
    "learning_rate=0.1\n",
    "results_log = []\n",
    "for i in range(0,20):\n",
    "    A = X_test\n",
    "    A_prime =tf.constant(A)\n",
    "\n",
    "\n",
    "    X = np.random.standard_normal((A.shape[0], embedding_dim))\n",
    "    X_prime = X\n",
    "    X_prime = tf.Variable(X_prime)\n",
    "\n",
    "    Y_prime = Y_final\n",
    "\n",
    "    result_val = Multi_Learn.predict(A_prime, X_prime, Y_prime, GLRM_loss_list, X_regularization_loss_list, Y_regularization_loss_list, num_iterations, learning_rate)\n",
    "    val_pred = result_val[0]\n",
    "    val_pred = np.hstack((val_pred, np.ones(shape=(val_pred.shape[0],1))))\n",
    "    val_pred = np.matmul(val_pred, beta)\n",
    "    val_pred = tf.nn.softmax(val_pred).numpy()\n",
    "    accuracy = accuracy_score(Y_test-1, np.argmax(val_pred, axis=1))\n",
    "\n",
    "    print(accuracy)\n",
    "    results_log.append(accuracy)\n",
    "\n",
    "\n",
    "params[\"accuracy\"] = results_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23a2a19b",
   "metadata": {},
   "outputs": [],
   "source": [
    "params['train_hyper_params'] = []\n",
    "\n",
    "with open('./logs/Microarray_'+ str(round(np.mean(results_log),5))+'_Accuracy_MSE_MAX.json', 'w') as f:\n",
    "    f.write(json.dumps(params, sort_keys=True, indent=4, separators=(',', ': ')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2daa955",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_final.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce090ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(Y_final.numpy()).to_csv('gene_embedding_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1911077f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a34ce171",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_test-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fb218c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.argmax(val_pred, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b0657c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "params['tol'] = 10e-6\n",
    "# type(params['embedding_matrix'])\n",
    "for key in params.keys():\n",
    "    print(key, type(params[key]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ea08000",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d34aeea",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89b40320",
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.argmax(val_pred, axis=1)\n",
    "# val_pred\n",
    "# len(Y_test-1)\n",
    "Y_test-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b24b5731",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.argmax(val_pred, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8cc1d4e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34f25389",
   "metadata": {},
   "outputs": [],
   "source": [
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34fb8398",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_pred = np.hstack((result_val[0], A_prime))\n",
    "val_pred = np.hstack((val_pred, np.ones(shape=(val_pred.shape[0],1))))\n",
    "regress_val = np.matmul(val_pred, beta)\n",
    "np.mean((scaler2.inverse_transform(Y_test) - scaler2.inverse_transform(regress_val))**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d376c42e",
   "metadata": {},
   "outputs": [],
   "source": [
    "regress_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e30e421",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65aee6e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "confusion_matrix(Y_test, predictions_val), confusion_matrix(Y_train, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69c99ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "409/(409+200), 409/(409+857)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "979995fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "305/(305+133), 305/(305+961)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07698ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "2027/(2027+538), 2027/(2027+3343)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a5846e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "X_train_embedding = result[0][:, 1:embedding_dim+1]\n",
    "X_train_embedding = X_train_embedding.numpy()\n",
    "\n",
    "X_test_embedding = result_val[0].numpy()\n",
    "\n",
    "\n",
    "clf = LogisticRegression().fit(X_train_embedding, Y_train)\n",
    "pred_logistic = clf.predict(X_train_embedding)\n",
    "pred_logistic_val = clf.predict(X_test_embedding)\n",
    "accuracy_score(Y_train, pred_logistic), accuracy_score(Y_test, pred_logistic_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21d387d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(Y_test, pred_logistic_val), confusion_matrix(Y_train, pred_logistic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f17149e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "328/(328+236), 328/(328+938)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aedaa965",
   "metadata": {},
   "outputs": [],
   "source": [
    "1565/(1565+650), 1565/(1565+3805)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9377afd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "beta[5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6461d6e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1db71d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "A_prime[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17cabbd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c98a3071",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "prob_val[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9bc4471",
   "metadata": {},
   "outputs": [],
   "source": [
    "prob[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4508e482",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = tf.Variable([1,1,1])\n",
    "tf.where(temp!=1).shape[0]\n",
    "# tf.where([True, False, False, True]).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6dcba4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.min(val_pred,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ddcd2bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.min(result[0],axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2100c827",
   "metadata": {},
   "outputs": [],
   "source": [
    "beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ada90a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "result[0][3:10,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e00b9b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "'beta' in params.keys()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
