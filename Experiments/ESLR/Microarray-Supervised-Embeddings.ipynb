{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8541f4bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from importlib import reload\n",
    "import json\n",
    "from matplotlib import pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f18b8c23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "module_path = os.path.abspath(os.path.join('../../'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "import MultiLearn_GLRM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9bf28c0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('./microarray/xtrain.data', sep='\\s+', header=None)\n",
    "df_label_train = pd.read_csv('./microarray/ytrain.data', sep='\\s+', header=None)\n",
    "df_test = pd.read_csv('./microarray/xtest.data', sep='\\s+', header=None)\n",
    "df_label_test = pd.read_csv('./microarray/ytest.data', sep='\\s+', header=None)\n",
    "\n",
    "df_train.loc[-1] = df_label_train.loc[0].to_list()\n",
    "df_train.index = df_train.index + 1  \n",
    "df_train = df_train.sort_index()\n",
    "df_train = df_train.T\n",
    "\n",
    "df_test.loc[-1] = df_label_test.loc[0].to_list()\n",
    "df_test.index = df_test.index + 1  \n",
    "df_test = df_test.sort_index()\n",
    "df_test = df_test.T\n",
    "df_test = df_test.dropna()\n",
    "\n",
    "X_train = df_train.drop(columns=[0])\n",
    "Y_train = df_train[0]\n",
    "Y_train = np.array(Y_train, dtype=np.int64).reshape(-1,1)\n",
    "\n",
    "X_test = df_test.drop(columns=[0])\n",
    "Y_test = df_test[0]\n",
    "Y_test = np.array(Y_test, dtype=np.int64).reshape(-1,1)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.fit_transform(X_test)\n",
    "\n",
    "\n",
    "params = {\n",
    "    \"train_hyper_params\": None,\n",
    "    \"test_hyper_params\": None,\n",
    "    \"beta\": None,\n",
    "    \"embedding_matrix\": None,\n",
    "    \"train_error\": None,\n",
    "    \"test_error\": None,\n",
    "    \"embedding_dim\": None,\n",
    "    \"predictor_scaling_parmas\": None,\n",
    "    \"target_scaling_parmas\": None\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "84527605",
   "metadata": {},
   "outputs": [],
   "source": [
    "params[\"predictor_scaling_parmas\"] =  {\n",
    "    \"mean\" : scaler.mean_.tolist(),\n",
    "    \"var\" : scaler.mean_.tolist()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "07764b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_class = max(max(Y_train), max(Y_test))[0]\n",
    "\n",
    "one_hot_targets = np.eye(n_class)[Y_train.reshape(-1,)-1]\n",
    "Train = np.hstack((X_train, one_hot_targets))\n",
    "np.random.shuffle(Train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "96597f86",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0\n",
      "iter: 1 Total loss: tf.Tensor(23.219868052901376, shape=(), dtype=float64) [4.595402075471945, 20.031861142819945]\n",
      "iter: 67 Total loss: tf.Tensor(11.618460227990836, shape=(), dtype=float64)\n",
      "iter: 1 Total loss: tf.Tensor(11.613192596348481, shape=(), dtype=float64)\n",
      "iter: 47 Total loss: tf.Tensor(0.9770960138407593, shape=(), dtype=float64)\n",
      "epoch: 1\n",
      "iter: 1 Total loss: tf.Tensor(0.9766250150246143, shape=(), dtype=float64) [0.5965622271432717, 0.9895781264457892]\n",
      "iter: 29 Total loss: tf.Tensor(0.7082633597281974, shape=(), dtype=float64)\n",
      "iter: 1 Total loss: tf.Tensor(0.7079515726999297, shape=(), dtype=float64)\n",
      "iter: 31 Total loss: tf.Tensor(0.6489566486474923, shape=(), dtype=float64)\n",
      "epoch: 2\n",
      "iter: 1 Total loss: tf.Tensor(0.6487903546737517, shape=(), dtype=float64) [0.0809762515520296, 0.8742002196542202]\n",
      "iter: 14 Total loss: tf.Tensor(0.6272095078541872, shape=(), dtype=float64)\n",
      "iter: 1 Total loss: tf.Tensor(0.6269702545439384, shape=(), dtype=float64)\n",
      "iter: 7 Total loss: tf.Tensor(0.6182851134875716, shape=(), dtype=float64)\n",
      "epoch: 3\n",
      "iter: 1 Total loss: tf.Tensor(0.616525683046648, shape=(), dtype=float64) [0.056948098700201386, 0.8064775464239816]\n",
      "iter: 15 Total loss: tf.Tensor(0.6037747289572813, shape=(), dtype=float64)\n",
      "iter: 1 Total loss: tf.Tensor(0.6035398719110752, shape=(), dtype=float64)\n",
      "iter: 11 Total loss: tf.Tensor(0.5915134704349714, shape=(), dtype=float64)\n",
      "epoch: 4\n",
      "iter: 1 Total loss: tf.Tensor(0.590674935503026, shape=(), dtype=float64) [0.049976635233586336, 0.7029697749423022]\n",
      "iter: 12 Total loss: tf.Tensor(0.5824724154266492, shape=(), dtype=float64)\n",
      "iter: 1 Total loss: tf.Tensor(0.5822313503156726, shape=(), dtype=float64)\n",
      "iter: 7 Total loss: tf.Tensor(0.5797102177891178, shape=(), dtype=float64)\n",
      "epoch: 5\n",
      "iter: 1 Total loss: tf.Tensor(0.5784758826827903, shape=(), dtype=float64) [0.04727670795102837, 0.6447748462135262]\n",
      "iter: 8 Total loss: tf.Tensor(0.5747976888091102, shape=(), dtype=float64)\n",
      "iter: 1 Total loss: tf.Tensor(0.5745746063111865, shape=(), dtype=float64)\n",
      "iter: 7 Total loss: tf.Tensor(0.5725695677375519, shape=(), dtype=float64)\n",
      "epoch: 6\n",
      "iter: 1 Total loss: tf.Tensor(0.5719380735785838, shape=(), dtype=float64) [0.04618326048271957, 0.6143279186860741]\n",
      "iter: 7 Total loss: tf.Tensor(0.5691609130346059, shape=(), dtype=float64)\n",
      "iter: 1 Total loss: tf.Tensor(0.5689361236747433, shape=(), dtype=float64)\n",
      "iter: 7 Total loss: tf.Tensor(0.5676534469740635, shape=(), dtype=float64)\n",
      "epoch: 7\n",
      "iter: 1 Total loss: tf.Tensor(0.5671583127046419, shape=(), dtype=float64) [0.04640157165979202, 0.5916577699487269]\n",
      "iter: 5 Total loss: tf.Tensor(0.565397908069765, shape=(), dtype=float64)\n",
      "iter: 1 Total loss: tf.Tensor(0.5651602673476693, shape=(), dtype=float64)\n",
      "iter: 6 Total loss: tf.Tensor(0.5644343928491715, shape=(), dtype=float64)\n",
      "epoch: 8\n",
      "iter: 1 Total loss: tf.Tensor(0.5642716579385714, shape=(), dtype=float64) [0.04349483313528717, 0.5791755386742454]\n",
      "iter: 4 Total loss: tf.Tensor(0.5631248780170677, shape=(), dtype=float64)\n",
      "iter: 1 Total loss: tf.Tensor(0.5629067294021713, shape=(), dtype=float64)\n",
      "iter: 5 Total loss: tf.Tensor(0.5623364652371003, shape=(), dtype=float64)\n",
      "epoch: 9\n",
      "iter: 1 Total loss: tf.Tensor(0.5617399457800846, shape=(), dtype=float64) [0.045386915828614795, 0.5707994588628682]\n",
      "iter: 4 Total loss: tf.Tensor(0.5607284250898321, shape=(), dtype=float64)\n",
      "iter: 1 Total loss: tf.Tensor(0.5605301755194196, shape=(), dtype=float64)\n",
      "iter: 6 Total loss: tf.Tensor(0.5604113719652772, shape=(), dtype=float64)\n",
      "epoch: 10\n",
      "iter: 1 Total loss: tf.Tensor(0.5601680727376501, shape=(), dtype=float64) [0.04410125596534338, 0.5646843515913018]\n",
      "iter: 3 Total loss: tf.Tensor(0.5595018166389878, shape=(), dtype=float64)\n",
      "iter: 1 Total loss: tf.Tensor(0.5593054956976873, shape=(), dtype=float64)\n",
      "iter: 4 Total loss: tf.Tensor(0.559131672651925, shape=(), dtype=float64)\n",
      "epoch: 11\n",
      "iter: 1 Total loss: tf.Tensor(0.5588682878418098, shape=(), dtype=float64) [0.04401801756556981, 0.5603234347495455]\n",
      "iter: 3 Total loss: tf.Tensor(0.5583352130082488, shape=(), dtype=float64)\n",
      "iter: 1 Total loss: tf.Tensor(0.5581726286999554, shape=(), dtype=float64)\n",
      "iter: 4 Total loss: tf.Tensor(0.5577825261377574, shape=(), dtype=float64)\n",
      "epoch: 12\n",
      "iter: 1 Total loss: tf.Tensor(0.5575929969006834, shape=(), dtype=float64) [0.044066356898168145, 0.5574756666658461]\n",
      "iter: 3 Total loss: tf.Tensor(0.557106811996023, shape=(), dtype=float64)\n",
      "iter: 1 Total loss: tf.Tensor(0.556957437073013, shape=(), dtype=float64)\n",
      "iter: 4 Total loss: tf.Tensor(0.5568212808875966, shape=(), dtype=float64)\n",
      "epoch: 13\n",
      "iter: 1 Total loss: tf.Tensor(0.5565607125615031, shape=(), dtype=float64) [0.043332369460834304, 0.5548547512991037]\n",
      "iter: 2 Total loss: tf.Tensor(0.5563007779447592, shape=(), dtype=float64)\n",
      "iter: 1 Total loss: tf.Tensor(0.556119249984196, shape=(), dtype=float64)\n",
      "iter: 4 Total loss: tf.Tensor(0.5558163841400223, shape=(), dtype=float64)\n",
      "epoch: 14\n",
      "iter: 1 Total loss: tf.Tensor(0.5555717242499789, shape=(), dtype=float64) [0.043613033116370555, 0.5533903547217739]\n",
      "iter: 2 Total loss: tf.Tensor(0.5553381788231893, shape=(), dtype=float64)\n",
      "iter: 1 Total loss: tf.Tensor(0.5551683464447947, shape=(), dtype=float64)\n",
      "iter: 4 Total loss: tf.Tensor(0.5552086919396034, shape=(), dtype=float64)\n",
      "epoch: 15\n",
      "iter: 1 Total loss: tf.Tensor(0.5549421851616223, shape=(), dtype=float64) [0.04350226018565537, 0.5521344397692345]\n",
      "iter: 2 Total loss: tf.Tensor(0.5547370927625301, shape=(), dtype=float64)\n",
      "iter: 1 Total loss: tf.Tensor(0.5545836198340893, shape=(), dtype=float64)\n",
      "iter: 4 Total loss: tf.Tensor(0.5546248670625369, shape=(), dtype=float64)\n",
      "epoch: 16\n",
      "iter: 1 Total loss: tf.Tensor(0.5543213837053516, shape=(), dtype=float64) [0.04359933879538478, 0.551120669053315]\n",
      "iter: 2 Total loss: tf.Tensor(0.5541299157711126, shape=(), dtype=float64)\n",
      "iter: 1 Total loss: tf.Tensor(0.553986496967634, shape=(), dtype=float64)\n",
      "iter: 4 Total loss: tf.Tensor(0.5541283546086004, shape=(), dtype=float64)\n",
      "epoch: 17\n",
      "iter: 1 Total loss: tf.Tensor(0.5537627885022127, shape=(), dtype=float64) [0.04291147047097611, 0.550286613076815]\n",
      "iter: 2 Total loss: tf.Tensor(0.5535929956645467, shape=(), dtype=float64)\n",
      "iter: 1 Total loss: tf.Tensor(0.5534635349315067, shape=(), dtype=float64)\n",
      "iter: 4 Total loss: tf.Tensor(0.553642293656226, shape=(), dtype=float64)\n",
      "epoch: 18\n",
      "iter: 1 Total loss: tf.Tensor(0.5533094999249399, shape=(), dtype=float64) [0.04337539815999503, 0.549549585742785]\n",
      "iter: 2 Total loss: tf.Tensor(0.5531550114295714, shape=(), dtype=float64)\n",
      "iter: 1 Total loss: tf.Tensor(0.553035177321314, shape=(), dtype=float64)\n",
      "iter: 6 Total loss: tf.Tensor(0.5531322991630718, shape=(), dtype=float64)\n",
      "epoch: 19\n",
      "iter: 1 Total loss: tf.Tensor(0.5528761021753212, shape=(), dtype=float64) [0.04356402748938359, 0.548974198857144]\n",
      "iter: 2 Total loss: tf.Tensor(0.5527338021519286, shape=(), dtype=float64)\n",
      "iter: 1 Total loss: tf.Tensor(0.552622543771792, shape=(), dtype=float64)\n",
      "iter: 6 Total loss: tf.Tensor(0.5526882378184046, shape=(), dtype=float64)\n",
      "epoch: 20\n",
      "iter: 1 Total loss: tf.Tensor(0.552532703602792, shape=(), dtype=float64) [0.043972354874614206, 0.5485129351817096]\n",
      "iter: 2 Total loss: tf.Tensor(0.5524007574868243, shape=(), dtype=float64)\n",
      "iter: 1 Total loss: tf.Tensor(0.5522971319962133, shape=(), dtype=float64)\n",
      "iter: 4 Total loss: tf.Tensor(0.5524457478450224, shape=(), dtype=float64)\n",
      "Final loss: tf.Tensor(0.5522524684581189, shape=(), dtype=float64) best loss: tf.Tensor(0.5522524684581189, shape=(), dtype=float64)\n"
     ]
    }
   ],
   "source": [
    "reload(MultiLearn_GLRM)\n",
    "from MultiLearn_GLRM import Multi_Learn, generate_AXY\n",
    "from MultiLearn_GLRM.Params.supervised_embedding_params import Multilearn_GLRM_Supervised_Embeddings_Train_Params, Multilearn_GLRM_Supervised_Embeddings_Test_Params\n",
    "\n",
    "embedding_dim = 20\n",
    "functional_loss='WLSE'\n",
    "params[\"embedding_dim\"] = embedding_dim\n",
    "\n",
    "n_class = n_class\n",
    "\n",
    "A_prime, X_prime, Y_prime = generate_AXY.get_supervised_embedding_glrm_train_form(Train[:,:-n_class], Train[:,-n_class:], n_class, embedding_dim)\n",
    "GLRM_loss_list, X_regulariation_list, Y_regulariation_list, X_grad_restrictions, Y_grad_restrictions = Multilearn_GLRM_Supervised_Embeddings_Train_Params(A_prime, X_prime, Y_prime, n_class)\n",
    "\n",
    "params[\"train_hyper_params\"] = [GLRM_loss_list, X_regulariation_list, Y_regulariation_list]\n",
    "\n",
    "num_iterations= 200\n",
    "learning_rate=0.05\n",
    "result = Multi_Learn.alternating_minimization(A_prime, X_prime, Y_prime, GLRM_loss_list, X_regulariation_list, Y_regulariation_list, X_grad_restrictions, Y_grad_restrictions, num_iterations, learning_rate, n_class, functional_loss=functional_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "58042702",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "beta = result[1][1:,0:n_class]\n",
    "Y_final = result[1][1:embedding_dim+1,n_class:]\n",
    "params[\"beta\"] = beta.numpy().tolist()\n",
    "params[\"embedding_matrix\"] = Y_final.numpy().tolist()\n",
    "\n",
    "train_prob = tf.nn.softmax(np.matmul(result[0], result[1][:,0:n_class])).numpy()\n",
    "label_train_shuffled =  np.argmax(Train[:,-n_class:], axis=1)\n",
    "params[\"train_accuracy\"] = accuracy_score(label_train_shuffled, np.argmax(train_prob, axis=1))\n",
    "params[\"train_accuracy\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "517df8b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.95\n",
      "0.95\n",
      "1.0\n",
      "0.95\n",
      "0.95\n",
      "0.95\n",
      "1.0\n",
      "1.0\n",
      "0.95\n",
      "1.0\n",
      "0.95\n",
      "0.95\n",
      "0.95\n",
      "0.95\n",
      "0.95\n",
      "0.95\n",
      "0.95\n",
      "0.95\n",
      "0.95\n",
      "0.95\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.04000000000000026"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reload(MultiLearn_GLRM)\n",
    "from MultiLearn_GLRM import Multi_Learn, generate_AXY\n",
    "from MultiLearn_GLRM.Params.supervised_embedding_params import Multilearn_GLRM_Supervised_Embeddings_Train_Params, Multilearn_GLRM_Supervised_Embeddings_Test_Params\n",
    "\n",
    "A = X_test\n",
    "A_prime =tf.constant(A)\n",
    "\n",
    "\n",
    "X = np.random.standard_normal((A.shape[0], embedding_dim))\n",
    "X_prime = X\n",
    "X_prime = tf.Variable(X_prime)\n",
    "\n",
    "Y_prime = Y_final\n",
    "\n",
    "one_hot_targets_val = np.eye(n_class)[Y_test.reshape(-1,)-1]\n",
    "one_hot_targets_val\n",
    "\n",
    "GLRM_loss_list, X_regularization_loss_list, Y_regularization_loss_list, X_grad_restrictions, Y_grad_restrictions =  Multilearn_GLRM_Supervised_Embeddings_Test_Params(A_prime, X_prime, Y_prime)\n",
    "params[\"test_hyper_params\"] = [GLRM_loss_list, X_regularization_loss_list, Y_regularization_loss_list]\n",
    "\n",
    "num_iterations=10000\n",
    "learning_rate=0.1\n",
    "results_log = []\n",
    "for i in range(0,20):\n",
    "    A = X_test\n",
    "    A_prime =tf.constant(A)\n",
    "\n",
    "\n",
    "    X = np.random.standard_normal((A.shape[0], embedding_dim))\n",
    "    X_prime = X\n",
    "    X_prime = tf.Variable(X_prime)\n",
    "\n",
    "    Y_prime = Y_final\n",
    "\n",
    "    result_val = Multi_Learn.predict(A_prime, X_prime, Y_prime, GLRM_loss_list, X_regularization_loss_list, Y_regularization_loss_list, num_iterations, learning_rate)\n",
    "    val_pred = result_val[0]\n",
    "    val_pred = np.hstack((val_pred, np.ones(shape=(val_pred.shape[0],1))))\n",
    "    val_pred = np.matmul(val_pred, beta)\n",
    "    val_pred = tf.nn.softmax(val_pred).numpy()\n",
    "    accuracy = accuracy_score(Y_test-1, np.argmax(val_pred, axis=1))\n",
    "\n",
    "    print(accuracy)\n",
    "    results_log.append(accuracy)\n",
    "\n",
    "\n",
    "params[\"accuracy\"] = results_log\n",
    "1 - np.mean(results_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c229acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "beta = result[1][1:,0:n_class]\n",
    "Y_final = result[1][1:embedding_dim+1,n_class:]\n",
    "params[\"beta\"] = beta.numpy().tolist()\n",
    "params[\"embedding_matrix\"] = Y_final.numpy().tolist()\n",
    "\n",
    "train_prob = tf.nn.softmax(np.matmul(result[0], result[1][:,0:n_class])).numpy()\n",
    "label_train_shuffled =  np.argmax(Train[:,-n_class:], axis=1)\n",
    "params[\"train_accuracy\"] = accuracy_score(label_train_shuffled, np.argmax(train_prob, axis=1))\n",
    "params[\"train_accuracy\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9c26c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_train_shuffled =  np.argmax(Train[:,-n_class:], axis=1)\n",
    "label_train_shuffled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c067f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.argmax(train_prob, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b70b2c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(beta, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7f69bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(train_prob, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eebbba8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dict_keys(['SupervisedEmbeddings', 'Factorization', 'WLSE'])\n",
    "# result[-1].keys()\n",
    "len(result[-1]['SupervisedEmbeddings']), len(result[-1]['Factorization'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be09a8ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_values = list(range(0, len(result[-1]['SupervisedEmbeddings'])))\n",
    "len(x_values[::20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d7ea820",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "x_values = list(range(0, len(result[-1]['SupervisedEmbeddings'])))\n",
    "\n",
    "plt.plot(x_values[::20], result[-1]['SupervisedEmbeddings'][::20],'-.', label=\"Supervised Learning (w: 0.5)\")\n",
    "plt.plot(x_values[::20], result[-1]['Factorization'][::20], '-.', label=\"Matrix Factorization (w: 0.5)\")\n",
    "plt.plot(x_values[::20], result[-1]['weighted_average'][::20],'-x', label=\"Functional: Weighted Average\")\n",
    "plt.yscale('log')\n",
    "# plt.xticks( x_values[::20])\n",
    "plt.ylabel('Loss (Log Scale)')\n",
    "plt.xlabel('Iteration')\n",
    "plt.title('Task Loss vs Iteration (alpha=0.1)')\n",
    "plt.legend()\n",
    "plt.savefig('./microarray/plots/MSE_wavg_microarray_loss_vs_iteration_alpha_1e-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "300bc778",
   "metadata": {},
   "outputs": [],
   "source": [
    "# alpha = 0.1\n",
    "# w1 = 0.5\n",
    "# w2= 0.5\n",
    "\n",
    "# SupervisedEmbeddings_weights = w1*np.exp(alpha*np.array(result[-1]['SupervisedEmbeddings']))\n",
    "# Factorization_weights = w2*np.exp(alpha*np.array(result[-1]['Factorization']))\n",
    "# SupervisedEmbeddings_weights_normalized = SupervisedEmbeddings_weights/(SupervisedEmbeddings_weights+Factorization_weights)\n",
    "# Factorization_weights_normalized = Factorization_weights/(SupervisedEmbeddings_weights+Factorization_weights)\n",
    "\n",
    "# plt.figure(figsize=(10, 6))\n",
    "# x_values = list(range(0, len(result[-1]['SupervisedEmbeddings'])))\n",
    "\n",
    "# plt.plot(x_values[::20], SupervisedEmbeddings_weights_normalized[::20],'-s', label=\"Supervised Learning\")\n",
    "# plt.plot(x_values[::20], Factorization_weights_normalized[::20], '-s', label=\"Matrix Factorization\")\n",
    "\n",
    "# # plt.xticks( x_values[::20])\n",
    "# plt.ylabel('Task Weights')\n",
    "# plt.xlabel('Iteration')\n",
    "# plt.title('Task Weights vs Iteration (alpha=0.1)')\n",
    "# plt.legend()\n",
    "\n",
    "# plt.savefig('./microarray/plots/MSE_max_microarray_task_weight_vs_iteration_alpha_1e-1')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "558196dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# res = np.matmul(result[0], result[1][:,0:n_class])\n",
    "# np.mean((Y_train - res)**2)\n",
    "# 0.4495674496068231\n",
    "\n",
    "# res = np.matmul(result[0], result[1][:,0:n_class])\n",
    "# np.mean((Y_train - res)**2)\n",
    "# 0.4011543372347976\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1e7f876",
   "metadata": {},
   "outputs": [],
   "source": [
    "A_prime.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6db0fa4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "<tf.Tensor: shape=(11, 1), dtype=float64, numpy=\n",
    "array([[-0.02956712],\n",
    "       [-0.43500185],\n",
    "       [ 0.46663761],\n",
    "       [ 0.28479513],\n",
    "       [-0.04591487],\n",
    "       [ 0.22732162],\n",
    "       [ 0.19878154],\n",
    "       [-0.21300145],\n",
    "       [ 0.0071286 ],\n",
    "       [ 0.11102323],\n",
    "       [-0.00936144]])>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5f49308",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "reload(MultiLearn_GLRM)\n",
    "from MultiLearn_GLRM import Multi_Learn, generate_AXY\n",
    "from MultiLearn_GLRM.Params.supervised_embedding_params import Multilearn_GLRM_Supervised_Embeddings_Train_Params, Multilearn_GLRM_Supervised_Embeddings_Test_Params\n",
    "\n",
    "A = X_test\n",
    "A_prime =tf.constant(A)\n",
    "\n",
    "\n",
    "X = np.random.standard_normal((A.shape[0], embedding_dim))\n",
    "X_prime = X\n",
    "X_prime = tf.Variable(X_prime)\n",
    "\n",
    "Y_prime = Y_final\n",
    "\n",
    "one_hot_targets_val = np.eye(n_class)[Y_test.reshape(-1,)-1]\n",
    "one_hot_targets_val\n",
    "\n",
    "GLRM_loss_list, X_regularization_loss_list, Y_regularization_loss_list, X_grad_restrictions, Y_grad_restrictions =  Multilearn_GLRM_Supervised_Embeddings_Test_Params(A_prime, X_prime, Y_prime)\n",
    "params[\"test_hyper_params\"] = [GLRM_loss_list, X_regularization_loss_list, Y_regularization_loss_list]\n",
    "\n",
    "num_iterations=10000\n",
    "learning_rate=0.1\n",
    "results_log = []\n",
    "for i in range(0,20):\n",
    "    A = X_test\n",
    "    A_prime =tf.constant(A)\n",
    "\n",
    "\n",
    "    X = np.random.standard_normal((A.shape[0], embedding_dim))\n",
    "    X_prime = X\n",
    "    X_prime = tf.Variable(X_prime)\n",
    "\n",
    "    Y_prime = Y_final\n",
    "\n",
    "    result_val = Multi_Learn.predict(A_prime, X_prime, Y_prime, GLRM_loss_list, X_regularization_loss_list, Y_regularization_loss_list, num_iterations, learning_rate)\n",
    "    val_pred = result_val[0]\n",
    "    val_pred = np.hstack((val_pred, np.ones(shape=(val_pred.shape[0],1))))\n",
    "    val_pred = np.matmul(val_pred, beta)\n",
    "    val_pred = tf.nn.softmax(val_pred).numpy()\n",
    "    accuracy = accuracy_score(Y_test-1, np.argmax(val_pred, axis=1))\n",
    "\n",
    "    print(accuracy)\n",
    "    results_log.append(accuracy)\n",
    "\n",
    "\n",
    "params[\"accuracy\"] = results_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23a2a19b",
   "metadata": {},
   "outputs": [],
   "source": [
    "params['train_hyper_params'] = []\n",
    "\n",
    "with open('./logs/Microarray_'+ str(round(np.mean(results_log),5))+'_Accuracy_MSE_MAX.json', 'w') as f:\n",
    "    f.write(json.dumps(params, sort_keys=True, indent=4, separators=(',', ': ')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2daa955",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_final.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce090ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(Y_final.numpy()).to_csv('gene_embedding_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1911077f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a34ce171",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_test-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fb218c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.argmax(val_pred, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b0657c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "params['tol'] = 10e-6\n",
    "# type(params['embedding_matrix'])\n",
    "for key in params.keys():\n",
    "    print(key, type(params[key]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ea08000",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d34aeea",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89b40320",
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.argmax(val_pred, axis=1)\n",
    "# val_pred\n",
    "# len(Y_test-1)\n",
    "Y_test-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b24b5731",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.argmax(val_pred, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8cc1d4e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34f25389",
   "metadata": {},
   "outputs": [],
   "source": [
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34fb8398",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_pred = np.hstack((result_val[0], A_prime))\n",
    "val_pred = np.hstack((val_pred, np.ones(shape=(val_pred.shape[0],1))))\n",
    "regress_val = np.matmul(val_pred, beta)\n",
    "np.mean((scaler2.inverse_transform(Y_test) - scaler2.inverse_transform(regress_val))**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d376c42e",
   "metadata": {},
   "outputs": [],
   "source": [
    "regress_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e30e421",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65aee6e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "confusion_matrix(Y_test, predictions_val), confusion_matrix(Y_train, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69c99ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "409/(409+200), 409/(409+857)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "979995fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "305/(305+133), 305/(305+961)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07698ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "2027/(2027+538), 2027/(2027+3343)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a5846e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "X_train_embedding = result[0][:, 1:embedding_dim+1]\n",
    "X_train_embedding = X_train_embedding.numpy()\n",
    "\n",
    "X_test_embedding = result_val[0].numpy()\n",
    "\n",
    "\n",
    "clf = LogisticRegression().fit(X_train_embedding, Y_train)\n",
    "pred_logistic = clf.predict(X_train_embedding)\n",
    "pred_logistic_val = clf.predict(X_test_embedding)\n",
    "accuracy_score(Y_train, pred_logistic), accuracy_score(Y_test, pred_logistic_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21d387d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(Y_test, pred_logistic_val), confusion_matrix(Y_train, pred_logistic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f17149e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "328/(328+236), 328/(328+938)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aedaa965",
   "metadata": {},
   "outputs": [],
   "source": [
    "1565/(1565+650), 1565/(1565+3805)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9377afd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "beta[5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6461d6e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1db71d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "A_prime[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17cabbd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c98a3071",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "prob_val[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9bc4471",
   "metadata": {},
   "outputs": [],
   "source": [
    "prob[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4508e482",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = tf.Variable([1,1,1])\n",
    "tf.where(temp!=1).shape[0]\n",
    "# tf.where([True, False, False, True]).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6dcba4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.min(val_pred,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ddcd2bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.min(result[0],axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2100c827",
   "metadata": {},
   "outputs": [],
   "source": [
    "beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ada90a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "result[0][3:10,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e00b9b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "'beta' in params.keys()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
